[["index.html", "A hands-on guide to FAIR and structured ecological data Preface Acknowledgements", " A hands-on guide to FAIR and structured ecological data Cherine C. Jantzen &amp; Stefan J.G. Vriend 2024-07-26 Preface This interactive manual aims to provide ecologists and other data owners of ecological data with a hands-on guide on how to make your ecological data more structured and more FAIR and tackle the problems that can occur along this process. Availability of example code This guide was developed based on practical work on different datasets. Throughout this guide, we refer to examples and code snippets from this work. The complete R code can be found on this GitHub repository. How to cite and licence If you want to cite this guide, please refer to it as: Jantzen, C.C., &amp; Vriend, S.J.G. (2024). A hands-on guide to FAIR and structured ecological data. https://lter-life.github.io/FDFDT-Manual/docs/index.html &lt; doi &gt; This guide is licensed under a Creative Commons Attribution-Noncommercial 4.0 International licence (CC BY-NC 4.0). How to provide feedback If you have any suggestions, questions or comments while reading through this guide or want to provide any other feedback on how to improve this guide, we are happy to receive your feedback through the Github repository where you can add your remarks as a new issue! Disclaimer We want to emphasise here that there is no one-size-fits-all solution and every dataset will bring its own challenges that need individual solutions. We cannot tackle all of these challenges but only give recommendations based on our own work with a range of different ecological datasets. The FAIR principles are not set in stone but rather intended as guidance. Practical implications should be interpreted by each community individually, while sticking as closely as possible to the principles themselves (cf. Maturity Indicator Authoring Group). We therefore also want to stress that the way we implement FAIR in this guide is only one way of operationalising it and is based on our own interpretations and extensions of the FAIR principles (Wilkinson et al. (2016)). Acknowledgements We would like to thank the FAIR Data for Digital Twins team - Cees Hof, Kim Ferguson, and Eric Kuijt - for the regular discussions, feedback and suggestions on the evaluation tool and guide. We thank Marcel E. Visser and Wim Hugo for their input and advice on the overall project. Thanks to Parinaz Rashidi, W. Daniel Kissling, Geerten M. Hengeveld and others involved in LTER-LIFE, for their contributions to the guide and for their efforts in continuing the development of this work through LTER-LIFE. Many thanks to the custodians of the datasets that are used as examples throughout this guide. The lessons learned from FAIRifying and structuring these datasets form the bedrock for the design and implementation of the workflow. We thank Judith Risse, Marcel E. Visser (bud burst data), Joost Vogels, Wilco C.E.P. Verberk (cricket data), G.F. (Ciska) Veen, Freddy ten Hooven, and Wim H. van der Putten (CLUE data), as well as all the people who contributed to the many years of data collection. We thank the Royal Netherlands Academy of Arts and Sciences (KNAW) for their financial support through the Institutes Research Fund 2022. References Wilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., Silva Santos, L. B. da, Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR guiding principles for scientific data management and stewardship. Scientific Data, 3(1). https://doi.org/10.1038/sdata.2016.18 "],["00-preface-part2.html", "Who this guide is for Why you should use this guide How to read this guide How this guide came to be", " Who this guide is for This guide is for anybody who has ecological data and wants to improve their data management by making their data more structured and more FAIR, for example: ecologists who want to improve their research data management and by that make their data available for reuse by others students who collect data for their thesis or internship and want to make their data reusable for others after their project is finished institutions and organisations that want to provide their employees with some practical guidance on how to increase their data maturity data stewards and managers who want to assist their colleagues in improving their research data management anyone who wants to learn more about data management, FAIR and structured data and metadata, and best practices for increasing the reusability of data Why you should use this guide Data management is the practice of taking care of data throughout its entire lifecycle, from its collection, processing and use to its storage and sharing. Throughout the whole lifecycle, good data management is crucial to ultimately enhance the reusability of the data for yourself and others. For example, by storing your data persistently and implementing versioning, you lower the risk of data loss and have higher traceability of changes and errors. Additionally, by describing, annotating, and organising the data, it becomes better understandable for others, facilitating its reuse and increasing its impact and that of the associated research. One way of reaching the goals of good data management is to make your data findable, accessible, interoperable and reusable (FAIR). For this, FAIR guiding principles have been developed that provide people with a set of considerations to evaluate whether data can be discovered and reused by others. See this chapter for more info on FAIR. How to read this guide This guide is designed in a way that you can only read the chapters that are relevant for improving the maturity of your data. To find these chapters, we provide you with a self-evaluation tool that shows to what extent your data already complies with the FAIR principles. Directly based on the outcome of the FAIR and structure evaluation you will be provided with a tailored list of the chapters in this guide you could go through to further mature your data. You therefore do not have to read the full guide from top to bottom but should easily be able to only go through the chapters most relevant to you. How this guide came to be The project “Data for Digital Twins - Piloting a FAIR Data Infrastructure for the Advanced Modelling of Ecological Data”, or FAIR Data for Digital Twins for short, was a collaborative project in 2023-2024 between the Netherlands Institute of Ecology (NIOO-KNAW) and the Dutch Data Archiving and Networked Services (DANS-KNAW) funded by the research fund of the Royal Netherlands Academy of Arts and Sciences (KNAW). The project aimed to explore how to make quantitative ecological data more FAIR and fit for advanced analytical and modelling purposes, such as artificial intelligence and digital twins. One of the main goals of this project was to use hands-on experience from FAIRifying a range of different ecological datasets to create a manual that guides ecologists step-by-step through the process of FAIRifying their ecological data themselves. Many ecological datasets are not compliant with the FAIR data principles, which makes it difficult to share and exchange biological data. This is also true for the data collected at NIOO and together with the general movement towards open and FAIR data, this initialised this project. "],["01-introduction.html", " 1 Introduction 1.1 What is FAIR? 1.2 Adding structure to FAIR 1.3 FAIR is not open 1.4 Why should you make your data more FAIR?", " 1 Introduction 1.1 What is FAIR? Research data often is stored on local hard drives, not well described or not formatted in a consistent and standard way. This makes it very difficult for others (and often even for the data owner themselves) to reuse the data or reproduce the research, because the data is not straightforward to understand, cannot be interpreted correctly, or, in case of local storage, might be unknown to others. In order to change this and increase the reusability of data, Wilkinson et al. (2016) introduced the FAIR concept together with 15 guiding principles providing guidance on how to implement it. FAIR stands for Findable, Accessible, Interoperable and Reusable, which more concrete means the following: Findable. This means that the metadata (and the data) can be easily found by humans and computers and that machine-readable metadata allows for automatic discovery of the data by machines. Accessible. It is clearly stated how the user can access the data and whether, for example, authorisation or authentication are required. Interoperable. In order to integrate the data with other data and to incorporate it into workflows or applications, the data needs to be fully compatible with other data. In easier words this means that data resources should ‘speak the same language’ to be used together. Reusable. The ultimate goal of the FAIR concept is to make the data reusable, which means that there is proper annotation in the form of metadata that allows users (and machines) to understand the data and correctly interpret it. Figure 1.1: The FAIR concept. The four letters of FAIR stand for findable, accessible, interoperable and reusable and come with 15 guiding principles. The 15 guiding principles describe different elements or characteristics of datasets that can be adhered to in different combinations – partly through the distinction between data and metadata – and to different maturity levels (see also FAIR Data Maturity Model. Specification and Guidelines of RDA). As a result, datasets can vary widely in their FAIRness level and datasets that have the same overall level of FAIRness may differ in the components of FAIR they adhere to. For an entertaining but serious description of the FAIR principles and their meaning, we recommend reading “A FAIRy tale - A fake story in a trustworthy guide to the FAIR principles for research data” created within the Danish ‘FAIR across’ (FAIR på tværs) project: https://forskningsdata.dk/fairytale/ (Hansen et al. (2018)). 1.2 Adding structure to FAIR In this guide we extend FAIR as it is defined by the FAIR principles in order to make it workable in practice. This means that we include the component of structuring your data into the FAIRification process. Structuring your data, i.e., organising it in a tidy way, can make the data easier to access for others and enhances its understandability. We therefore think that this is a crucial component of increasing the reusability of your data. 1.3 FAIR is not open It is important to stress here that FAIR is not equal to open. While the metadata of your data should be openly available to enable the discovery of your data by others, the data itself does not have to be open. If you have reasons for not making the data openly available to everyone (e.g., because it contains sensitive information), this can and should be made clear in the metadata where according licensing information and access rights can be provided. As long as this is guaranteed, restricted or closed data can also be completely FAIR. 1.4 Why should you make your data more FAIR? Depending on the current maturity of your data, making data FAIR certainly requires some effort, but the benefits are evident both for yourself and for others. Findable and well-annotated data facilitates your own reuse of your data in the future and the reuse of your data by others, which simultaneously increases the visibility and the impact of your research. Increasing the interoperability of your data additionally increases the possibilities for collaborations, as your data can be integrated with other data, and hence will benefit the scientific community as a whole. References Hansen, K. K., Buss, M., &amp; Haahr, L. S. (2018). A FAIRy tale. Zenodo. https://doi.org/10.5281/ZENODO.2248200 Wilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., Silva Santos, L. B. da, Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR guiding principles for scientific data management and stewardship. Scientific Data, 3(1). https://doi.org/10.1038/sdata.2016.18 "],["01.1-FAIR-assessment.html", "1.5 FAIR and structure evaluation of your data", " 1.5 FAIR and structure evaluation of your data Datasets vary widely in format and complexity, as well as degree of structure and FAIRness. It is therefore important to first evaluate which state your data is in and what steps you can take to increase the maturity of your data. Even though there is already a range of FAIR assessment tools available (see here for an overview), that can either automatically assess the FAIRness of your data or provide self-assessment questionnaires, we decided to develop our own tool for several reasons: Existing tools tend to purely assign a score to the data but provide little guidance on how to actually improve the FAIRness of the data (Krans et al. (2022)). In contrast, our tool directly provides suggestions on how to improve the maturity of the data based on the given answers and by directing the user to the corresponding chapters of this guide. We do not score the data for every letter of FAIR as done in other tools, but score them in the four properties metadata, storage, standards and structure (more details below). While the evaluation questions of the first three components are directly derived from the FAIR principles, the structure component is an extension of FAIR not included in other tools. Automated tools often require the data to be available online already, which is not the state at which we want to start with our evaluation. Our evaluation tool “FAIR + Structure Evaluation Tool” provides you with a set of simple questions about four properties of your data: metadata, storage, standards and structure. Metadata is data about your data and contains information about the who, where, what, when and how of data collection allowing another user of your data to understand and reuse it without prior knowledge of the data. Storage is about whether your data is stored persistently and in a way that makes it findable and accessible to others. Standards describe uniform, community-accepted formats in which both data and metadata are stored and which enhance compatibility with other datasets. Structure is about whether your data is organised in a consistent and logical way that makes the data easier to understand for others. Apart from the questions about structure, the questions in the evaluation are all directly based on the FAIR principles. Figure 1.2 visualises how the data properties in the evaluation link to the letters of FAIR. Your answers will provide you with a circle diagram showing how mature your dataset is in each of the properties and an associated list of chapters of this manual you can work through to further improve the maturity of your data. Figure 1.2: Link between data properties and FAIR. The letters on the left stand for the data properties used in the FAIR and structure evaluation (metadata, storage, standard, structure). Each plus indicates that the data property improves the maturity of each of the respective letter of FAIR. A blank space means that this data property has no direct influence on the respective letter of FAIR. It will likely be difficult to reach the full score in all of the properties of your data, but this is also not necessarily the aim of this evaluation or guide. Every improvement is already a great step! Especially the components that link to interoperability are more difficult to implement, as true interoperability generally is a big challenge within and across disciplines (Pagano et al. (2013)). One step to reach interoperability is to use a language for knowledge representation and linked data, such as RDF (Resource Description Framework). However, as this is more the expertise of data scientists or information scientists instead of ecologists, we will not look into these topics in this guide and, hence, it is not possible to reach the full score in the evaluation of “Standards” by only following this guide. For more information on full interoperability see this section. References Krans, N. A., Ammar, A., Nymark, P., Willighagen, E. L., Bakker, M. I., &amp; Quik, J. T. K. (2022). FAIR assessment tools: Evaluating use and performance. NanoImpact, 27, 100402. https://doi.org/10.1016/j.impact.2022.100402 Pagano, P., Candela, L., &amp; Castelli, D. (2013). Data interoperability. Data Science Journal, 12(0), GRDI19–GRDI25. https://doi.org/10.2481/dsj.grdi-004 "],["01.2-Mapping-assessment-questions.html", "1.6 Mapping of evaluation questions to FAIR principles", " 1.6 Mapping of evaluation questions to FAIR principles As mentioned in section 1.5, we based the questions of the evaluation tool related to the categories metadata, storage and standards directly on the FAIR principles. As shown in Figure 1.3, each FAIR principle translates to one or more questions. Principles that are applicable to both data and metadata are split into separate questions. Figure 1.3: Each of the FAIR principles (grey boxes) can directly be linked to one or more of the questions of our evaluation tool (coloured boxes, pink for metadata, blue for storage, green for standards). The questions about structure are missing, as they are not directly derived from the FAIR principles. "],["02-example-datasets.html", " 2 Example datasets 2.1 Bud burst data 2.2 Cricket data 2.3 CLUE field vegetation cover data (i.e., CLUE data)", " 2 Example datasets To illustrate the process of FAIRifying and structuring ecological data, we rely on several example datasets we used to develop this guide. These datasets were selected because they cover a broad variety of different structures and different levels of FAIR, ranging from observational to experimental data and from highly unstructured and undocumented data to data that already fulfils some of the FAIR principles. For all example datasets we went through the full workflow described in this guide, except the step of publishing the standardized and structured version of the data on a repository. This is still work in progress, as we rely on datasets of others whose permission is needed for that. Without storing the data on a repository, many of the circles in the evaluation cannot be filled. In the following figures we therefore use solid circles for already fulfilled evaluation criteria and striped circles for criteria that will be fulfilled once the dataset is stored in a suitable repository. 2.1 Bud burst data This dataset contains long-term data on the phenology of different tree species. Every year in spring, deciduous trees develop new leaves from their buds at a certain point in time. The timing of this can be crucial for herbivores, such as caterpillars, relying on newly emerging leaves as an important food source. To monitor shifts in tree phenology, the Department of Animal Ecology at NIOO-KNAW has assessed leaf development every year since 1988 by scoring the stage of bud opening on a fixed scale (cf. Visser &amp; Holleman (2001)). Figure 2.1: FAIR assessment of bud burst data. Striped circles indicate the hypothetical status of the dataset once it is stored in a repository (see note box above). 2.2 Cricket data This dataset contains experimental data on invertebrates belonging to the publication of Vogels et al. (2021). They tested whether changes in plant N:P ratios following sod-cutting constrain the reproductive potential in the European field cricket (Gryllus campestris). Different phosphorus and liming treatments were applied to vegetation plots in the heathlands of the Veluwe and the plants of these plots used in a feeding experiment in which the reproductive success of the female crickets was measured. Figure 2.2: FAIR assessment of cricket data. Striped circles indicate the hypothetical status of the dataset once it is stored in a repository (see note box above). 2.3 CLUE field vegetation cover data (i.e., CLUE data) This dataset consists of data on vegetation cover collected in a long-term grassland biodiversity field experiment site in the Veluwe, Netherlands, also abbreviated as CLUE fields (= changing land usage, enhancement of biodiversity and ecosystem development). Data collection started in 1996 and was done by the Department of Terrestrial Ecology of NIOO-KNAW. On a former agricultural field, experimental plots have been established in a random block design and different sowing treatments (i.e., high diversity, low diversity, continued agricultural rotation, and natural succession), as well as soil inoculation treatments have been applied in two different experiments in 1996. In both experiments, percent cover of each occurring plant species is measured annually. The data of both experiments resulted in two independent datasets. They are however very similar and their FAIRification process was nearly the same, so we treat them as one in this guide. Figure 2.3: FAIR assessment of CLUE data. Striped circles indicate the hypothetical status of the dataset once it is stored in a repository (see note box above). References Visser, M. E., &amp; Holleman, L. J. M. (2001). Warmer springs disrupt the synchrony of oak and winter moth phenology. Proceedings of the Royal Society of London. Series B: Biological Sciences, 268(1464), 289–294. https://doi.org/10.1098/rspb.2000.1363 Vogels, J. J., Verberk, W. C. E. P., Kuper, J. T., Weijters, M. J., Bobbink, R., &amp; Siepel, H. (2021). How to restore invertebrate diversity of degraded heathlands? A case study on the reproductive performance of the field cricket gryllus campestris (l.). Frontiers in Ecology and Evolution, 9. https://doi.org/10.3389/fevo.2021.659363 "],["03-workflow.html", " 3 Workflow", " 3 Workflow The general workflow of making your data more FAIR and structured is shown in Figure 3.1 and shortly described in the following section. To make your data more FAIR and structured, we recommend you to: First, describe your data and thereby gather metadata. Afterwards, you should decide whether your raw data should be mobilised to an online repository. If yes, you have to choose a suitable repository and upload your metadata and data there. Next, you can start to standardise both the data and the metadata. With which of the two you start does not matter, as they can both be structured and standardised separately. For the metadata: First, choose a metadata standard. Next, map the metadata to the selected standard. For the data: First, choose a suitable data standard. Then, map the data to that standard. For further standardisation of your data, use ontologies and biological taxonomies where possible to describe the contents of the data. After standardising the data, you should choose a standardised structure for the data. Restructure your data according to the chosen structure. Once standards and structure are applied, structural metadata can be created that describes how the data is organised in its new form. As a last step, your standardised data and metadata should be persistently stored in an online storage. If you have not chosen a repository before (or if you want to store the standardised data elsewhere), you should: Choose a suitable repository that accepts your new data structure/format. Mobilise the data and the metadata to the chosen repository. Even though this guide is tailored to ecological data, the workflow presented here should be generally applicable to any type of data, only the implementations of each step will differ. In the following chapters, we will go through each of the steps and provide some general background, introduce some options for implementation for ecological data and considerations that can help to choose the best suited option for your data, as well as a short insight into our own choices for the example datasets. Figure 3.1: Diagram of the workflow presented in this guide to FAIRify data. "],["04-Describe-data.html", " 4 Primary data 4.1 Describe your data (Metadata)", " 4 Primary data 4.1 Describe your data (Metadata) The first step before you store your data properly should always be to describe and annotate it. This type of description is called metadata and provides information about your dataset that helps to find, understand, and reuse it. We can distinguish between administrative, descriptive and structural metadata: Administrative metadata includes information about the authors, usage (i.e., licence), and maintenance of the data Descriptive metadata includes information about the contents, provenance (i.e., the source and/or history), and coverage (e.g., time, geographic location) of the data Structural metadata provides details about how the dataset is organised and how individual data files are related to each other Many repositories require a minimal set of metadata when uploading data, which can either directly be entered in a fill-in form provided by the repository or added in a separate file that is uploaded together with the data. The simplest form of metadata is a README text file containing some basic information on what the data is about. There are, however, no regulations on what information a README should contain and in practice, the extent and detail of it varies a lot. In general, the richer the data is described the better. Based on best practices of writing README files and in terms of standardising the metadata in a later step, we recommend to gather the following information about your data: About the dataset: Title of the dataset Short description of the contents of each data file Short description of the methods used for data collection Responsible parties, including those who created the data and those who can be contacted to answer questions about the data When was the data collected? Which time span is covered? Where has the data been collected? Licencing information: How can others use your data? To find out more about different licences, see section Licencing Changes in the data or updates should ideally also be recorded in the README Do different tables within a file and/or different files in the dataset link to each other? If yes, a short description of how they link (e.g., by which identifier) Describe the context of the dataset by cross-referencing other relevant and related datasets or metadata. These links should be meaningful by qualifying how different resources link to each other (e.g., dataset A is a derivative of dataset B or B contains complementary information to A). For tabular data: In addition to providing metadata at the dataset level, as listed above, it is recommended for tabular data to provide a data dictionary (sometimes called a codebook). A data dictionary provides the meaning of the columns and values in your data files. It serves as a reference guide for people who want to reuse your data, as it improves the interpretation and understanding of the data. For data stored in a relational database, the data dictionary is an active document, meaning that it is updated whenever the tables and columns in the database change. In most other cases, data dictionaries are static documents that need to be created manually. When you do so, describe: The meaning or definition of columns The data type (e.g., integer, string, datetime) The unit in which the measurement is given The accepted values for a column, including the meaning of codes (e.g., 0 = absent, 1 = present) (optional) If you include personal information, like email addresses, of other people in the metadata that is made available online later on, it should always be asked for permission beforehand. 4.1.1 Licencing If you deposit your data online, it is important to communicate to potential users what permissions they have and arrange a legal agreement in the form of a licence. A commonly used set of licences are the copyright licences of Creative Commons (CC). Scientific datasets most often are published under a CC BY licence, but Creative Commons also offers other licensing options, as stated in Table 4.1. They also offer an interactive tool (License chooser) that can help you to choose the best suited licence for your data. Table 4.1. Creative commons licences and their meanings. Licence What can users do with your data? CC BY users can use, distribute and build upon the data but have to give credits to the creator CC BY-NC users can use, distribute and build upon the data but have to give credits to the creator and data cannot be used commercially CC BY-SA users can use, distribute and build upon the data, they have to give credits to the creator and adaptations need to be shared under the same terms CC BY-NC-SA users can use, distribute and build upon the data, they have to give credits to the creator, adaptations need to be shared under the same terms and data cannot be used commercially CC BY-ND users are not allowed to adapt or modify the data, but can copy or distribute it if they give credits to the creator CC BY-NC-ND users are not allowed to adapt or modify the data, but can copy or distribute it if they give credits to the creator and data cannot be used commercially CC0 (“CC Zero”) copyrights are given up and work is put into the public domain Carefully consider which licence to choose. It cannot be revoked and every user of the data needs to comply with the licence’s conditions of use, even if the data is no longer distributed. "],["04.1-Structure-tabular-data.html", "4.2 Structuring your tabular data", " 4.2 Structuring your tabular data A common way of storing biodiversity data is in tables. However, simply entering the data into a spreadsheet does often not adhere to the strict requirements of tabular data, which has the following characteristics: Structured in rows and columns A row contains information about a single entity (e.g., a person’s name, age, or occupation), whilst a single column describes the same property of each entity (e.g., the names of all persons) Rectangular shape, in which each row has the same number of columns and each column the same number of rows Cells in a row/column can be left empty For a more extensive and formal description of tabular data, see W3C’s tabular data model. To standardise the data and make it reusable to others and to follow the remaining steps of this guide, it is a necessary prerequisite that data that is meant to be tabular follows the aforementioned characteristics. If not, reshape your data accordingly. If your data consists of multiple tables, the relationships between these tables should be made clear. This can be done either by describing it in a README file (see section 4.1) or, if your data is already in a standard format, in a more standardised way, such as the meta.xml file described here. "],["05-storage.html", " 5 Storing your data 5.1 Which data to store? 5.2 Choosing a repository 5.3 Repositories to store biodiversity data 5.4 Tools to help you 5.5 Our choice", " 5 Storing your data Data is often stored locally on personal computers or external hard drives which makes the data unavailable to others and increases the risk of data loss. These drawbacks can be overcome by mobilising your data to an online repository. 5.1 Which data to store? Before choosing a suitable repository, you have to decide which version of your data you wish to share: your data in its original format (i.e., primary data) or your data in a standard format. Data in its original format, provided that it is accompanied by detailed metadata to enable optimal reuse (see section 4.1), is closest to the raw data and can be shared without requiring additional effort to reformat the data. Alternatively, data in a common standard format can be reused by others in your community as well as more easily integrated with other datasets in the same format. You may also consider providing both versions of the data next to each other in the same repository allowing users to choose the version best suited for their own needs. Either way, it’s recommended to always share your metadata. Even when you are unable to share your data if, for example, it contains ecologically or personally sensitive information, metadata-only records are valuable resources. By sharing your metadata on an online repository, others can learn from the considerations you made when collecting data or the methods you followed, and easily assess whether they want to request your data. 5.2 Choosing a repository The range of existing repositories is very large and while some are domain specific, there are other repositories that accept data from different disciplines. To find the repository that is best suited for your data it can be helpful to take the following points into consideration: Which data formats are accepted? Depending on the format your data is stored in, not every repository will accept your data. Many repositories however accept a wide range of formats but have preferred formats to upload your data in, as these formats have best long-term guarantees in sustainability and accessibility (see here for an example). What repository is commonly used in your organisation, community or discipline? Check which repositories your direct colleagues in your organisation, or other data providers in your community or field of research use. Using such repositories will likely make your data better linked to other datasets and found more easily by others in your discipline. Is a persistent identifier assigned and explicitly mentioned? To ensure that your dataset can always be found, it is important that your data is associated with a persistent identifier (PID) and that this PID is explicitly mentioned in the metadata. Most repositories assign a PID when you submit your data, which is preferred over repositories in which you would need to assign the PID yourself. Different repositories also use different identifier systems which can be another selection criterion if you have personal preferences for a specific identifier system. How can the dataset be accessed? Do users have to download the dataset directly from the repository’s website or is there a more automated option available, such as an API (Application Programming Interfaces)? Is the repository using an open, free and standardised protocol, like the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH), to make the data accessible? Is metadata stored persistently even when the data is no longer available? The metadata of your dataset should be stored persistently so that users can still find out about the existence of your data, even if it is no longer available on the repository itself. Do you want version control? Version control means that changes in the data and reuploads are tracked and that multiple versions of your data are available. Not all repositories provide version control, so if this is important for you, make sure that the chosen repository provides this option. At re3data.org, for example, you can directly filter for repositories with versioning. Is the repository indexing the data? To make your data findable on the web by search engines, it has to be registered or indexed in a catalogue. Many repositories do this automatically and make your dataset findable, for example, through Google dataset search. Do you retain custody of your data or is it transferred to the repository? There are repositories that obtain the custody of your data once it is uploaded there, meaning that the data management is done by the repository and your possibilities to manage the data are then very limited. This can be beneficial if you do not want to have further responsibility for your data, but if you still want to be able to manage the data yourself after uploading it, a repository where the custody stays with you will be better suited. 5.3 Repositories to store biodiversity data Here are some examples of commonly used repositories for biodiversity data. This list is non-exclusive; other options might exist that are more suitable for your data. 5.3.1 Global repositories GBIF, the Global Biodiversity Information Facility, is a data infrastructure and international network providing open access to biodiversity data. It holds species occurrence data from different sources, ranging from museum collections to field observations. In the coming years, GBIF is expanding to include a variety of other data types, including eDNA data, camera trap data, and data on species interactions (GBIF (2021)). Data is stored using data standards, such as Darwin Core, and is openly accessible to everyone. OBIS, the Ocean Biodiversity Information System, is an international databank system for maritime biodiversity and biogeographic data with the objective to provide the largest knowledge base on the diversity, distribution and abundance of marine organisms. Both GBIF and OBIS use an Integrated Publishing Tool (IPT) as a way to submit data. It is a tool that facilitates the creation of metadata in a standard format and helps with mapping the data to Darwin Core and structure it in a Darwin Core Archive (for more information see later chapters). There is extensive documentation on how to use the IPT by GBIF here. Once the dataset is published it is assigned a DOI. Dataverse is an open-source repository software that has installations (i.e., repositories based on the software) all over the world. Data can only be published on a Dataverse instance if you are part of one of the collaborating institutions but is always accessible to everyone. A full list of Dataverse installations can be found here. Zenodo is an open European repository hosted by CERN and in contrast to the other repositories listed here not domain specific. It contains research data from several research domains, provides versioning, DOI assignment, restricted access options if wanted and assures persistent long-term storage of your data. 5.3.2 Dutch repositories Since the project partners behind this guide are situated in the Netherlands, we also want to highlight some Dutch repositories. DataverseNL is a Dutch installation of Dataverse and is hosted by DANS-KNAW. Researchers from all collaborating institutes within the Netherlands can store their data there and make it available to everybody. Reasons for uploading data to DataverseNL: It automatically gets a persistent identifier The metadata is findable through web services like Google dataset search The data is easily retrievable through an API There is versioning to track changes and re-uploads of the data The custody of the data stays with the data owners DANS Data Station Life Sciences The DANS Data Station for Life Sciences is a repository hosted by DANS-KNAW which is based on the Dataverse software. It is open for every individual researcher independent of institution both to store and retrieve data. It is always free of charge to access data and up until a data size of 50 GB also to store data. The Data Station for life sciences is domain specific, accepting data of medical, health and green life sciences, while there are other Data Stations hosted by DANS for other domains, such as social sciences, archeology and physical and technical sciences. It also allows access restrictions, which is helpful if you want to store your data persistently but not necessarily make it open for everyone. 5.4 Tools to help you re3data.org is a registry of repositories for research data where you can filter a broad range of repositories for different criteria, such as subject, country, persistent identifier system, versioning or licences. fairsharing.org is a registry that lists a variety of standards, databases and policies. To find repositories you can either directly search for a specific one and get detailed information on it or search all repositories/databases and filter for a range of options, such as country, subject or specific tags. DataCite Commons provides a repository finder that integrates information on repositories from re3data within the FAIRisFAIR project. It allows users to search for repositories by keywords and filter the repositories for certificates, such as the CoreTrustSeal, or the software they are based on. 5.5 Our choice For the dataset we owned ourselves within our department (bud burst), we decided to store the primary data on DataverseNL. Besides the reasons listed above, we chose DataverseNL because it was already used by many other researchers from our institute and it was suitable for our data type. For us it was especially important that we retain the custody of our data, which was a reason to not choose the Data Station. Additionally, we wanted to store our data within the Netherlands and it was a natural choice to choose a repository hosted by one of the project partners. References GBIF. (2021). GBIF strategic framework 2023-2027. https://doi.org/10.35035/doc-0kkq-0t82 "],["06-standard.html", " 6 Standardise your data 6.1 Choose a data standard 6.2 Data standards for biodiversity data 6.3 Tools to help you 6.4 Our choice 6.5 Mapping your data to Darwin Core", " 6 Standardise your data 6.1 Choose a data standard Once you have considered the mobilisation of your data and metadata to an online repository, you can start to standardise the data. The first step here is to choose a data standard, i.e., a standard format to name and organise the data. There is a wide range of standards available and many of them are directly tailored to a certain domain. Standards within the field of biodiversity are mostly maintained by the non-profit organisation Biodiversity Information Standards (TDWG). Which standard is best suited for your data depends on its content and format. In order to choose a standard, you should consider: Is the standard accepted and used by your community? If the community is already familiar with the standard the threshold to reuse your standardised data is likely lower. Is the standard well-maintained and stable? Data standards should be maintained, further developed and adapted by following the developments in the field they are tailored to and the needs of the community. Terms should also be stable and not change in meaning and always be referred to by a persistent identifier. 6.2 Data standards for biodiversity data Darwin Core (DwC) is a stable data standard tailored to describe biodiversity data by using a defined library of terms. It is an extension of the Dublin Core Metadata Initiative and maintained by TDWG (Biodiversity Information Standards). Darwin Core facilitates sharing of biodiversity data from various sources as every term has a clear definition and a URL, which can be used irrespective of technology, e.g., XML or RDF. ABCD (Access to Biological Collection Data) is a standard to facilitate the access to and exchange of data about specimens and observations. It is a highly structured, comprehensive and still evolving standard that is, for example, used within GBIF. It is maintained by TDWG and consists of an ontology of terms to describe the data. 6.3 Tools to help you A helpful tool to find a standard is FAIRsharing, which is a registry that lists a variety of standards and provides detailed information, for example, about their maintenance, background and how they are linked to other standards. It also allows filtering for certain criteria, such as licensing, country or domain. An overview of the standards maintained by TDWG can be found here. 6.4 Our choice For biodiversity data, a widely used data standard is Darwin Core. It is well suited to be used with species occurrence data that is either collected in the field, in experiments or comes from museum collections. For all the datasets we used to develop this guide, Darwin Core was a suitable standard, as it captures all the information in the data, is easy to apply through extensive documentation and applicable to the tabular format of our data. For this reason, we will focus mainly on Darwin Core in the following chapters, because we think that the tabular format is probably one of the most common formats biodiversity data is stored in. 6.4.1 Why Darwin Core? Darwin Core is widely used in the ecological community and required for publishing data on big ecological data infrastructures, such as GBIF and OBIS. With its primary focus on taxa and their occurrences in nature, it is well suited to describe most ecological datasets, with the necessary flexibility to describe different types of data, such as observational or experimental data. Additionally, mapping variables of ecological data to Darwin Core terms is relatively straightforward and there is detailed description available on each Darwin Core term. 6.5 Mapping your data to Darwin Core 6.5.1 Darwin Core namespaces Namespaces are prefixes used to distinguish between terms from different vocabularies or standards, ensuring there are no conflicts between similarly named terms. In the context of Darwin Core, four main namespaces are used: dwc: marks standard Darwin Core terms and generally has string values (e.g., dwc:scientificName) dc:: marks terms belonging to “elements/1.1” namespace of Dublin Core and generally has string values (e.g., dc:title) dwciri: marks Darwin Core terms that must be used with Internationalized Resource Identifier (IRI) values and are generally used in RDFs (e.g., dwciri:recordedBy) dcterms: refers to terms coming from the “terms” namespace of Dublin Core and their values depend on the details of the respective term (e.g., dcterms:identifier) 6.5.2 Darwin Core terms Every column name in your data should be mapped to a Darwin Core term. This sometimes requires restructuring the data slightly or adding additional information in extra columns, e.g., units. The values within a column (i.e., DwC term) have to be compliant with the definitions given in the Darwin Core List of terms. Table 6.1. Selected set of Darwin Core terms of each class. Bold terms are described in more detail in the following sections. Event Occurrence Taxon MeasurementOrFact Location Organism MaterialEntity GeologicalContext eventID occurrenceID taxonID measurementID locationID organismID materialEntityID geologicalContext parentEventID recordedBy kingdom parentMeasurementID country organismName preparations earliestEonOrLowestEonothem eventType recordedByID phylum measurementType countryCode organismScope disposition earliestEraOrLowestErathem eventDate recordedByID class measurementValue verbatimLocality associatedOrganisms verbatimLabel earliestEpochOrLowestSeries eventTime individualCount order measurementUnit decimalLongitude previousIdentifications associatedSequences lowestBiostratigraphicZone year organismQuantity family measurementMethod decimalLatitude organismRemarks materialEntityRemarks lithostratigraphicTerms month organismQuantityType genus measurementAccuracy geodeticDatum formation day sex specificEpithet measurementDeterminedDate coordinateUncertaintyInMeters bed samplingProtocol behavior scientificName measurementDeterminedBy footprintWKT sampleSizeValue lifeStage acceptedNameUsage measurementRemarks minimumElevationInMeters sampleSizeUnit degreeOfEstablishment nameAccordingTo minimumDepthinMeters samplingEffort occurrenceStatus higherClassification georeferenceProtocol fieldNotes occurrenceRemarks taxonRank eventRemarks catalogNumber vernacularName 6.5.3 Terms of class Event Before assigning the terms of the event class to your data, you should define what exactly one event in your data is. An event is generally defined as an action that occurs at a certain time and place. Depending on how your data is collected, there might also be some hierarchy in your events that should be accounted for. Defining how events are structured in your data and which measurements or occurrences belong together, makes it easier to properly map your data to the respective terms, especially eventID and parentEventID, and later on facilitates structuring of the data. 6.5.3.1 eventID &amp; parentEventID The eventID can be a globally unique identifier or an identifier specific to the dataset. For more information on how to create globally unique identifiers, see this chapter. If you choose to create identifiers specific to the dataset, we recommend establishing a structure that simultaneously is informative about the event. If there is a hierarchy in the events, eventIDs should build on the parentEventIDs. We recommend using separators (e.g., “_” or “-”) to indicate the different blocks of the event levels within an eventID. Some guides about persistent identifiers (e.g., Richards et al. (2011)) state that IDs should be opaque, meaning that they do not give any information about what they describe or relationships between resources. By this it can be avoided that the ID contains information that might no longer be true at a later time (because the resource has changed). However, these guides mostly refer to opaque identifiers on the data or resource level and not on the record level, which is why we decided to increase human-readability by creating informative dataset-specific IDs. This problem does of course not occur if you choose GUIDs. Bud burst: In the bud burst data, we have three different event levels. The highest level describes the event of going to the field in a certain year to a certain area. The eventID therefore consists of an area abbreviation and the year, e.g., HV2004. For the event level below, the sampling on a specific day within a year and area, the previous eventID becomes the parentEventID and the level 2 eventID extends the parentEventID by a separator and the day of the year, e.g., HV2004_99. The third and lowest event level describes the sampling of a specific tree on a day within an area and year. The eventID of level 2 becomes the parentEventID and the level 3 eventID extends the parentEventID by an underscore and the number of the tree, e.g. HV2004_99_5. This way, the eventID is easily human-readable and directly gives the most important information about the event. Crickets: In the cricket data, there were two different event types, one relating to measurements that have been taken on plants and the other to measurements of individual crickets. For the plants, each plot-treatment combination was measured once, leading to one event for each of them. The eventIDs were therefore simply the plot name and a treatment code, for example, PM1-T1 (PM1 being the plot name, T1 the first treatment). Conversely, defining events for cricket measurements posed more challenges due to the lack of specific date and time information in the data. By using the publication belonging to the dataset and the documentation file accompanying the data, it was possible to identify which measurements were taken simultaneously, allowing them to be grouped into the same event. This approach yielded 18 distinct event groups per individual cricket from which we build the eventID by merging the cricket identifier with the event group number, e.g., Cr1-05 (indicating individual cricket number 1 and event group 5). 6.5.3.2 Date information (eventDate, year, month, day) eventDate is necessary to be filled but can vary in its precision. If a full date is available, use the ISO standard (8601-1:2019) date format of YYYY-MM-DD. You should always give the date as precise as possible, so if the exact day is not available, give at least the year and month, etc. You can also give an interval if a distinct point in time is not available by separating the days or months or years by a slash, e.g., 2014-04-01/08 meaning some time in the interval between the first April 2014 and the 08 April 2014. If the interval starts and ends at days of different months, also repeat the year (e.g., 2014-04-01/2014-07-01). In addition to eventDate, we recommend to use the terms year, month and day (if applicable) as this can facilitate later analysis with the data. If eventDates of different levels are not all of the same format, for example the parent event only has the year (2024), while the child event has an exact date (2024-11-05), the event dates can be transformed to characters, as programs like R do not accept different date formats in the same column. In this case, the additional columns of year, month and day are especially helpful, as missing values can be left empty/filled with NA, while the rest is numeric. 6.5.3.3 Time information (eventTime) If you also have time information in your data, this can either also be stored in eventDate or additionally in an extra column as eventTime. As with date, time should be given in time of day (in 24h system) following the ISO standard 8601-1:2019. Time intervals can again be given by separating times with a slash, e.g., 13:00:00/15:30:00 (the interval between 13:00 and 15:30). If time information should be added to eventDate (which is recommended, as eventDate should be as precise as possible), the date and the time are separated with a “T”, for example, 2018-08-29T15:19. Time Zones can be specified by adding a “Z” at the end, if the time is recorded in UTC (e.g., 2010-02-16T08:40Z) or by stating the deviation from UTC by adding a plus or minus and the number of hours of the deviation, e.g., 2010-02-16T08:40-0300 meaning UTC minus 3 hours. If neither of both is given, it is assumed that the time is given as the local time. 6.5.4 Terms of class Location 6.5.4.1 Coordinates (decimalLonigtude, decimalLatitude, geodeticDatum) Exact coordinates for locations can be given with the terms decimalLongitude and decimalLatitude. If these are filled, the term geodeticDatum should also be filled, giving the spatial reference system in which the coordinates are given. Best practice is to use the EPSG codes. For hierarchical events, like in the bud burst case, coordinates should only be assigned to the event levels they were actually measured, e.g., only for level 3 events where the exact coordinates of the trees are known. 6.5.4.2 Geographic descriptions (verbatimLocality, continent, islandGroup, island, country, municipality, city, stateProvince, county, countryCode) In addition to the coordinates, which we strongly recommend to include, the term verbatimLocality can sometimes be useful. It gives the original textual description of the location, which can be particularly useful if areas have well-known names and makes these locations easier recognisable. There is also a range of other terms that provide geographic description and can be used to determine the location of events, such as continent, islandGroup, island, country, municipality, city, stateProvince or county. In addition to country, you can also use countryCode, which gives the standard code for the country, ideally the two letter codes of ISO 3166-1-alpha-2. 6.5.5 Terms of class Organism The term “organism” in Darwin Core is defined more broadly than in the strict biological sense and can refer to either a specific organism or a defined group of organisms that are taxonomically homogenous. This means a single cricket can be considered an organism, as well as a flock of birds belonging to the same species. 6.5.5.1 organismID Individual organisms can be assigned a unique organismID. This is helpful, if you have several data records that belong to the same individual or organism for maintaining this link between records (e.g., measurements of the same tree in several years). Previously there was the DwC-term individualID, which nowadays is deprecated and replaced with organismID. As with other ID fields, the organismID can either be globally unique or specific to the dataset. 6.5.6 Terms of class Occurrence Occurrence is generally defined as the existence of an organism at a certain time and place. 6.5.6.1 occurrenceID The occurrenceID assigns a unique ID to every occurrence record. Several occurrenceIDs can be associated with a single eventID, for example when different species occurred at the same event. You can either assign GUIDs (see here) or identifiers specific to the dataset. If you choose to create dataset-specific identifiers, we recommend creating informative IDs that give information about the occurrences they describe (see eventID for more details). If your data also includes an eventID, we recommend proceeding with the block-structure of IDs we have already used there by extending the eventID of the corresponding event with a new block that numbers the occurrences of that event. However, if there are occurrence records for different event levels, extending the eventID may lead to unequal length of the occurrenceIDs from the different event levels. This can be confusing and lead to doubled IDs, as higher level occurrenceIDs then have the same length as for example lower level eventIDs (i.e., HV2004_99_5 as an eventID refers to a third level event but as an occurrenceID it could also refer to the fifth occurrence record of the higher level eventID HV2004_99). To avoid this confusion and ensure unique IDs, we therefore add the prefix “o” (for occurrence) in the block of the ID that numbers the occurrences, e.g., HV2004_99_5_o1. This method helps maintain clarity and uniqueness across different event levels. crickets: occurrenceID = Cr1-05_1 (corresponding to eventID Cr1-05, indicating the first occurrence of that event) 6.5.6.2 Taxonomic information Taxonomic information should always be specified to the lowest level possible. Which level this is can be specified with the term taxonRank. We recommend to use at least the following DwC terms to describe taxon information: kingdom phylum class order family genus specificEpithet scientificName The specificEpithet only contains the second part in the scientific name of a species, while the scientificName contains the full scientific name (i.e., genus + species epithet) together with author and date information, if available. Taxonomic information can be added manually but we recommend to directly query a biological taxonomy for the complete classification of the taxa in your data, to avoid misspellings and/or the use of outdated classification. Some taxa come with several authorship information or with synonym names. To deal with this, it can again be helpful to directly retrieve the taxonomic information from a biological taxonomy, as they also contain information on the accepted usage of classifications. Which taxonomies exist for this and tools to help you use them, will be explained in more detail in the section biological taxonomies. However, even if you retrieve taxonomic information automatically from a taxonomy, we strongly recommend to double check the retrieved information manually, as mistakes can easily occur in the case of rare species or taxa with similar names (e.g., for the plant genus Convolvulus and the animal genus Conovulus). Crickets: For the cricket dataset, we queried the taxonomic information of the European field cricket (Gryllus campestris) directly from GBIF, which resulted in the following taxonomic terms (table 6.1). Table 6.1. Taxonomic information for Gryllus campestris as retrieved from GBIF stored in respective Darwin Core terms. scientificName kingdom phylum class order family genus specificEpithet taxonRank Gryllus campestris Linnaeus, 1758 Animalia Arthropoda Insecta Orthoptera Gryllidae Gryllus campestris species CLUE data: The CLUE data covers around 130 different plant species and many of them were either misspelt or synonym names were used. One example is shown in table 6.2 where the species name in the data was Deschampsia flexuos which is a synonym of the accepted species name Avenella flexuosa. This was automatically detected while retrieving the taxonomic information from GBIF and the corresponding information correctly assigned accordingly. Table 6.2. Taxonomic information for Deschampsia flexuosa, which is a synonym of Avenella flexuosa, as retrieved from GBIF and stored in respective Darwin Core terms. scientificName kingdom phylum class order family genus specificEpithet taxonRank Deschampsia flexuosa (L.) Trin. Plantae Tracheophyta Liliopsida Poales Poaceae Avenella flexuosa species 6.5.6.3 individualCount and organismQuantity In the occurrence file, it is important to quantify how many organisms have occurred. There are two different terms to do so, individualCount and organismQuantity. While individualCount is unitless and only gives the number of individuals present at the time of the occurrence, organismQuantity is more flexible, as it describes the quantity of an organism. As organisms do not have to be individuals (see above), it can for example also give the percentage of biomass of an organism or quantify it on a certain scale. Therefore, organismQuantity always requires the additional term organismQuantityType that describes the corresponding quantification system. GBIF provides a Quantity Type Vocabulary with terms they recommend to use to describe the organismQuantityType. There is ongoing discussion on which of the two terms to use. Some suggest deprecating the term individualCount, as the same information can easily be stored in organismQuantity with higher flexibility. However, individualCount seems to be a standard term that is widely used in specific disciplines, where the terms are not necessarily viewed as redundant. Moreover, it was demanded that there should first be standardised vocabulary in place for organismQuantity and organismQuantityType. Based on the greater flexibility and the potential of deprecating individualCount, we recommend using organismQuantity when possible. 6.5.7 Terms of class Measurement or fact The measurement or fact terms require your measurement records to be in a long format. As research data often is stored in a wide format, you will first need to pivot your data before mapping is possible, meaning that all your measured values (or facts) are in one column and have a variable description in a separate column. Wide versus long format When storing multiple measurements (or observations) of the same subject in a table, you can store this in a wide or in a long format. Data in the wide format is structured with each row representing a single subject and each column representing a different measure. In contrast, in the long format, each row represents a single measurement or observation and the measured value and measurement type are stored in separate columns. In Darwin Core, measurements are stored in a long format. 6.5.7.1 measurementValue &amp; measurementUnit After transforming your data into the long format, the column containing the measured values or facts can be mapped to the Darwin Core term measurementValue. Measurements come with units, which are often either stored within the column heading or not specified at all in the raw data. Darwin Core accounts for that by the term measurementUnit, which has to be added if you use measurementValue. It is best practice to use SI (International System of Units) units if possible. For unitless measurements, the measurementUnit should still be present in the data but can be left empty/filled with NA. 6.5.7.2 measurementType &amp; measurementMethod With the long format and all measurements being stored in measurementValue, you additionally need to store what has been measured, i.e., what each measurement or fact means. This is done by the term measurementType. Additionally, you should use the term measurementMethod that states how each measurement was measured. For both terms, it is recommended to use controlled vocabulary where possible. For more information on the use of ontologies, see section ontologies. Bud burst: To exemplify how the four terms measurementValue, measurementUnit, measurementType and measurementMethod are used, the following shows the previous state of the measurements in the bud burst raw data (top) and how the columns containing measurements are mapped to the Darwin Core terms (bottom). Table 6.3. Bud burst data in original state. Year TreeID TreeTopScore TreeAllScore 1988 61 2 2 Table 6.4. Bud burst data after mapping to Darwin Core terms. measurementType measurementValue measurementUnit measurementMethod bud burst stage (PO:0025532) of the tree crown 2 NA https://doi.org/10.1098/rspb.2000.1363 bud burst stage (PO:0025532) of the whole tree 2 NA https://doi.org/10.1098/rspb.2000.1363 6.5.7.3 measurementID The measurementID assigns a unique ID to every measurement or fact. Several measurementIDs can belong to one occurrenceID and/or eventID, for example when different characteristics of the same individual are measured. If you choose to create identifiers specific to the dataset, we recommend proceeding with the block-structure of IDs we have already used for the eventID and occurrenceID. If there are measurements at different event levels, extending the occurrenceID or eventID by a separator and a number for the measurement will lead to unequal length of measurementIDs from different event levels and increases the possibility that IDs are not unique within the dataset. To avoid this, we add the prefix “m” (for measurement) in the block of the ID that numbers the measurements. Budburst: measurementID = HV2004_99_5_1_m2 for the second measurement of the eventID HV2004_99_5 and the occurrenceID HV2004_99_5_1 Crickets: measurementID = Cr1-05_1_m8 for the eighth measurement of eventID Cr1-05 and occurrenceID Cr-05_1 References Richards, K., White, R., Nicolson, N., &amp; Pyle, R. (2011). A beginner’s guide to persistent identifiers. https://doi.org/10.35035/MJGQ-D052 "],["06.1-taxonomies.html", "6.6 Ontologies and controlled vocabulary 6.7 Biological taxonomies 6.8 Creating GUIDs 6.9 Cross-referencing other resources in the data", " 6.6 Ontologies and controlled vocabulary Ontologies provide definitions of terms and their relationships to other terms in a human-interpretable way and thereby create a semantic model of the concepts that are used within a specific research domain. In simpler words, ontologies are a way of showing the terms used in a subject area and how they relate to one another. With these references you can ensure that terms in your data are always interpreted the same way and clearly understood by others. For example, ontologies can be particularly useful for filling in Darwin Core terms like measurementType or measurementMethod (see section Terms of class Measurement or fact). Next to ontologies, you can also refer to terms listed in a thesaurus, which can be seen as a domain specific dictionary. In contrast to ontologies, searching for a specific term across different thesauri can be a bit more cumbersome, as there are no look-up services where you can directly query several thesauri simultaneously. However, thesauri can be quite helpful for filling in the keywords of your metadata, and it is recommended to use them. We therefore provide a few examples of thesauri tailored to biodiversity or ecological data: GEMET - General Multilingual Environmental Thesaurus EnvThes - Thesaurus for long term ecological research, monitoring and experiments UNESCO Thesaurus 6.6.1 Tools to help you The Ontology Lookup Service (OLS) is a repository for biomedical ontologies but it also holds plenty of terms and ontologies relevant for ecology. You can search across ontologies for specific terms or filter for certain ontologies. There is also an API available to facilitate the use of OLS in workflows/programs. Ontobee is a linked data server and another option to browse through around 260 different ontologies and directly search for specific terms. If you are more interested in finding an ontology dedicated to a specific domain, looking directly at the OBO foundry can be helpful. The OBO Foundry (Open Biological and Biomedical Ontology Foundry) is tailored to biological sciences and develops and maintains ontologies. It is not searchable for individual terms but provides information on each ontology. If you chose a specific ontology and before using it you want to assess how FAIR this ontology is, you can use FOOPS!. It is considered an ontology pitfall scanner for FAIR and by providing the URI of an ontology it assesses how well the ontology matches the FAIR principles. 6.7 Biological taxonomies There is a diversity of biological taxonomies that you can use to query taxonomic information for the taxa occurring in your dataset. In this guide we cannot cover all of them but we want to provide some more information on a selected set of taxonomies. 6.7.1 GBIF Backbone taxonomy The GBIF backbone taxonomy, as the name indicates, builds the basis of the indexing of the species occurrence records stored at GBIF and aims to cover all the species that GBIF deals with. It further aims to bring all different taxa names together and organise them. Taxa are assembled from a hierarchical list of 105 sources, using the Catalogue of Life (COL) as a starting point and thereby tightly linking these two taxonomies. Species not found in the COL are then assembled from the remaining sources that are checked afterwards, making the GBIF backbone taxonomy relatively wide-ranging. 6.7.2 Catalogue of Life (COL) The Catalogue of Life is an international community for listing species and aims to create a consistent and up-to-date list of currently accepted species across all known taxonomic groups, which is freely accessible. Besides listing taxa, it aims to show all scientific names a taxon is referenced by. 6.7.3 Encyclopedia of Life (EOL) The Encyclopedia of Life aims to gather knowledge about life on earth and make it globally, openly and freely accessible to everyone. Besides taxonomic information, it also provides details on food webs and other ecological aspects of taxa. The community behind it consists of open access biodiversity knowledge providers, such as museums, libraries and universities. 6.7.4 Integrated Taxonomic Information System (ITIS) ITIS is an authoritative system that contains information about taxa and their relationships. It provides a comprehensive and openly available taxonomy and is used as the taxonomic backbone for the Encyclopedia of Life and within the Catalogue of Life. It aims to provide a comprehensive taxonomy of species worldwide to allow sharing of biodiversity data. 6.7.5 World Registry of Marine Species (WoRMS) WoRMS is authoritative classification and catalogue for marine taxa managed by taxonomists and thematic experts that includes accepted and synonym taxonomic information allowing for interpretation of the taxonomic literature. It is the recommended biological taxonomy to retrieve information from when publishing data to OBIS. 6.7.6 Tools to help you If you want to retrieve taxonomic information directly from one of the aforementioned taxonomies, there is a helpful R package available that effectively uses the APIs of each of these taxonomies, which is called taxize (Chamberlain et al. (2022)). With taxize you can do plenty of different operations, for example, directly parsing in a list of taxa and retrieving their taxonomic classification or their identifiers from one of the taxonomies (e.g., get_gbifid_() retrieves the taxon information from the GBIF backbone taxonomy). If you want to check whether the species names you use in your data are up to date, if they are spelled correctly or if you only have common names but not scientific names in your data, you can use the global name resolving function of taxize (gnr_resolve()). The Global Names Resolver is a service provided by the EOL and shows you which names could be matched to your input name and in which taxonomies or data sources they can be found. Bud burst: One of the tree species in the bud burst data is the Pedunculate oak (Quercus robur). To get detailed taxonomic information for this species, we query it from the GBIF backbone taxonomy. As the results are presented in a list, we additionally bind the rows into a data frame using the dplyr package (Wickham, François, et al. (2023)). taxonInfo &lt;- taxize::get_gbifid_(sci = &quot;Quercus robur&quot;) |&gt; dplyr::bind_rows() taxonInfo ## usagekey scientificname rank status matchtype canonicalname ## 1 2878688 Quercus robur L. species ACCEPTED EXACT Quercus robur ## 2 8206510 Quercus robur (Ten.) A.DC. species SYNONYM EXACT Quercus robur ## 3 7911626 Quercus robur Asso, 1779 species DOUBTFUL EXACT Quercus robur ## 4 7586523 Quercus robur Pall. species DOUBTFUL EXACT Quercus robur ## confidence kingdom phylum order family genus species ## 1 97 Plantae Tracheophyta Fagales Fagaceae Quercus Quercus robur ## 2 97 Plantae Tracheophyta Fagales Fagaceae Quercus Quercus robur ## 3 96 Plantae Tracheophyta Fagales Fagaceae Quercus Quercus robur ## 4 96 Plantae Tracheophyta Fagales Fagaceae Quercus Quercus robur ## kingdomkey phylumkey classkey orderkey familykey genuskey specieskey synonym ## 1 6 7707728 220 1354 4689 2877951 2878688 FALSE ## 2 6 7707728 220 1354 4689 2877951 2878688 TRUE ## 3 6 7707728 220 1354 4689 2877951 7911626 FALSE ## 4 6 7707728 220 1354 4689 2877951 7586523 FALSE ## class acceptedusagekey ## 1 Magnoliopsida NA ## 2 Magnoliopsida 2878688 ## 3 Magnoliopsida NA ## 4 Magnoliopsida NA ## note ## 1 &lt;NA&gt; ## 2 Similarity: name=110; authorship=0; classification=-2; rank=6; status=0; score=114 ## 3 Similarity: name=110; authorship=0; classification=-2; rank=6; status=-5; score=109 ## 4 Similarity: name=110; authorship=0; classification=-2; rank=6; status=-5; score=109 There are four matches of our taxon in the GBIF backbone taxonomy. In this example, they differ in their scientific name and the author information given there. If we look at the column “status”, is becomes clear that only one of the matches contains the accepted scientific name, while the second match is a synonym and the others are considered “doubtful”. We therefore want to filter the results for only the matches that have the status “accepted” and the matchtype “exact”, which means that the canonical name matches our input name letter by letter. We again use the package dplyr to filter the data, which leaves us with one match. taxonInfo |&gt; dplyr::filter(status == &quot;ACCEPTED&quot; &amp; matchtype == &quot;EXACT&quot;) ## usagekey scientificname rank status matchtype canonicalname confidence ## 1 2878688 Quercus robur L. species ACCEPTED EXACT Quercus robur 97 ## kingdom phylum order family genus species kingdomkey ## 1 Plantae Tracheophyta Fagales Fagaceae Quercus Quercus robur 6 ## phylumkey classkey orderkey familykey genuskey specieskey synonym ## 1 7707728 220 1354 4689 2877951 2878688 FALSE ## class acceptedusagekey note ## 1 Magnoliopsida NA &lt;NA&gt; If you have more than one taxon in your data, you can also directly query the taxonomic information for a number of species at once. tree_species &lt;- c(&quot;Quercus robur&quot;, &quot;Quercus rubra&quot;, &quot;Larix kaempferi&quot;, &quot;Pinus sylvestris&quot;, &quot;Betula pendula&quot;) taxize::get_gbifid_(sci = tree_species) |&gt; dplyr::bind_rows() |&gt; dplyr::filter(status == &quot;ACCEPTED&quot; &amp; matchtype == &quot;EXACT&quot;) ## usagekey scientificname rank status matchtype ## 1 2878688 Quercus robur L. species ACCEPTED EXACT ## 2 2880539 Quercus rubra L. species ACCEPTED EXACT ## 3 2686157 Larix kaempferi (Lamb.) Carrière species ACCEPTED EXACT ## 4 5285637 Pinus sylvestris L. species ACCEPTED EXACT ## 5 5331916 Betula pendula Roth species ACCEPTED EXACT ## canonicalname confidence kingdom phylum order family genus ## 1 Quercus robur 97 Plantae Tracheophyta Fagales Fagaceae Quercus ## 2 Quercus rubra 97 Plantae Tracheophyta Fagales Fagaceae Quercus ## 3 Larix kaempferi 97 Plantae Tracheophyta Pinales Pinaceae Larix ## 4 Pinus sylvestris 98 Plantae Tracheophyta Pinales Pinaceae Pinus ## 5 Betula pendula 99 Plantae Tracheophyta Fagales Betulaceae Betula ## species kingdomkey phylumkey classkey orderkey familykey genuskey ## 1 Quercus robur 6 7707728 220 1354 4689 2877951 ## 2 Quercus rubra 6 7707728 220 1354 4689 2877951 ## 3 Larix kaempferi 6 7707728 194 640 3925 2686156 ## 4 Pinus sylvestris 6 7707728 194 640 3925 2684241 ## 5 Betula pendula 6 7707728 220 1354 4688 2875008 ## specieskey synonym class acceptedusagekey note ## 1 2878688 FALSE Magnoliopsida NA &lt;NA&gt; ## 2 2880539 FALSE Magnoliopsida NA &lt;NA&gt; ## 3 2686157 FALSE Pinopsida NA &lt;NA&gt; ## 4 5285637 FALSE Pinopsida NA &lt;NA&gt; ## 5 5331916 FALSE Magnoliopsida NA &lt;NA&gt; If you query your taxonomic information from a taxonomy you should however always check manually, whether the taxa are identified correctly. Not all taxa are present in all taxonomies or names between taxa are so similar that they are confused for the same taxon in the name matching process. 6.8 Creating GUIDs A globally unique identifier (GUID) is a text string of 36 characters that can be used, amongst others, to assign unique identifiers to each data record. It was established as a variation of the Universally Unique Identifier (UUID) but now both are used synonymously. In contrast to other persistent identifiers (PID) that are assigned to the data level, such as DOI, GUIDs do not have to be issued by a central authority but can be created individually by using specific algorithms or generators. There are different types of GUID, for more information see here. Structure of GUIDs: A GUID is build as follows: {XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX} where each X is a hexadecimal digit, meaning a number from 0 to 9 or a letter from A to F. This structure ensures an extremely low probability of duplication, making each GUID globally unique. To assign GUIDs to your data you can: use an online GUID generator, for example https://www.uuidgenerator.net use the R package uuid(Urbanek &amp; Ts’o (2023)) and its function UUIDgenerate() 6.9 Cross-referencing other resources in the data Some contents or entities in your data might also be mentioned in another source or other sources might contain related records. To provide context to your data and to properly embed the data into related resources, your data should refer to these by cross-referencing, for example, their unique identifier. Identifiers for the same entity or concept are likely different in different resources and therefore the different identifiers should be mapped to one another. The simplest way of mapping is through a comma- or tab-separated file where each row describes a single entity and the columns provide the identifiers in each dataset. There are other ways of doing this and a more detailed description of these mapping methods can be found here. Crickets: The European field cricket (Gryllus campestris Linnaeus, 1758) is listed in several taxonomies with different identifiers. To integrate these different sources, links can be described by creating a mapping file (e.g., CSV, see below), where the row represents the species, and columns contain the identifiers from each taxonomy. scientificName,GBIF_ID,COL_ID,BOL_ID &quot;Gryllus campestris (Linnaeus, 1785)&quot;,1716462,9GQRY,208632 Note: All IDs are the real IDs from the respective taxonomies but we have not done this mapping in the cricket data ourselves (i.e., it cannot be found in the code on GitHub). This structured approach ensures that data referencing the same entity across different systems is linked effectively, facilitating data integration and analysis. References Chamberlain, S., Szoecs, E., Foster, Z., &amp; Arendsee, Z. (2022). Taxize: Taxonomic information from around the web. https://docs.ropensci.org/taxize/ Urbanek, S., &amp; Ts’o, T. (2023). Uuid: Tools for generating and handling of UUIDs. https://www.rforge.net/uuid Wickham, H., François, R., Henry, L., Müller, K., &amp; Vaughan, D. (2023). Dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr "],["07-structure.html", " 7 Structuring your standardised data 7.1 Relational database 7.2 Darwin Core Archive 7.3 The Core &amp; its extensions 7.4 Which extension(s)?", " 7 Structuring your standardised data Research data comes in all different shapes and is by far not always structured or stored in an organised way. Reusing this data becomes much more difficult if no clear structure is detectable and easily leads to misinterpretations or misunderstanding of the data. By applying a data standard and thereby standardising your column names, as is done with using Darwin Core, your data already becomes much more structured and understandable, as there have to be defined columns that can correspond with these terms. Darwin Core terms (or other data standards) can however be applied irrespective of the overall structure of your data, so the next step in making your data more FAIR is to also standardise the structure of your data. Specifically tailored to Darwin Core, we present two options to structure your data in the following: a relational database and the Darwin Core Archive. 7.1 Relational database Relational databases are a common format in structuring your data files, where individual files are linked to each other by specific identifiers. This allows for a high flexibility in structuring your data and does as such not require your data to be in a standardised format. In Darwin Core several ID fields exist with which tables/files can be connected to each other, allowing the possibility to structure your standardised data as a relational database (Figure 7.1). In contrast to Darwin Core Archives (see Chapter Darwin Core Archive), relational databases can reduce redundancy in the data and are more flexible in linking information. Figure 7.1: Relational database structure of Darwin Core files. Arrows refer to the identifier through which two files are linked. 7.2 Darwin Core Archive For biodiversity data that uses Darwin Core terms, Darwin Core Archives (DwC-A) are one standard format to organise your data, which is also required when you want to publish data on the aforementioned repositories GBIF and OBIS. There are four main components that together, bundled in one zip-folder, build the Darwin Core Archive: the core file several extension files (optional) an EML file (see later section) a meta XML file (see later section) 7.3 The Core &amp; its extensions The core file is the central element of the archive to which each of the extension files has to be linked. Extensions can only be linked to the core and not to each other, resulting in a so-called star schema (see Figure 7.2). This structure is heavily influenced by GBIF and is widely accepted but it has limited flexibility and partly leads to unnecessary redundancy in the data. A new version of GBIFs data model is currently (per June 2024) under development that aims to tackle this issue but that is not yet available, which is why we stick with the star schema for now. Both core and extensions contain one record per row and are linked by the core identifier. Figure 7.2: Schematic of Darwin Core Archive. The data is structured into a core file and sourrounded by extension files in a star shaped manner. The data is accompanied by two metadata files. 7.3.1 Find the core The first step to build your archive therefore is to choose your core file. Which core to choose depends on the type of your data, which for biodiversity data is likely one of the following: Sampling event data: data contains information on ecological studies or monitoring programs, where the sampling is mostly quantitative, calibrated and according to certain protocols and with a documented sampling effort. → core file: Event Checklist data: data contains information on annotated species checklists, taxonomic catalogues or other information about taxa. → core file: Taxon Occurrence data: data contains information on the evidence of an occurrence of a specific taxon either in nature, a collection or a datasets. This is the case if you have a list of species that occur at a particular place and a specific time but this was not measured by following a certain sampling protocol. → core file: Occurrence 7.3.2 Occurrence core vs. event core It is not always directly clear what the difference between the occurrence and the event core is and when you should use which of them. The most crucial difference between the two is whether the data was collected following a certain protocol with documented sampling effort. If that is the case, you have sampling event data and therefore an event core. If data collection did not follow a sampling protocol, your core will be the occurrence file. For measurements on individual specimens and museum collections in general, occurrence is however mostly the preferred core. Most of the ecological data falls into sampling event data, as was the case for all of the datasets we used in the development of this guide. We will therefore focus on Event as the core file. If you have checklist data, there is already a detailed description on how to transform that into a Darwin Core Archive in the publication of Reyserhove et al. (2020). 7.4 Which extension(s)? Once you have determined your core and it is clear which information goes into it, you have to find one or more extensions if you need to store remaining information. The following extension files are possible: Event (if event is not the core) Occurrence (if occurrence is not the core) Taxon (if taxon is not the core) (extended) measurement or fact: The measurementOrFact file contains information on all the actual measurement values or facts that have been recorded for the records in the core file. The measurementOrFact file is always an extension file. There is also the extendedMeasurementOrFact extension, which is developed to be used with an event core and creates an additional link between the occurrence extension file and the measurements by including the occurrenceID next to the coreID (i.e. eventID). ResourceRelationship: describes the relationships between resources in a Darwin Core Occurrence, Event, or Taxon Core to resources in an extension or external to the dataset Identification: additional information on multiple identifications for species listed in Occurrence core dnaDerivedData: DNA related data, used either with occurrence or event core Releve: GBIF specific relevé file containing information on vegetation plot survey measurements; to be used together with event core and occurrence extension MaterialSample Amplification: information on DNA amplifications based on a schema from the Global Genome Biodiversity Network (GGBN) HumboldtEcologicalInventory: information on ecological inventories related to event core Based on our use cases with an event core, we recommend occurrence and extended measurement or fact as extension files for sampling event data, because this allows storing taxonomic information for every event, as well as all measured values. References Reyserhove, L., Desmet, P., Oldoni, D., Adriaens, T., Strubbe, D., Davis, A. J. S., Vanderhoeven, S., Verloove, F., &amp; Groom, Q. (2020). A checklist recipe: making species data open and FAIR. Database, 2020, baaa084. https://doi.org/10.1093/database/baaa084 "],["07.1-arrange-DwCA.html", "7.5 Arrange Darwin Core terms in core and extension files", " 7.5 Arrange Darwin Core terms in core and extension files In the Darwin Core Archive the Darwin Core terms are sorted into the different files, each having terms that are required and some that we highly recommend to use, while others are optional additions. Generally, every extension file needs to contain the core identifier of the core file. For example, if you have an event core, the core ID is the field “eventID” and every extension file therefore needs to have the column eventID, linking records to the records in the core. In the following we provide an overview which terms can be included in which file. This is based on the documentation in the GBIF schema repository. Additionally we indicate which terms we view as required to have in each file and which we strongly recommend, adapted from the data quality requirements of GBIF for each data type (Sampling event, occurrence, checklist data). 7.5.1 General terms (Terms of class Record-level) There are terms that can always be included in the core-file, independent of what the core is. Those are record-level terms, which are generic and can apply to any type of records in the data. The content of these terms can be considered metadata, which is why it is likely that you do not have corresponding columns in your data yet but need to create new columns. Many of these terms are from the Dublin Core namespace and few come with a fixed vocabulary to use to fill them. The following terms can be useful to add, while none of them are required: type: to be filled with one of the terms in the DCMI type vocabulary language: use controlled vocabulary, such as the ISO language code licence: see section licencing rightsHolder accessRights bibliographicCitation: should provide clear information on how to cite the resource itself references institutionCode institutionID datasetID basisOfRecord: best practise is to use controlled vocabulary, such as the names of the Darwin Core classes: MaterialEntity, PreservedSpecimen, FossilSpecimen, LivingSpecimen, MaterialSample, Event, HumanObservation, MachineObservation, Taxon, Occurrence, MaterialCitation 7.5.2 Event file The event file can include the following terms: all terms of the class Event all terms of the class Location all terms of the class GeologicalContext Required: eventID eventDate samplingProtocol sampleSizeValue &amp; sampleSizeUnit Recommended: parentEventID (if applicable) samplingEffort decimalLatitude &amp; decimalLongitude &amp; geodeticDatum coordinateUncertaintyInMeters countryCode 7.5.3 Occurrence file All of the terms that can be included in the event file can also be included in the occurrence file. The occurrence file is however more flexible and additionally allows for more terms. You can additionally include: all terms of the class Occurrence all terms of the class Organism all terms of the class Taxon all terms of the class Identification Required: scientificName occurrenceStatus basisOfRecord occurrenceID Recommended: taxonRank kingdom (or other higher taxonomy) decimalLongitue &amp; decimalLatitude &amp; geodeticDatum coordinateUncertaintyInMeters countryCode individualCount or organismQuantity &amp; organismQuantityType 7.5.4 Taxon file The taxon file only contains the record-level terms and all terms of the class taxon. Required: taxonID scientificName taxonRank Recommended: kingdom (or other higher taxonomy) parentNameUsageID acceptedNameUsageID 7.5.5 MeasurementOrFact file The MeasurementOrFact file is linked to the core by including the coreID and besides that can only include the terms of the class MeasurementOrFact, while no terms of other classes should be included. The extendedMeasurementOrFact extension additionally includes the occurrenceID and three ID fields: measurementTypeID, measurementValueID, and measurementUnitID. These fields are not part of the Darwin Core namespace but refer to OBIS. "],["08-metadata.html", " 8 Standardise and structure your metadata 8.1 Metadata standards (for biodiversity data) 8.2 Tools to help you 8.3 Our choice 8.4 EML terms", " 8 Standardise and structure your metadata Once you have described your data and assigned metadata to it (see section 4), you can make your data more FAIR by also mapping your metadata to a standard format. Similar to data standards described in a previous section, there are also several metadata standards available to do so. The distinction between metadata standards and data standards is however often not that clear and Darwin Core sometimes is also referred to as a metadata standard. 8.1 Metadata standards (for biodiversity data) As with the data standards, a wide range of metadata standards exists, partly tailored to specific research domains. For ecological data, a widely used metadata standard, for example by OBIS and GBIF, is the Ecological metadata language (EML), which is also the required format for metadata in the Darwin Core Archives. It consists of a set of defined terms used to describe metadata and is compatible with other community standards. EML uses a readable XML markup syntax that balances the machine- and human-readability and is structured in modules making it relatively flexible in describing metadata. The developers provide a good overview of all the modules and EML terms together with important details and definitions in their interactive schema documentation. Dublin Core is a widely adopted, universal metadata standard maintained by the Dublin Core Metadata Initiative(DCMI). It consists of a set of 15 core terms used to describe the basic elements of any resource, online or physical. In addition, Dublin Core contains several dozen properties, classes, data types, and vocabulary encoding schemes that help refine the core description. 8.2 Tools to help you The metadata standard catalog, provides a nice overview of the different metadata standards and is searchable, for example, for specific topics. 8.3 Our choice For all of our datasets we choose EML as the metadata standard because it is very flexible and provides all the necessary terms to describe our metadata. Especially through its terms about spatial, temporal and taxonomic coverage it captures the key elements of our datasets and is much better suited to describe ecological data then Dublin Core, for example. Additionally, as we went with Darwin Core Archives to structure the data, EML was required to use as a metadata standard. 8.4 EML terms EML consists of a wide range of terms of which some are required, while others might just be nice to have depending on what information your metadata contains. In general, terms can have several levels of subterms and we will not cover all of them here (detailed information on every term can be found here and some best practices here). For all terms containing text, the “xml:lang” attribute should be added if the language is not English. Be aware that EML terms use the spelling of American English. Required terms EML comes with only three terms that are required to make the EML document schema valid. &lt;title&gt;: a brief title that provides enough information to distinguish this dataset or resource from others and makes clear what the data is about. The “xml:lang” attribute can be used to specify in which language the title is given, which is especially useful if you want to give alternative titles in different languages. &lt;creator&gt;: information on the person or organisation that created the dataset (subterms see below) &lt;contact&gt;: information on the person or organisation that contacted about the data, e.g., if questions arise (subterms see below) Highly recommended terms The following set of terms is not strictly required by EML but we would highly recommend to provide as much of them as possible, as this increases the richness of the metadata and provides valuable information about the data that is helpful for others to understand and reuse the data. &lt;metadataProvider&gt;: information about the person that provides the metadata in the EML file Creator, contact and metadataProvider The creator and the contact can either be a named person, a certain position that is always staying the same even though the people in the position might change, or an organisation. For each of these cases, different subterms exist in all three terms (including &lt;metadataProvider&gt;) to describe the person/position/organisation accordingly. It is therefore required to choose at least one of the terms &lt;individualName&gt;, &lt;organizationName&gt; or &lt;positionName&gt;. &lt;individualName&gt;: the name of an individual person can be given with three subterms: &lt;salutation&gt;: can be a title (e.g., “Dr.”) or another salutation (“Mr.”/“Ms.”) &lt;givenName&gt; &lt;surName&gt; &lt;organisationName&gt; &lt;positionName&gt; &lt;address&gt; &lt;city&gt; &lt;postalCode&gt; &lt;country&gt; &lt;electronicMalAddress&gt; &lt;userID&gt;: an identifier that links the person or organisation to a directory of individuals, e.g., an ORCID. This term requires the attribute “directory” to state which directory the ID refers to and will generally be an URL (e.g., https://orcid.org) The &lt;contact&gt;, &lt;creator&gt; and &lt;metadataProvider&gt; can be different people/organisations/positions but can also all be the same. For example, if the same person is the &lt;metadataProvider&gt; and &lt;contact&gt; for a dataset, their name and email address can be provided once as part of the &lt;metadataProvider&gt; element, and then their &lt;id&gt; can be used in the &lt;references&gt; element of &lt;contact&gt;. This reduces the chance of introducing error, and allows one to specify that two pieces of information are identical. The metadata provider is assigned with “id-1”. As the contact for the dataset is the same person, it can directly be referred to “id-1” instead of adding the full personal information again. &lt;metadataProvider id=&quot;id-1&quot; scope=&quot;document&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mary&lt;/givenName&gt; &lt;surName&gt;Shelley&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Netherlands Institute of Ecology (NIOO-KNAW)&lt;/organizationName&gt; &lt;electronicMailAddress&gt;m.shelley@nioo.knaw.nl&lt;/electronicMailAddress&gt; &lt;/metadataProvider&gt; &lt;contact&gt; &lt;references&gt;id-1&lt;/references&gt; &lt;/contact&gt; &lt;language&gt;: provides the language the resource is written in and can either be a well-known language name or ideally, a ISO language code &lt;abstract&gt;: a short summary of the data that includes basic information to give an idea what the data is about &lt;para&gt;: Text is stored in this subterm, which can either directly have a text string or a set of subterms consisting of markup tags allowing for formatting of the text. &lt;keywordSet&gt;: can either be one or many keywords describing the resource &lt;keyword&gt;: each keyword is stored separately in this subterm as text. &lt;keywordThesaurus&gt;: If the keywords are part of a thesaurus, it can be specified which thesaurus they are derived from. &lt;coverage&gt;: Describes the spatial, temporal and taxonomic coverage of the resource. &lt;temporalCoverage&gt;: information on what time span is covered in the data. Use ISO 8601 (i.e., YYYY-MM-DD) for date and time information. Needs either one of the subterms: &lt;singleDateTime&gt;: describes a single date and time either by describing a calendar date or a geologic date, using the subterms: &lt;calendarDate&gt; &lt;time&gt; &lt;alternativeTimeScale&gt;: can describe alternative time scale, e.g., geological date, with a range of subterms &lt;rangeOfDates&gt;: allows to describe a time range by specifying start and end date (with the same subterms as for singleDateTime). Can be repeated if there are several time ranges that need to be described. &lt;beginDate&gt; &lt;endDate&gt; &lt;geographicCoverage&gt;: information about the spatial extent of the data that should be given as text and coordinates with the subterms: &lt;geographicDescription&gt;: short text explanation of the spatial extent &lt;boundingCoordinates&gt;: stating the four edges of a bounding box, by giving: &lt;westBoundingCoordinate&gt; &lt;eastBoundingCoordinate&gt; &lt;northBoundingCoordinate&gt; &lt;southBoundingCoordinate&gt; &lt;taxonomicCoverage&gt;: information about the taxa covered in the data &lt;generalTaxonomicCoverage&gt;: text description of the taxa included in the data &lt;taxonomicClassification&gt;: taxonomic classification of the range of taxa included in the data, specified through a set of subterms: &lt;taxonRankName&gt;: taxonomic level of information &lt;taxonRankValue&gt;: name of the taxonomic rank being described &lt;commonName&gt; &lt;taxonID&gt;: identifier of the taxon from a certain provider that has to be specified with the attribute “provider” containing a URI (e.g., provider = “https://eol.org” if the taxon ID is retrieved from EOL) &lt;maintenance&gt;: information on the frequency with which the data is updated and whether data collection is still ongoing &lt;maintenanceUpdateFrequency&gt;: needs to be filled with a term of the EML MaintUpFreqType, for example: annually, asNeeded, biannually, daily, irregular or unknown &lt;description&gt;: text description of maintenance stated with the &lt;para&gt; subterm &lt;methods&gt;: stepwise information on methods for data collection &lt;methodStep&gt;: can be repeated for each method step and includes a range of subterms to specify instruments, software, protocols, descriptions, and citations of the methods &lt;pubDate&gt;: publication date of the resource in ISO 8601 (i.e., YYYY-MM-DD) &lt;licensed&gt;: information on the licence of the data and how it can be used by others, for information in which licence to assign (see section licencing) &lt;licenseName&gt;: official name of the licence &lt;url&gt;: URL referring to the licence (e.g., https://creativecommons.org/licenses/by/4.0/) Other terms There are plenty of other terms that can be suitable to use for your data. The following are just some examples, for a full overview check the EML schema documentation. &lt;project&gt;: broader background information on the project in which the data was collected, needs to have the subterms: &lt;title&gt; &lt;personnel&gt; Optional subterms: &lt;abstract&gt; &lt;funding&gt; &lt;studyAreaDescription&gt; &lt;intellectualRights&gt;: information on the intellectual property rights as text using &lt;para&gt; subterm &lt;alternateIdentifier&gt; &lt;additionalInfo&gt;: any information that cannot be captured by the remaining terms; filled with text using &lt;para&gt; subterm &lt;introduction&gt;: overview of background and context of the dataset, similar to an introduction of a journal article &lt;usageCitation&gt;: citation to articles or other products where the data is used, consists of a range of subterms "],["08.1-structural-metadata.html", "8.5 Structural metadata - The meta.xml file", " 8.5 Structural metadata - The meta.xml file The meta-file contains the structural metadata and is required by the Darwin Core Archive. It describes how the files are organised (i.e., which file is the core, which are the extensions), and it links every column of each file to its corresponding Darwin Core term by providing its URI (Uniform Resource Identifier). The file format of the meta-file is XML. Bud burst: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;archive xmlns=&quot;http://rs.tdwg.org/dwc/text/&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:xs=&quot;http://www.w3.org/2001/XMLSchema&quot; xsi:schemaLocation=&quot;http://rs.tdwg.org/dwc/text/ http://rs.tdwg.org/dwc/text/tdwg_dwc_text.xsd&quot;&gt; &lt;core encoding=&quot;UTF-8&quot; fieldsTerminatedBy=&quot;,&quot; linesTerminatedBy=&quot;\\n&quot; fieldsEnclosedBy=&quot;&quot; ignoreHeaderLines=&quot;1&quot; rowType=&quot;http://rs.tdwg.org/dwc/terms/Event&quot;&gt; &lt;files&gt; &lt;location&gt;budburst_event.csv&lt;/location&gt; &lt;/files&gt; &lt;id index=&quot;0&quot;/&gt; &lt;field index=&quot;0&quot; term=&quot;http://rs.tdwg.org/dwc/terms/eventID&quot;/&gt; &lt;field index=&quot;1&quot; term=&quot;http://rs.tdwg.org/dwc/terms/parentEventID&quot;/&gt; &lt;field index=&quot;2&quot; term=&quot;http://rs.tdwg.org/dwc/terms/samplingProtocol&quot;/&gt; &lt;field index=&quot;3&quot; term=&quot;http://rs.tdwg.org/dwc/terms/sampleSizeValue&quot;/&gt; &lt;field index=&quot;4&quot; term=&quot;http://rs.tdwg.org/dwc/terms/sampleSizeUnit&quot;/&gt; &lt;field index=&quot;5&quot; term=&quot;http://rs.tdwg.org/dwc/terms/eventDate&quot;/&gt; &lt;field index=&quot;6&quot; term=&quot;http://rs.tdwg.org/dwc/terms/year&quot;/&gt; &lt;field index=&quot;7&quot; term=&quot;http://rs.tdwg.org/dwc/terms/month&quot;/&gt; &lt;field index=&quot;8&quot; term=&quot;http://rs.tdwg.org/dwc/terms/day&quot;/&gt; &lt;field index=&quot;9&quot; term=&quot;http://rs.tdwg.org/dwc/terms/country&quot;/&gt; &lt;field index=&quot;10&quot; term=&quot;http://rs.tdwg.org/dwc/terms/countryCode&quot;/&gt; "],["08.2-XML-in-R.html", "8.6 Tools to help you", " 8.6 Tools to help you Both the EML and the meta file are XML files. XML stands for extensible Markup Language and is a human- and machine-readable format to store and transport data. The general structure of XML files is a hierarchical tree consisting of a root element and an unlimited number of sub-elements of different levels, as well as attributes and text. For an extensive description on how to build XML files, see the tutorial of the W³ schools. To create the two XML files needed for the Darwin Core Archive, you do not have to be an expert in XML. In R, there are several packages that facilitate the creation of XML and even more specific, EML files, for example emld (Boettiger et al. (2020)), EML (Boettiger &amp; Jones (2022)) and xml2 (Wickham, Hester, et al. (2023)). Additionally, you can find a nice overview over packages and helpful websites to build EML files in R on the website of the LivingNorwayR package. 8.6.1 EML.xml file The key for creating the EML.xml file are lists. The content of every EML term has to be stored as a character in a list. If terms consist of subterms, they have to be stored in nested lists within the list of the parent term (see example box below). For a full example, you can also browse our GitHub repository. Bud burst: # Provide keywords and their thesaurus fo the EML term &quot;keywordSet&quot; keywordSet &lt;- list(list(keyword = list(&quot;bud burst&quot;, &quot;trees&quot;, &quot;ecology&quot;, &quot;plant phenology&quot;), keywordThesaurus = &quot;envThes&quot;), list(keyword = &quot;oak&quot;, keywordThesaurus = &quot;GEMET&quot;)) keyword and keywordThesaurus are the EML subterms of keywordSet, which looks like this in XML: &lt;keywordSet&gt; &lt;keyword&gt;bud burst&lt;/keyword&gt; &lt;keyword&gt;trees&lt;/keyword&gt; &lt;keyword&gt;ecology&lt;/keyword&gt; &lt;keyword&gt;plant phenology&lt;/keyword&gt; &lt;keywordThesaurus&gt;envThes&lt;/keywordThesaurus&gt; &lt;/keywordSet&gt; &lt;keywordSet&gt; &lt;keyword&gt;oak&lt;/keyword&gt; &lt;keywordThesaurus&gt;GEMET&lt;/keywordThesaurus&gt; &lt;/keywordSet&gt; The lists of all terms have to be combined in a final list that can then be converted into an XML file through the function write_eml() of the EML package. Some EML terms require XML attributes (see EML terms). These can be assigned using the xml2 package by first identifying all nodes for which the attribute should be set (xml2::xml_find_all()) and then setting the attribute with xml2::xml_set_attr()). Bud burst: The following example shows how the final list is converted into the EML file and how to set the attribute “provider” for the taxonID. # Combine all components in one list eml &lt;- list(dataset = list(title = title, creator = creator, pubDate = publication_date, language = language, abstract = abstract, keywordSet = keywords, licensed = licensed, coverage = coverage, contact = contact_person, methods = methods, maintenance = maintenance, intellectualRights = intellectualRights), packageId = packageId, system = &quot;uuid&quot;) # Write EML file EML::write_eml(EML, file = &quot;cricket_EML.xml&quot;) # Add attributes for specific nodes ------------------------------- # Read EML file as XML file EML &lt;- xml2::read_xml(&quot;cricket_EML.xml&quot;) # Identify all taxonId nodes for which attribute shall be set taxonId_node &lt;- xml2::xml_find_all(EML, xpath = &quot;//taxonId&quot;) # Set &quot;provider&quot; attribute for taxonId nodes xml2::xml_set_attr(taxonId_node, attr = &quot;provider&quot;, value = &quot;https://www.gbif.org/&quot;) Once you set all required attributes, you should check whether your EML file is schema-valid. This can be done with the function eml_validate() of the emld package. 8.6.2 meta.xml file In contrast to the EML file, where the metadata is specific to the dataset and has to be filled in by hand, the meta file always consists of the same content only depending on the file and column names of the individual Darwin Core Archive. The root element &lt;archive&gt; consists of the subelements &lt;core&gt; and &lt;extension&gt; (once for every extension file). Within these elements, there is a &lt;field&gt; element for every column, linking to the URI of the corresponding Darwin Core term. For a more detailed description of the components of the meta.xml file see the Darwin Core text guide. As the meta.xml is always structured in the same way and the contents can directly be derived from the column headers in the Darwin Core Archive files and do not need to be filled individually, the creation of the meta.xml file can be automated. Here you can find a function that you can use to do this in R. Crickets: A shortened example of the meta.xml file of the cricket data. &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;archive xmlns=&quot;http://rs.tdwg.org/dwc/text/&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:xs=&quot;http://www.w3.org/2001/XMLSchema&quot; xsi:schemaLocation=&quot;http://rs.tdwg.org/dwc/text/ http://rs.tdwg.org/dwc/text/tdwg_dwc_text.xsd&quot;&gt; &lt;core encoding=&quot;UTF-8&quot; fieldsTerminatedBy=&quot;,&quot; linesTerminatedBy=&quot;\\n&quot; fieldsEnclosedBy=&quot;&quot; ignoreHeaderLines=&quot;1&quot; rowType=&quot;http://rs.tdwg.org/dwc/terms/Event&quot;&gt; &lt;files&gt; &lt;location&gt;crickets_event.csv&lt;/location&gt; &lt;/files&gt; &lt;id index=&quot;0&quot;/&gt; &lt;field index=&quot;0&quot; term=&quot;http://rs.tdwg.org/dwc/terms/eventID&quot;/&gt; &lt;field index=&quot;1&quot; term=&quot;http://rs.tdwg.org/dwc/terms/samplingProtocol&quot;/&gt; &lt;field index=&quot;2&quot; term=&quot;http://rs.tdwg.org/dwc/terms/sampleSizeValue&quot;/&gt; &lt;field index=&quot;3&quot; term=&quot;http://rs.tdwg.org/dwc/terms/sampleSizeUnit&quot;/&gt; &lt;field index=&quot;4&quot; term=&quot;http://rs.tdwg.org/dwc/terms/eventDate&quot;/&gt; &lt;field index=&quot;5&quot; term=&quot;http://rs.tdwg.org/dwc/terms/year&quot;/&gt; [...] &lt;/core&gt; &lt;extension encoding=&quot;UTF-8&quot; fieldsTerminatedBy=&quot;,&quot; linesTerminatedBy=&quot;\\n&quot; fieldsEnclosedBy=&quot;&quot; ignoreHeaderLines=&quot;1&quot; rowType=&quot;http://rs.iobis.org/obis/terms/ExtendedMeasurementOrFact&quot;&gt; &lt;files&gt; &lt;location&gt;crickets_extendedmeasurementorfact.csv&lt;/location&gt; &lt;/files&gt; &lt;coreid index=&quot;1&quot;/&gt; &lt;field index=&quot;0&quot; term=&quot;http://rs.tdwg.org/dwc/terms/measurementID&quot;/&gt; &lt;field index=&quot;2&quot; term=&quot;http://rs.tdwg.org/dwc/terms/measurementType&quot;/&gt; &lt;field index=&quot;3&quot; term=&quot;http://rs.tdwg.org/dwc/terms/measurementValue&quot;/&gt; &lt;field index=&quot;4&quot; term=&quot;http://rs.tdwg.org/dwc/terms/measurementUnit&quot;/&gt; [...] References Boettiger, C., &amp; Jones, M. B. (2022). EML: Read and write ecological metadata language files. https://docs.ropensci.org/EML/ Boettiger, C., Jones, M. B., &amp; Mecum, B. (2020). Emld: Ecological metadata as linked data. https://docs.ropensci.org/emld/ Wickham, H., Hester, J., &amp; Ooms, J. (2023). xml2: Parse XML. https://xml2.r-lib.org/ "],["09-fully-interoperable.html", " 9 How to reach full interoperability", " 9 How to reach full interoperability As mentioned before (see note box), you cannot reach the full score in the evaluation of your data by going through this guide alone. This is because we do not cover the implementation of the FAIR principle I1: (Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation. We think that data scientists rather than ecologists are better equipped at using, for example, RDF (Resource description framework) or OWL (Web ontology language) for knowledge representation and transforming the data accordingly. If you are however interested in making your data fully FAIR and cover principle I1, the following resources can be a good starting point: Open data support - Training Module 1.3: Introduction to RDF &amp; SPARQL Darwin Core RDF guide Cambridge semantics - Semantics university: Learn RDF W3C Web Ontology Language guide "],["10-glossary.html", " 10 Glossary", " 10 Glossary API An Application Programming Interface (API) defines a way of communication between two applications based on requests and responses, which allows data exchange between these applications. More information: https://www.neonscience.org/about/faq/what-api Core file The core file is the central element of the Darwin Core Archive to which all other files are linked. Different files are possible to be used as the core depending on the type of the data. While checklist data requires a taxon file as the core, occurrence data requires an occurrence file and sampling-event data an event file. More information: https://ipt.gbif.org/manual/en/ipt/latest/dwca-guide Darwin Core (DwC) Darwin Core is a stable data standard maintained by TDWG (Biodiversity Information Standards) tailored to describe biodiversity data by using a defined library of terms. It is an extension of the Dublin Core Metadata Initiative and facilitates sharing of biodiversity data from various sources. Every term has a clear definition and a URL, which can be used irrespective of technology, e.g., XML or RDF. Source: https://www.tdwg.org/standards/dwc/ Darwin Core Archive Darwin Core Archives are a way of structuring a dataset that uses the Darwin Core standard. It consists of a core file and a number of extension files, as well as two files describing the metadata of the dataset. The first is called meta.xml and is a XML file describing the structural metadata, i.e., the organisation of the files and maps each column name (i.e., Darwin Core term) to its identifier. The second metadata file is the EML file containing the descriptive and administrative metadata using the metadata standard Ecological metadata language. More information: https://ipt.gbif.org/manual/en/ipt/latest/dwca-guide Data(set) Data is defined as information that is collected to be used for decision making or refers to information in a digital format that can be used by computers. Dataset refers to a collection of data. The two terms are not always used consistently across sources. Data is often used to describe the collected/recorded data itself (in contrast to metadata that describes it). Dataset is often used to describe all files of collected data, or the combination of data and metadata. Sources: https://dictionary.cambridge.org/dictionary/english/data https://dictionary.cambridge.org/dictionary/english/dataset Data dictionary A data dictionary is a collection of names, definitions, and attributes about data elements (e.g., columns in a table) in a dataset. It provides guidance on interpretation, accepted meanings and representation (i.e., format) so that others can easily understand your data. Data dictionaries often include a definition and/or description, the data type (e.g., integer, string), the measurement units (e.g., metre), the allowable values (e.g., 1-10) and what they mean (e.g., 0 = absent, 1 = present). More information: https://help.osf.io/article/217-how-to-make-a-data-dictionary Data lifecycle The data lifecycle describes the stages that the data goes through, starting with planning of data collection, followed by the data collection or acquisition. Afterwards the data is processed to then be used and analysed. After that, the data should be properly stored and curated to be preserved in the long run, which finally leads to publishing and sharing the data, allowing others to reuse it, with which the cycle can start again. There are however different versions of the data lifecycle, differing slightly in the respective steps. - More information: https://www.britishecologicalsociety.org/wp-content/uploads/Publ_Data-Management-Booklet.pdf Data management Data management is the practice of taking care of data throughout its entire lifecycle, from its collection, processing and use to its storage and sharing. Throughout the whole lifecycle, good data management is crucial to ultimately enhance the reusability of the data for yourself or others. - More information: https://www.britishecologicalsociety.org/wp-content/uploads/Publ_Data-Management-Booklet.pdf Dublin Core Dublin Core is a metadata standard consisting of a set of 15 terms used to describe metadata. The terms themselves consist of several dozen properties, classes, data types, and vocabulary encoding schemes and are maintained by the Dublin Core Metadata Initiative (DCMI). More information: https://www.dublincore.org/specifications/dublin-core/dcmi-terms/ EML EML, the Ecological metadata language, is a metadata standard commonly used for ecological data. It consists of a set of terms with which metadata can be described. EML uses a readable XML markup syntax that balances the machine and human readability and is structured in modules making it relatively flexible in describing metadata. More information: https://eml.ecoinformatics.org/schema/ Event Event is a Darwin Core class describing an action that occurs at some location during some time. In the context of Darwin Core archives events can be stored in an event-file that stores information on the time and place of the events, but also more administrative information, such as the data collector, sampling effort or sample size. Sources: https://dwc.tdwg.org/terms/#event https://rs.gbif.org/core/dwc_event_2024-02-19.xml Extension file Extension files are a file type that can exist in Darwin Core Archives next to the core file. There can be an unlimited number of extensions, each holding a different kind of information and each row within an extension file directly links to a record in the core file through the core identifier. Commonly used extensions for biodiversity data collected on living organisms are occurrence, taxon or measurement or fact. More information: https://ipt.gbif.org/manual/en/ipt/latest/dwca-guide FAIR FAIR stands for Findable, Accessible, Interoperable and Reusable and comes with 15 guiding principles developed by Wilkinson et al. (2016). Findability means that the metadata (and the data) can easily be found by humans and computers and that machine-readable metadata allows for automatic discovery of the data by machines. Additionally, it is clearly stated how the user can access the data and whether, for example, authorisation or authentication are required (Accessibility). Interoperability means that the data is fully compatible with other data making it integrable with other data resources and allows to incorporate it into workflows or applications. In easier words this means that data resources should ‘speak the same language’ to be used together. The ultimate goal of the FAIR concept is to make the data reusable (Reusability), which means that there is proper annotation in the form of metadata that allows users (and machines) to understand the data and correctly interpret it. More information: https://www.go-fair.org/fair-principles/ GBIF GBIF, the global biodiversity information facility, is a data infrastructure and international network providing open access to biodiversity data. Source: https://www.gbif.org/ IRI, URI and URL Uniform resource identifiers (URI) are a standardised way to identify resources of information on the internet. One of the most common forms of the URI is the URL, the uniform resource locator, often also called a web address. The IRI (Internationalized Resource Identifier) is an internationalised expansion of the URI supporting a wider range of characters besides the Latin alphabet. While URL is location oriented and describes the physical location of a resource on the internet, the URI is identifier oriented and refers to the resource itself. They are however often treated the same way. Sources and more information: GBIF (2011). A Beginner’s Guide to Persistent Identifiers, version 1.0. Released on 9 February 2011. Authors Kevin Richards, Richard White, Nicola Nicolson, Richard Pyle, Copenhagen: Global Biodiversity Information Facility, 33 pp, accessible online at http://links.gbif.org/persistent_identifiers_guide_en_v1.pdf https://www.w3.org/2001/Talks/0912-IUC-IRI/paper.html MeasurementOrFact Measurement or fact is a Darwin Core class describing measurement values and or facts about a resource. It can also be an extension file of the Darwin Core archive storing the measurements or facts belonging to the events or occurrences described in the core file. Source: https://dwc.tdwg.org/terms/#measurementorfact Metadata Metadata contains information about other data and provides descriptions that make the data easier to understand and reuse. Structural metadata contains information about how the data is organised, what variables mean and how files relate to one another, while administrative metadata provides information about ownership, preservation and rights and licences of the data. Thirdly, descriptive metadata describes the background of the data, such as how, where and when it was collected, who is responsible and what contents the data cover. More information: https://www.w3.org/TR/dwbp/#metadata Meta.xml The meta.xml file is a component of the Darwin Core Archive and contains its structural metadata in an XML file. The file describes how the data files in the archive are linked to each other and provides for each column the identifier of the corresponding Darwin Core term. More information: https://ipt.gbif.org/manual/en/ipt/latest/dwca-guide Mobilisation Transferring data that is stored locally to an online repository and thereby enabling its reuse by others. More information: https://www.gbif.org.nz/mobilising/ OBIS OBIS, the Ocean Biodiversity Information System, is an international databank system for maritime biodiversity and biogeographic data with the objective to provide the largest knowledge base on the diversity, distribution and abundance of marine organisms. Source: https://obis.org/about/ Occurrence Occurrence is a Darwin Core class describing the existence or presence of an organism at a certain place and time. In the context of Darwin Core archives, occurrences are stored in an occurrence-file that contains quantitative information about organisms, as well as taxonomic information. Sources: https://dwc.tdwg.org/terms/#occurrence https://rs.gbif.org/core/dwc_occurrence_2024-02-23.xml Ontology Ontologies provide definitions of terms by defining their relation to other terms in a human interpretable way and thereby create a semantic model of the concepts that are used within a specific research domain. More information: https://www.ontotext.com/knowledgehub/fundamentals/what-are-ontologies/ OWL (Web Ontology Language) OWL is a semantic web language for ontologies in the World Wide Web designed for knowledge representation that is machine-readable and can represent rich and complex knowledge about things and their relations to each other. Source: https://www.w3.org/OWL/ Persistent identifier (PID) A persistent identifier is a unique identification code belonging to a digital resource that ensures persistent identification of the resource, even if the web address of the creator of the resource changes. For detailed information on different PID systems see here. Provenance Provenance is a form of metadata that provides information about the history of the dataset, which includes people and parties that were involved in its creation, as well as how, where and when the data was created. Additionally, the provenance details should inform about previous versions of the data and the change history. More information: https://www.pldn.nl/wiki/Provenance RDF RDF, the resource description framework, is a framework to represent interconnected data on the web. In RDF, objects are linked using semantic triples consisting of two objects and the link between them, often referred to as subject (= object 1), predicate ( = link) and object (= object 2). RDFs are machine readable and allow high interoperability between different data sources. Source: https://www.w3.org/RDF/ Standardisation Converting data and/or metadata to a standard format to increase compatibility with other data. More information: https://www.sisense.com/glossary/data-standardization/ Star schema The star schema is the way the core file and the extension files are organised in the Darwin Core Archive. Each of the extension files directly links to the core file, while they cannot link to one another, creating what is considered to be a star-shape. More information: https://ipt.gbif.org/manual/en/ipt/latest/dwca-guide Taxonomy Taxonomy is a standard way of describing, naming and classifying things in a hierarchical way and in a biological context, classifying living and extinct organisms and the relationship between taxa. Sources: http://purl.obolibrary.org/obo/NCIT_C17469 http://purl.obolibrary.org/obo/GSSO_004259 XML Extensible markup language, XML, is a hardware- and software-independent tool for storing and transmitting data by balancing human-readability with machine-readability. It is the file format used for the two metadata files in the Darwin Core Archive. Source: https://www.w3schools.com/xml/ References Wilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., Silva Santos, L. B. da, Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR guiding principles for scientific data management and stewardship. Scientific Data, 3(1). https://doi.org/10.1038/sdata.2016.18 "],["11-references.html", " 11 References", " 11 References Boettiger, C., &amp; Jones, M. B. (2022). EML: Read and write ecological metadata language files. https://docs.ropensci.org/EML/ Boettiger, C., Jones, M. B., &amp; Mecum, B. (2020). Emld: Ecological metadata as linked data. https://docs.ropensci.org/emld/ Chamberlain, S., Szoecs, E., Foster, Z., &amp; Arendsee, Z. (2022). Taxize: Taxonomic information from around the web. https://docs.ropensci.org/taxize/ GBIF. (2021). GBIF strategic framework 2023-2027. https://doi.org/10.35035/doc-0kkq-0t82 Hansen, K. K., Buss, M., &amp; Haahr, L. S. (2018). A FAIRy tale. Zenodo. https://doi.org/10.5281/ZENODO.2248200 Krans, N. A., Ammar, A., Nymark, P., Willighagen, E. L., Bakker, M. I., &amp; Quik, J. T. K. (2022). FAIR assessment tools: Evaluating use and performance. NanoImpact, 27, 100402. https://doi.org/10.1016/j.impact.2022.100402 Pagano, P., Candela, L., &amp; Castelli, D. (2013). Data interoperability. Data Science Journal, 12(0), GRDI19–GRDI25. https://doi.org/10.2481/dsj.grdi-004 Reyserhove, L., Desmet, P., Oldoni, D., Adriaens, T., Strubbe, D., Davis, A. J. S., Vanderhoeven, S., Verloove, F., &amp; Groom, Q. (2020). A checklist recipe: making species data open and FAIR. Database, 2020, baaa084. https://doi.org/10.1093/database/baaa084 Richards, K., White, R., Nicolson, N., &amp; Pyle, R. (2011). A beginner’s guide to persistent identifiers. https://doi.org/10.35035/MJGQ-D052 Urbanek, S., &amp; Ts’o, T. (2023). Uuid: Tools for generating and handling of UUIDs. https://www.rforge.net/uuid Visser, M. E., &amp; Holleman, L. J. M. (2001). Warmer springs disrupt the synchrony of oak and winter moth phenology. Proceedings of the Royal Society of London. Series B: Biological Sciences, 268(1464), 289–294. https://doi.org/10.1098/rspb.2000.1363 Vogels, J. J., Verberk, W. C. E. P., Kuper, J. T., Weijters, M. J., Bobbink, R., &amp; Siepel, H. (2021). How to restore invertebrate diversity of degraded heathlands? A case study on the reproductive performance of the field cricket gryllus campestris (l.). Frontiers in Ecology and Evolution, 9. https://doi.org/10.3389/fevo.2021.659363 Wickham, H., François, R., Henry, L., Müller, K., &amp; Vaughan, D. (2023). Dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr Wickham, H., Hester, J., &amp; Ooms, J. (2023). xml2: Parse XML. https://xml2.r-lib.org/ Wilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., Silva Santos, L. B. da, Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR guiding principles for scientific data management and stewardship. Scientific Data, 3(1). https://doi.org/10.1038/sdata.2016.18 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
