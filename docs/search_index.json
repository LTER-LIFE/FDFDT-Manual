[["index.html", "A hands-on guide to FAIR and structured ecological data Preface Who this guide is for Why you should use this guide and improve your data management How to read this guide", " A hands-on guide to FAIR and structured ecological data Cherine C. Jantzen &amp; Stefan J.G. Vriend 2024-06-11 Preface This interactive manual aims to provide ecologists and other data owners of ecological data with a hands-on guide on how to make your ecological data more FAIR and tackle the problems that can occur along this process. Who this guide is for This guide is for anybody who has ecological data and wants to improve their data management by making their data more FAIR, especially: ecologists who want to improve their research data management and by that make their data available for reuse by others students who collect data for their thesis and want to make their data reusable for others after their project is finished institutions and organisations that want to provide their employees with some guidance on how to increase their data maturity data stewards who … data managers who … anyone else … Why you should use this guide and improve your data management Data management is the practice of taking care of data throughout its entire lifecycle, from its collection, processing and use to its storage and sharing. Throughout the whole lifecycle, good data management is crucial to ultimately enhance the reusability of the data for yourself or others. For example, by storing your data persistently and implementing versioning, you lower the risk of data loss and have higher traceability of changes and errors. Additionally, by describing, annotating, and organising the data, it becomes better understandable for others, facilitating its reuse and increasing its impact and that of the associated research. How to read this guide This guide is designed in a way that you can only read the chapters that are relevant for improving the maturity of your data. To find these chapters, we provide you with a self-assessment tool that shows to what extent your data already complies with the FAIR principles. Directly based on the outcome of the FAIR assessment you will be provided with a tailored list of the chapters in this guide you could go through to further mature your data. You therefore do not have to read the full guide from top to bottom but should easily be able to only go through the chapters most relevant to you. Disclaimer We want to emphasise here that there is no one-size-fits-all solution and every dataset will bring its own challenges that need individual solutions. We cannot tackle all of these challenges but only give recommendations based on our own work with a range of different ecological datasets. "],["introduction.html", " 1 Introduction 1.1 What is FAIR? 1.2 Extending FAIR 1.3 FAIR is not open 1.4 Why should your data be FAIR? 1.5 FAIR assessment of your data", " 1 Introduction 1.1 What is FAIR? Research data often is stored on local hard drives, not well described or not formatted in a consistent and standard way. This makes it very difficult for others (and often even for the data owner themselves) to reuse the data or reproduce the research, because the data is not straightforward to understand, cannot be interpreted correctly, or, in case of local storage, might be unknown to others. In order to change this and increase the reusability of data, Wilkinson et al. (2015) introduced the FAIR concept together with 15 guiding principles providing guidance on how to implement it. FAIR stands for Findable, Accessible, Interoperable and Reusable, which more concrete means the following: Findable. This means that the metadata (and the data) can be easily found by humans and computers and that machine-readable metadata allows for automatic discovery of the data by machines. Accessible. It is clearly stated how the user can access the data and whether, for example, authorisation or authentication are required. Interoperable. In order to integrate the data with other data and to incorporate it into workflows or applications, the data needs to be fully compatible with other data. In easier words this means that data resources should ‘speak the same language’ to be used together. Reusable. The ultimate goal of the FAIR concept is to make the data reusable, which means that there is proper annotation in the form of metadata that allows users (and machines) to understand the data and correctly interpret it. 1.2 Extending FAIR In our guide we will extend FAIR as it would be strictly defined by the FAIR principles in order to make it workable in practice. This means that we also include the component of structuring your data into the process of FAIRifying it. Structured. Structuring your data, so organising it in a tidy way, can clearly make the data easier to access for others and enhances its understandability. We therefore think that this is a crucial component of increasing the reusability of your data. 1.3 FAIR is not open It is important to stress here that FAIR is not equal to open. While the metadata of your data should be openly available, informing others about the existence of your data, the data itself does not have to be open. If you have reasons for not making the data openly available to everyone (e.g., because it contains sensitive information), this can and should be made clear in the metadata where according licensing information and access rights can be provided. As long as this is guaranteed, restricted or closed data can also be completely FAIR. 1.4 Why should your data be FAIR? Depending on the current maturity of your data, making data FAIR certainly requires some effort, but the benefits are evident both for yourself and for others. Findable and well annotated data facilitates your own reuse of your data in the future and the reuse of your data by others, which simultaneously increases the visibility and the impact of your research. Increasing the interoperability of the data additionally increases the possibilities for collaborations and will in total benefit the scientific community. 1.5 FAIR assessment of your data As stated before, every dataset is different and datasets can vary widely in their level of FAIR. FAIR is a gradient (see also FAIR Data Maturity Model. Specification and Guidelines of RDA) and different datasets can fulfil different criteria of FAIR but reach the same overall level of FAIRness. It is therefore important to first assess which state your data is in and what steps your data need to be improved. There are already a range of FAIR assessment tools (see here for an overview), which score your data but there is much less guidance on how to actually improve the FAIRness of your data. Next to questionnaire-based assessment tools, there are also tools available that automatically assess the FAIRness of your data. Most of them however require the data to be already stored on an online repository, which is not the state at which we want to start with our FAIR assessment. We therefore developed our own tool that provides you with a set of simple questions about four properties of your data: metadata, storage, standards and structure. Metadata is data about your data and contains information about the who, where, what, when and how of data collection allowing another user of your data to understand and reuse it without prior knowledge of the data. Storage is about whether your data is stored persistently and in a way that makes it findable and accessible to others. Standards describe uniform, community-accepted formats in which both data and metadata are stored and which enhance compatibility with other datasets. Structure is about whether your data is organised in a consistent and logical way. Besides the questions about structure, they are all directly based on the FAIR principles. Figure 2 visualises how the data properties in the assessment link to the letters of FAIR. Your answers will provide you with a circle-diagram showing how mature your dataset is in each of the properties and connected to that a list of chapters of this manual you can work through to further improve the maturity of your data. Figure 1.1: Link between data properties and FAIR. The letters on the left stand for the data properties used in the FAIR assessment (metadata, storage, standard, structure). Each plus indicates that the data property improves the maturity of each of the respective letter of FAIR. A blanc space means that this data property has no direct influence on the respective letter of FAIR. It will likely be difficult to reach the full score in all of the properties of your data, but this also not necessarily the aim of this assessment or guide. Every improvement is already a great step! Especially the components that link to interoperability are more difficult to implement, as true interoperability generally is a big challenge within and across disciplines (Pagano et al., 2013). One step to reach interoperability is to use a language for knowledge representation and linked data, such as RDF (Resource Description Framework). However, as this is more the expertise of data or information scientists instead of ecologists, we will not look into these topics in this guide. "],["example-datasets.html", " 2 Example data sets 2.1 Bud burst data 2.2 Beech crop data 2.3 Cricket data 2.4 CLUE field vegetation cover data (i.e., CLUE data)", " 2 Example data sets To illustrate the process of FAIRifying ecological data we will rely on several example data sets we used to develop this guide. These data sets were selected because they cover a broad variety of different structures and different levels of FAIR, ranging from observational to experimental data and from highly unstructured and undocumented data to data that already fulfils some of the FAIR principles. 2.1 Bud burst data The first dataset contains long-term data on the phenology of different tree species. Every year in spring, deciduous trees develop new leaves from their buds at a certain point in time. The timing of this can be crucial for herbivores, such as caterpillars, relying on newly emerging leaves as an important food source. To monitor shifts in tree phenology, the Department of Animal Ecology at NIOO has assessed leaf development every year since 1988 by scoring the stage of bud opening on a fixed scale (cf. Visser &amp; Holleman, 2001). 2.2 Beech crop data A second data set collected by Animal Ecology/NIOO contains data on the amount of beech mast per year since 1976. Low, intermediate and high seed production in beech (Fagus sylvatica) trees alternate periodically over years. Since beech nuts are an important winter food resource for several species, such as passerine birds, seed availability is monitored yearly by collecting beech nuts in determined grids under a set of trees. These nuts are counted and weighted and based on this a so-called beech crop index can be calculated (cf. Perdeck et al., 2000). 2.3 Cricket data The third data set contains experimental data on invertebrates belonging to the publication of Vogels et al., 2021. They tested whether changes in plant N:P ratios following sod-cutting constrain the reproductive potential in the European field cricket (Gryllus campestris). Different phosphorus and liming treatments were applied to vegetation plots in the heathlands of the Veluwe and the plants of these plots used in a feeding experiment in which the reproductive success of the female crickets was measured. 2.4 CLUE field vegetation cover data (i.e., CLUE data) The last example dataset consists of data on vegetation cover collected in a long-term grassland biodiversity field experiment site in the Veluwe, Netherlands, also abbreviated as CLUE fields (= changing land usage, enhancement of biodiversity and ecosystem development). Data collection started in 1996 and was conducted by the Department of Terrestrial Ecology of NIOO-KNAW. On a former agricultural field, experimental plots have been established in a random block design and different sowing treatments (high diversity, low diversity, continued agricultural rotation, and natural succession), as well as soil inoculation treatments have been applied in two different experiments in 1996. In both experiments, percent cover of each occurring plant species is measured annually. The data of both experiments result in two independent datasets, they are however very similar and their FAIRification process was nearly the same so that they will be treated as one dataset, called CLUE data, in the following. "],["workflow.html", " 3 Workflow", " 3 Workflow The general workflow that we present in this guide to make your data more FAIR is shown in Figure ?? and shortly described in the following section. To make your data more FAIR, you should: First, describe your data and thereby gather metadata. Afterwards, you should decide whether your raw data should be mobilised to an online repository. If yes, you have to choose a suitable repository and upload your metadata and data there. Next, you can start to standardise both the data and the metadata. With which of the two you start does not matter, as they can both be structured and standardised separately. For the metadata: First, choose a metadata standard. Next, map the metadata to the selected standard. For the data: First, choose a suitable data standard. Then, map the data to that standard. For further standardisation of your data, use ontologies and biological taxonomies where possible to describe the contents of the data. After standardising the data, you should choose a standardised structure for the data. Restructure your data according to the chosen structure. Once standard and structure are applied, structural metadata can be created that describes how the data is organised in its new form. As a last step your standardised data and metadata should be persistently stored in an online storage. If you have not chosen a repository before (or if you want to store the standardised data elsewhere), you should: choose a suitable repository that accepts your new data structure/format. mobilise the data and the metadata to the chosen repository. Even though this guide is tailored to ecological data, the workflow presented here should be generally applicable to any type of data, only the implementations of each step will differ. In the following chapters, we will go through each of the steps and provide some general background, introduce some options for implementation for ecological data and considerations that can help to choose the best suited option for your data, as well as a short insight into our own choices for the example datasets. Figure 1.1: Diagram of the workflow presented in this guide to FAIRify data. "],["describe-your-data---metadata-content.html", " 4 Describe your data - Metadata (content) 4.1 Licencing", " 4 Describe your data - Metadata (content) The first step before you store your data properly should always be to describe and annotate it. This type of description is called metadata and basically provides information about your dataset that helps to understand it. We can differentiate into administrative metadata that, for example, contains information about the authors, the contents, coverage and the maintenance of the described data and structural metadata, which is giving information on how the data set is organised and how single data files connect to each other. Many repositories require a minimal set of metadata for uploading data, which can either directly be entered in a fill-in form provided by the repository or added in a separate file that is uploaded together with the data. The simplest form of metadata is a README text file containing some basic information on what the data is about. There are however no regulations on what information a README should contain and in reality, the extent of it varies a lot. In general, the richer the data is described the better. Based on best practices of writing readme files and in terms of standardising the metadata in a later step, we recommend to gather the following information about your data: About the dataset: Title of the dataset Short description what contents each data file contains Short description of the methods used for data collection Responsible parties, including who created the data and who can be contacted about the data When was the data collected? Which time span is covered? Where has the data been collected? Licencing information: How can others use your data? To find out more about different licences, see section Licencing. Changes in the data or updates should ideally also be recorded in the README About individual data files: What do columns mean? Which units belong to each column? If you include personal information, like email addresses, of other people in the metadata that is made available online later on, it should always be asked for permission beforehand. 4.1 Licencing If you deposit your data online it is important to communicate to potential users what permissions they have and arrange a legal agreement in the form of a licence. A commonly used set of licences are the copyright licences of [Creative Commons (CC)](https://creativecommons.org/share-your-work/cclicenses/. Scientific datasets most often are published under a CC BY licence, but Creative Commons also offers other licensing options, as stated in Table 4.1. They also offer an interactive tool (License chooser) that can help you to choose the best suited licence for your data. Table 4.1. Creative commons licences and their meanings. Licence What can users do with your data? CC BY users can use, distribute and build upon the data but have to give credits to the creator CC BY-NC users can use, distribute and build upon the data but have to give credits to the creator and data cannot be used commercially CC BY-SA users can use, distribute and build upon the data, they have to give credits to the creator and adaptations need to be shared under the same terms CC BY-NC-SA users can use, distribute and build upon the data, they have to give credits to the creator, adaptations need to be shared under the same terms and data cannot be used commercially CC BY-ND users are not allowed to adapt or modify the data, but can copy or distribute it if they give credits to the creator CC BY-NC-ND users are not allowed to adapt or modify the data, but can copy or distribute it if they give credits to the creator and data cannot be used commercially CC0 (“CC Zero”) copyrights are given up and work is put into the public domain Note: Carefully consider which licence to choose. It cannot be revoked and every user of the data needs to comply with the licence’s conditions of use, even if the data is no longer distributed. "],["storing-your-data.html", " 5 Storing your data 5.1 Choosing a repository 5.2 Repositories to store biodiversity data 5.3 Tools to help you 5.4 Our choice", " 5 Storing your data Data is often stored locally on personal computers or external hard drives which makes the data unavailable to others and increases the risk of data loss. These drawbacks can be overcome by mobilising your data to an online repository. Mobilising raw or standardised data? &lt;Mobilising raw data and benefits. Mobilising standard data and benefits. Metadata can “always” be mobilised.&gt; 5.1 Choosing a repository The range of existing repositories is very large and while some are domain specific, there are other repositories that accept data from different disciplines. To find the repository that is best suited for your data it can be helpful to take the following points into consideration: Which data formats are accepted? Depending on the format your data is stored in, not every repository will accept your data. Many repositories however accept a wide range of formats but have preferred formats to upload your data in, as these formats have best long-term guarantees in sustainability and accessibility (see here for an example). What repository is commonly used in your organisation, community or discipline? Check which repositories your direct colleagues in your organisation, or other data providers in your community or field of research use. Using such repositories will likely make your data better linked to other datasets and found more easily by others in your discipline. Is a persistent identifier assigned? To ensure that your dataset can always be found, it is important that your data is associated with a persistent identifier (PID). Most repositories assign a PID when you submit your data, which is preferred over repositories in which you would need to assign the PID yourself. Different repositories also use different identifier systems which can be another selection criterion if you have personal preferences for a specific identifier system. How can the dataset be accessed? Do users have to download the dataset directly from the repositories website or is there a more automated option available, such as an API (Application Programming Interfaces)? Can the data be easily accessed? Is metadata stored persistently even when the data is no longer available? The metadata of your dataset should be stored persistently so that users can still find out about the existence of your data, even if it is no longer available on the repository itself. Do you want version control? Not all repositories provide version control, meaning that changes in the data and reuploads are not tracked but only the most recent version is available. If this is an important component for you, you should make sure the chosen repository provides this option. At re3data.org, for example, you can directly filter for repositories with versioning. Do you retain custody of your data or is it transferred to the repository? There are repositories that obtain the custody of your data once it is uploaded there, meaning that the data management is done by the repository and your possibilities in managing the data are then very limited. This can be beneficial if you do not want to have further responsibility for your data, but if you want to still be able to manage the data yourself after uploading it, a repository where the custody stays with you will be better suited. 5.2 Repositories to store biodiversity data Here we want to provide you with some examples of commonly used repositories for biodiversity data but this is of course a non-exclusive list and other options might be more suitable for your data. 5.2.1 Global repositories GBIF, the Global Biodiversity Information Facility, is a data infrastructure and international network providing open access to biodiversity data. It holds species occurrence data from different sources, ranging from museum collections to field observations. Data is stored using data standards, such as Darwin Core, and is openly accessible to everyone. OBIS, the Ocean Biodiversity Information System, is an international databank system for maritime biodiversity and biogeographic data with the objective to provide the largest knowledge base on the diversity, distribution and abundance of marine organisms. Both GBIF and OBIS use an Integrated Publishing Tool (IPT) as a way to submit data. It is a tool that facilitates the creation of metadata in a standard format and helps with mapping the data to Darwin Core and structure it in a Darwin Core Archive (for more information see later chapters). There is extensive documentation on how to use the IPT by GBIF here. Once the dataset is published it is assigned a DOI. Dataverse is an open-source repository software that has installations (i.e., repositories based on the software) all over the world. Data can only be published on a Dataverse instance if you are part of one of the collaborating institutions but is always accessible to everyone. A full list of Dataverse installations can be found here. Zenodo is an open European repository hosted by CERN and in contrast to the other repositories listed here not domain specific. It contains research data from several research domains, provides versioning, DOI assignment, restricted access options if wanted and assures persistent long-term storage of your data. 5.2.2 Dutch repositories Since the project partners behind this guide are situated in the Netherlands, we also want to highlight some Dutch repositories. DataverseNL is a Dutch installation of Dataverse and is hosted by DANS-KNAW. Researchers from all collaborating institutes within the Netherlands can store their data there and make it available to everybody. Reasons for uploading data to DataverseNL: It automatically gets a persistent identifier The metadata is finable through web services like Google dataset search The data is easily retrievable through an API There is versioning to track changes and re-uploads of the data The custody of the data stays with the data owners DANS Data Station Life Sciences The DANS Data Station for Life Sciences is a repository hosted by DANS-KNAW which is based on the Dataverse software. It is open for every individual researcher independent of institution both to store and retrieve data. It is always free of charge to access data and up until a data size of 50 GB also to store data. The Data Station for life sciences is domain specific, accepting data of medical, health and green life sciences, while there are other Data Stations hosted by DANS for other domains, such as social sciences, archeology and physical and technical sciences. It also allows access restrictions, which is helpful if you want to store your data persistently but not necessarily make it open for everyone. 5.3 Tools to help you re3data.org is a registry of repositories for research data where you can filter a broad range of repositories for a different criteria, such as subject, country, persistent identifier system, versioning or licences fairsharing.org is a registry that lists a variety of standards, databases and policies. To find repositories you can either directly search for a specific one and get detailed information on it or search all repositories/databases and filter for a range of options, such as country, subject or specific tags. DataCite Commons provides a repository finder that integrates information on repositories from re3data within the FAIRisFAIR project. It allows users to search for repositories by keywords and filter the repositories for certificates, such as the CoreTrustSeal, or the software they are based on. 5.4 Our choice For the data sets we owned ourselves within our department (bud burst and beech crop ??), we decided to store the primary data on DataverseNL. Besides the reasons listed above, we chose DataverseNL because it was already used by many other researchers from our institute and it was suitable for our data type. For us it was especially important that we retain the custody of our data, which was a reason to not choose the Data Station. Additionally, we wanted to store our data within the Netherlands and it was a natural choice to choose a repository hosted by one of the project partners. "],["standardise-your-data.html", " 6 Standardise your data 6.1 Choose a data standard 6.2 Data standards for biodiversity data 6.3 Tools to help you 6.4 Our choice 6.5 Mapping your data to Darwin Core", " 6 Standardise your data 6.1 Choose a data standard Once you have deposited your data to an online repository, you can start to standardise the data. The first step here is to choose a data standard, i.e., a standard format to name and organise the data. There is a wide range of standards available and many of them are directly tailored to a certain domain. Standards within the field of biodiversity are mostly maintained by the non-profit organisation Biodiversity Information Standards (TDWG). Which standard is best suited for your data depends on its content and format. In order to choose a standard, you should consider: Is the standard accepted and used by your community? If the community is already familiar with the standard the threshold to reuse your standardised data is likely lower. Is the standard well maintained and stable? Data standards should be maintained, further developed and adapted by following the developments in the field they are tailored to and the needs of the community. Terms should also be stable and not change in meaning and always be referred to by a persistent identifier. 6.2 Data standards for biodiversity data Darwin Core (DwC) is a stable data standard tailored to describe biodiversity data by using a defined library of terms. It is an extension of the Dublin Core Metadata Initiative and maintained by TDWG (Biodiversity Information Standards). Darwin Core facilitates sharing of biodiversity data from various sources as every term has a clear definition and a URL, which can be used irrespective of technology, e.g., XML or RDF. ABCD (Access to Biological Collection Data) is a standard to facilitate the access to and exchange of data about specimens and observations. It is a highly structured, comprehensive and still evolving standard that is, for example, used within GBIF. It is maintained by TDWG and consists of an ontology of terms to describe the data. 6.3 Tools to help you A helpful tool to find a standard is FAIRsharing, which is a registry that lists a variety of standards and provides detailed information, for example, about their maintenance, background and how they are linked to other standards. It also allows filtering for certain criteria, such as licensing, country or domain. An overview of the standards maintained by TDWG can be found here. 6.4 Our choice For biodiversity data, a widely used data standard is Darwin Core. It is well suited to be used with species occurrence data that is either collected in the field, in experiments or comes from museum collections. For all the datasets we used to develop this guide, Darwin Core was a suitable standard, as it captures all the information in the data, is easy to apply through extensive documentation and applicable to the tabular format of our data. For this reason, we will focus mainly on Darwin Core in the following chapters, because we think that the tabular format is probably one of the most common formats biodiversity data is stored in. 6.4.1 Why Darwin Core? Darwin Core is widely used in the ecological community and required for publishing data on big ecological data infrastructures, such as GBIF and OBIS. With its primary focus on taxa and their occurrences in nature, it is well suited to describe most ecological datasets, with the necessary flexibility to describe different types of data, such as observational or experimental data. Additionally, mapping variables of ecological data to Darwin Core terms is relatively straightforward and there is detailed description available on each Darwin Core term. 6.5 Mapping your data to Darwin Core 6.5.1 Darwin Core namespaces The Darwin Core list of terms uses four different namespaces. The dwc: namespace marks Darwin Core terms and generally has string values, just as the dc: namespace that marks terms belonging to elements/1.1 namespace of Dublin Core. Terms of the dwciri: namespace are also Darwin Core terms but must be used with IRI values and are generally used in RDFs, while dcterms: refers to terms coming from the terms namespace of Dublin Core and their values depend on the details of the respective term. 6.5.2 Darwin Core terms Every column name in your data should be mapped to a Darwin Core term. This sometimes requires restructuring the data slightly or adding additional information in extra columns, e.g., units. The values within a column (i.e., DwC term) have to be compliant with the definitions given in the Darwin Core List of terms. Table 6.1. Selected set of Darwin Core terms of each class. Bold terms are described in more detail in the following sections. Event Occurrence Taxon MeasurementOrFact Location Organism MaterialEntity GeologicalContext eventID occurrenceID taxonID measurementID locationID organismID materialEntityID geologicalContext parentEventID recordedBy kingdom parentMeasurementID country organismName preparations earliestEonOrLowestEonothem eventType recordedByID phylum measurementType countryCode organismScope disposition earliestEraOrLowestErathem eventDate recordedByID class measurementValue verbatimLocality associatedOrganisms verbatimLabel earliestEpochOrLowestSeries eventTime individualCount order measurementUnit decimalLongitude previousIdentifications associatedSequences lowestBiostratigraphicZone year organismQuantity family measurementMethod decimalLatitude organismRemarks materialEntityRemarks lithostratigraphicTerms month organismQuantityType genus measurementAccuracy geodeticDatum formation day sex specificEpithet measurementDeterminedDate coordinateUncertaintyInMeters bed samplingProtocol behavior scientificName measurementDeterminedBy footprintWKT sampleSizeValue lifeStage acceptedNameUsage measurementRemarks minimumElevationInMeters sampleSizeUnit degreeOfEstablishment nameAccordingTo minimumDepthinMeters samplingEffort occurrenceStatus higherClassification georeferenceProtocol fieldNotes occurrenceRemarks taxonRank eventRemarks catalogNumber vernacularName 6.5.3 Terms of class Event Before assigning the terms of the event class to your data you should define what exactly one event is in your data. An event is generally defined as an action that occurs at a certain time and place. Depending on how your data is collected, there might also be some hierarchy in your events that should be accounted for. Defining how events are structured in your data and which measurements or occurrences belong together, makes it easier to properly map your data to the respective terms, especially eventID and parentEventID, and later on facilitates structuring of the data. 6.5.3.1 eventID &amp; parentEventID The eventID should be a globally unique identifier or an identifier specific to the data set. For more information on how to create globally unique and persistent identifiers, see this chapter. If you choose to create identifiers specific to the data set, we recommend establishing a structure that simultaneously is informative about the event. If there is a hierarchy in the events, eventIDs should build on the parentEventIDs. We recommend using separators (e.g., “_” or “-”) to indicate the different blocks of the event levels within an eventID. Example 1: This can be exemplified with our bud burst use case, where we have three different event levels. The highest level describes the event of going to the field in a certain year to a certain area. The eventID therefore consists of an area abbreviation and the year, e.g., HV2004. For the event level below, the sampling on a specific day within a year and area, the previous eventID becomes the parentEventID and the level 2 eventID extends the parentEventID by a separator and the day of the year, e.g., HV2004_99. The third and lowest event level describes the sampling of a specific tree on a day within an area and year. The eventID of level 2 becomes the parentEventID and the level 3 eventID extends the parentEventID by an underscore and the number of the tree, e.g. HV2004_99_5. This way, the eventID is easily human-readable and directly gives the most important information about the event. Example 2: In the cricket data, there were two different event types, one relating to measurements that have been taken on plants and the other to measurements of individual crickets. For the plants, each plot-treatment combination was measured once, leading to one event for each of them. The eventIDs were therefore simply the plot name and a treatment code, for example, PM1-T1 (PM1 being the plot name, T1 the first treatment). Defining events for the cricket measurements were more difficult, as the data did not specify concrete date and time information of the events. Through data documentation it became clear which measurements have been taken at the same point in time, so that we could group them into the same event. This leads to 18 different event groups per individual cricket, from which we build the eventID by combining the cricket identifier with the event group number, e.g., Cr1-15 (= individual cricket number 1 and event group 15). 6.5.3.2 Date information (eventDate, year, month, day) eventDate is necessary to be filled but can vary in its precision. If a full date is available, use the ISO standard (8601-1:2019) date format of YYYY-MM-DD. You always should give the date as precise as possible, so if the exact day is not available, give at least the year and month, etc. You can also give an interval if a distinct point in time is not available by separating the days or months or years by a slash, e.g., 2014-04-01/08 meaning some time in the interval between the first April 2014 and the 08 April 2014. If the interval starts and ends at days of different months, also repeat the year (e.g., 2014-04-01/2014-07-01). In addition to eventDate, we recommend to use the terms year, month and day (if applicable) as this can facilitate later analysis with the data. If eventDates of different levels are not all of the same format, for example the parent event only has the year (2024), while the child event has an exact date (2024-11-05), the event dates can be transformed to characters, as programs like R do not accept different date formats in the same column. In this case, the additional columns of year, month and day are especially helpful, as missing values can be left empty/filled with NA, while the rest is numeric. 6.5.3.3 Time information (eventTime) If you also have time information in your data, this can either also be stored in eventDate or additionally in an extra column as eventTime. As with date, time should be given in time of day (in 24h system) following the ISO standard 8601-1:2019. Time intervals can again be given by separating times with a slash, e.g., 13:00:00/15:30:00 (the interval between 13:00 and 15:30). If time information should be added to eventDate (which is recommended, as eventDate should be as precise as possible), the date and the time are separated with a “T”, for example, 2018-08-29T15:19. Time Zones can be specified by adding a “Z” at the end, if the time is recorded in UTC (e.g., 2010-02-16T08:40Z) or by stating the deviation from UTC by adding a plus or minus and the number of hours of the deviation, e.g., 2010-02-16T08:40-0300 meaning UTC minus 3 hours). If neither of both is given, it is assumed that the time is given as the local time. 6.5.4 Terms of class Location 6.5.4.1 Coordinates (decimalLonigtude, decimalLatitude, geodeticDatum) Exact coordinates for locations can be given with the terms decimalLongitude and decimalLatitude. If these are filled, the term geodeticDatum should also be filled, giving the spatial reference system in which the coordinates are given. Best practice is to use the EPSG codes. For hierarchical events, like in the bud burst case, coordinates should only be assigned to the event levels they were actually measured, e.g., only for level 3 events where the exact coordinates of the trees are known. 6.5.4.2 Geographic descriptions (verbatimLocality, continent, islandGroup, island, country, municipality, city, stateProvince, county, countryCode) In addition to the coordinates, which we strongly recommend to include, the term verbatimLocality can sometimes be useful. It gives the original textual description of the location, which can be particularly useful if areas have well-known names and makes these locations easier recognisable. There is also a range of other terms that provide geographic description and can be used to determine the location of events, such as continent, islandGroup, island, country, municipality, city, stateProvince or county. In addition to country, you can also use countryCode, which gives the standard code for the country, ideally the two letter codes of ISO 3166-1-alpha-2. 6.5.5 Terms of class Organism The term “organism” in Darwin Core is defined more broadly than in the strict biological sense and can refer to either a specific organism or a defined group of organisms that are taxonomically homogenous. This means a single cricket can be considered an organism, as well as a flock of birds belonging to the same species. 6.5.5.1 organismID Individual organisms can be assigned a unique organismID. This is helpful, if you have several data records that belong to the same individual or organism for maintaining this link between records (e.g., measurements of the same tree in several years). Previously there was the DwC-term individualID, which nowadays is deprecated and replaced with organismID. As with other ID fields, the organismID can either be globally unique or specific to the dataset. 6.5.6 Terms of class Occurrence Occurrence is generally defined as the existence of an organism at a certain time and place. 6.5.6.1 occurrenceID The occurrenceID assigns a unique ID to every occurrence record. Several occurrenceIDs can belong to one eventID, for example when different species occurred at the same event. If you choose to create identifiers specific to the dataset, we recommend proceeding with the block-structure of IDs we have already used for the eventID, which means extending the eventID of the corresponding event by a new block that numbers the occurrences of that event. If there are occurrence records for different event levels, extending the eventID will lead to unequal length of the occurrenceIDs from the different event levels. This can be confusing and lead to doubled IDs, as higher level occurrenceIDs then have the same length as for example lower level eventIDs (i.e., HV2004_99_5 as an eventID refers to a third level event but as an occurrenceID it could also refer to the fifth occurrence record of the higher level eventID HV2004_99). To avoid this and create unique IDs, we therefore add the prefix “o” (for occurrence) in the block of the ID that numbers the occurrences, e.g., HV2004_99_5_o1. OccurrenceID does however not have to be linked to a specific event and can also exist on its own, if there are no sampling events in the data. In that case, we recommend creating a dataset specific ID the same way we created the eventID. Example: - crickets: occurrenceID = Cr1-18_1 (corresponding to eventID Cr1-18, indicating the first occurrence of that event) 6.5.6.2 Taxonomic information Taxonomic information should always be specified to the lowest level possible. Which level this is can be specified with the term taxonRank. We recommend to use at least the following DwC terms to describe taxon information: kingdom phylum class order family genus specificEpithet scientificName The specificEpithet only contains the name of the first or species epithet, while the scientificName contains the full scientific name together with author and date information, if available. Taxonomic information can be added manually but we recommend to directly query a biological taxonomy for the complete classification of the taxa in your data, to avoid misspellings or the use of outdated classification. Generally, taxa sometimes come with several authorship information or with synonym names. To deal with this, it can again be helpful to directly retrieve the taxonomic information from a biological taxonomy, as they also contain information on the accepted usage of classifications. Which taxonomies exist for this and tools to help you use them, will be explained in more detail in the section biological taxonomies. However, even if you retrieve taxonomic information automatically from a taxonomy, we strongly recommend to double check the retrieved information manually, as mistakes can quickly occur through to, for example, similar taxa names or for rare species. Example 1: For the cricket dataset, we queried the taxonomic information of the European field cricket (Gryllus campestris) directly from GBIF, which resulted in the following taxonomic terms (table XX). Table 6.1. Taxonomic information for Gryllus campestris as retrieved from GBIF stored in respective Darwin Core terms. scientificName kingdom phylum class order family genus specificEpithet taxonRank Gryllus campestris Linnaeus, 1758 Animalia Arthropoda Insecta Orthoptera Gryllidae Gryllus campestris species Example 2: The CLUE data covers around 130 different plant species and many of them were either misspelt or synonym names were used. One example is shown in table XX where the species name in the data was Deschampsia flexuos which is a synonym of the species name Avenella flexuosa. This was automatically detected while retrieving the taxonomic information from GBIF and the corresponding information correctly assigned accordingly. Table 6.2. Taxonomic information for Deschampsia flexuosa, which is a synonym of Avenella flexuosa, as retrieved from GBIF and stored in respective Darwin Core terms. scientificName kingdom phylum class order family genus specificEpithet taxonRank Deschampsia flexuosa (L.) Trin. Plantae Tracheophyta Liliopsida Poales Poaceae Avenella flexuosa species 6.5.6.3 individualCount and organismQuantity In the occurrence file, it is important to quantify how many organisms have occurred. There are two different terms to do so, individualCount and organismQuantity. While individualCount is unitless and only gives the number of individuals present at the time of the occurrence, organismQuantity is more flexible, as it describes the quantity of an organism. As organisms do not have to be individuals (see above), it can for example also give the percentage of biomass of an organism or quantify it on a certain scale. Therefore, organismQuantity always requires the additional term organismQuantityType that describes the corresponding quantification system. GBIF provides a Quantity Type Vocabulary with terms they recommend to use to describe the organismQuantityType. There is ongoing discussion on which of the two terms to use. Some suggest deprecating the term individualCount, as the same information can easily be stored in organismQuantity with higher flexibility. However, individualCount seems to be a standard term that is widely used in specific disciplines, where the terms are not necessarily viewed as redundant. Moreover, it was demanded that there should first be standardised vocabulary in place for organismQuantity and organismQuantityType. Based on the greater flexibility and the potential of deprecating individualCount, we recommend using organismQuantity when possible. 6.5.7 Terms of class Measurement or fact The measurement or fact terms require your measurement records to be in a long format. As research data often is stored in a wide format, you will first need to pivot your data before mapping is possible, meaning that all your measured values (or facts) are in one column and have a variable description in a separate column. 6.5.7.1 measurementValue &amp; measurementUnit After transforming your data into the long format, the column containing the measured values or facts can be mapped to the Darwin Core term measurementValue. Measurements come with units, which are often either stored within the column heading or not specified at all in the raw data. Darwin Core accounts for that by the term measurementUnit, which has to be added if you use measurementValue. It is best practice to use SI (International System of Units) units if possible. For unitless measurements the measurementUnit should still be present in the data but can be left empty/filled with NA. 6.5.7.2 measurementType &amp; measurementMethod With the long format and all measurements being stored in measurementValue, you additionally need to store the information what has been measured, i.e., what each measurement or fact means. This is done by the term measurementType. Additionally, you should use the term measurementMethod that states how each measurement was measured. For both terms, it is recommended to use controlled vocabulary where possible. For more information on the use of ontologies, see section ontologies. Example: To exemplify how the four terms measurementValue, measurementUnit, measurementType and measurementMethod are used, the following shows the previous state of the measurements in the bud burst raw data (top) and how the columns containing measurements are mapped to the Darwin Core terms (bottom). Table 6.3. Bud burst data before (top) and after (bottom) mapping to Darwin Core terms. Year TreeID TreeTopScore TreeAllScore 1988 61 2 2 measurementType measurementValue measurementUnit measurementMethod bud burst stage (PO:0025532) of the tree crown 2 NA https://doi.org/10.1098/rspb.2000.1363 bud burst stage (PO:0025532) of the whole tree 2 NA https://doi.org/10.1098/rspb.2000.1363 6.5.7.3 measurementID The measurementID assigns a unique ID to every measurement or fact. Several measurementIDs can belong to one occurrenceID and eventID, for example when different characteristics of the same individual are measured. If you choose to create identifiers specific to the dataset, we recommend proceeding with the block-structure of IDs we have already used for the eventID and occurrenceID. If there are measurements at different event levels, extending the occurrenceID by a separator and a number for the measurement, will lead to unequal length of measurementIDs from different event levels and increases the possibility that IDs are not unique. To avoid this, we add the prefix “m” (for measurement) in the block of the ID that numbers the measurements (e.g., HV2004_99_5_1_m2 for the second measurement). "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
