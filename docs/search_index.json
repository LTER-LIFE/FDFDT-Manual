[["index.html", "A hands-on guide to FAIR and structured ecological data Preface How to cite and licence", " A hands-on guide to FAIR and structured ecological data Cherine C. Jantzen &amp; Stefan J.G. Vriend 2024-06-20 Preface This interactive manual aims to provide ecologists and other data owners of ecological data with a hands-on guide on how to make your ecological data more FAIR and tackle the problems that can occur along this process. How to cite and licence This guide is licenced under a Creative Commons Attribution-Noncommercial 4.0 International license (CC BY-NC 4.0). "],["00-preface-part2.html", "Who this guide is for Why you should use this guide and improve your data management How to read this guide", " Who this guide is for This guide is for anybody who has ecological data and wants to improve their data management by making their data more FAIR, especially: ecologists who want to improve their research data management and by that make their data available for reuse by others students who collect data for their thesis and want to make their data reusable for others after their project is finished institutions and organisations that want to provide their employees with some guidance on how to increase their data maturity data stewards who … data managers who … anyone else … Why you should use this guide and improve your data management Data management is the practice of taking care of data throughout its entire lifecycle, from its collection, processing and use to its storage and sharing. Throughout the whole lifecycle, good data management is crucial to ultimately enhance the reusability of the data for yourself or others. For example, by storing your data persistently and implementing versioning, you lower the risk of data loss and have higher traceability of changes and errors. Additionally, by describing, annotating, and organising the data, it becomes better understandable for others, facilitating its reuse and increasing its impact and that of the associated research. How to read this guide This guide is designed in a way that you can only read the chapters that are relevant for improving the maturity of your data. To find these chapters, we provide you with a self-assessment tool that shows to what extent your data already complies with the FAIR principles. Directly based on the outcome of the FAIR assessment you will be provided with a tailored list of the chapters in this guide you could go through to further mature your data. You therefore do not have to read the full guide from top to bottom but should easily be able to only go through the chapters most relevant to you. Disclaimer We want to emphasise here that there is no one-size-fits-all solution and every dataset will bring its own challenges that need individual solutions. We cannot tackle all of these challenges but only give recommendations based on our own work with a range of different ecological datasets. "],["01-introduction.html", " 1 Introduction 1.1 What is FAIR? 1.2 Extending FAIR 1.3 FAIR is not open 1.4 Why should your data be FAIR?", " 1 Introduction 1.1 What is FAIR? Research data often is stored on local hard drives, not well described or not formatted in a consistent and standard way. This makes it very difficult for others (and often even for the data owner themselves) to reuse the data or reproduce the research, because the data is not straightforward to understand, cannot be interpreted correctly, or, in case of local storage, might be unknown to others. In order to change this and increase the reusability of data, Wilkinson et al. (2016) introduced the FAIR concept together with 15 guiding principles providing guidance on how to implement it. FAIR stands for Findable, Accessible, Interoperable and Reusable, which more concrete means the following: Findable. This means that the metadata (and the data) can be easily found by humans and computers and that machine-readable metadata allows for automatic discovery of the data by machines. Accessible. It is clearly stated how the user can access the data and whether, for example, authorisation or authentication are required. Interoperable. In order to integrate the data with other data and to incorporate it into workflows or applications, the data needs to be fully compatible with other data. In easier words this means that data resources should ‘speak the same language’ to be used together. Reusable. The ultimate goal of the FAIR concept is to make the data reusable, which means that there is proper annotation in the form of metadata that allows users (and machines) to understand the data and correctly interpret it. Figure 1.1: The FAIR concept. The four letters of FAIR stand for findable, accessible, interoperable and reusable and come with 15 guiding principles. 1.2 Extending FAIR In our guide we will extend FAIR as it would be strictly defined by the FAIR principles in order to make it workable in practice. This means that we also include the component of structuring your data into the process of FAIRifying it. Structured. Structuring your data, so organising it in a tidy way, can clearly make the data easier to access for others and enhances its understandability. We therefore think that this is a crucial component of increasing the reusability of your data. 1.3 FAIR is not open It is important to stress here that FAIR is not equal to open. While the metadata of your data should be openly available, informing others about the existence of your data, the data itself does not have to be open. If you have reasons for not making the data openly available to everyone (e.g., because it contains sensitive information), this can and should be made clear in the metadata where according licensing information and access rights can be provided. As long as this is guaranteed, restricted or closed data can also be completely FAIR. 1.4 Why should your data be FAIR? Depending on the current maturity of your data, making data FAIR certainly requires some effort, but the benefits are evident both for yourself and for others. Findable and well annotated data facilitates your own reuse of your data in the future and the reuse of your data by others, which simultaneously increases the visibility and the impact of your research. Increasing the interoperability of the data additionally increases the possibilities for collaborations and will in total benefit the scientific community. References Wilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., Silva Santos, L. B. da, Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR guiding principles for scientific data management and stewardship. Scientific Data, 3(1). https://doi.org/10.1038/sdata.2016.18 "],["01.1-FAIR-assessment.html", "1.5 FAIR assessment of your data", " 1.5 FAIR assessment of your data As stated before, every dataset is different and datasets can vary widely in their level of FAIR. FAIR is a gradient (see also FAIR Data Maturity Model. Specification and Guidelines of RDA) and different datasets can fulfil different criteria of FAIR but reach the same overall level of FAIRness. It is therefore important to first assess which state your data is in and what steps your data need to be improved. There are already a range of FAIR assessment tools (see here for an overview), which score your data but there is much less guidance on how to actually improve the FAIRness of your data. Next to questionnaire-based assessment tools, there are also tools available that automatically assess the FAIRness of your data. Most of them however require the data to be already stored on an online repository, which is not the state at which we want to start with our FAIR assessment. We therefore developed our own tool that provides you with a set of simple questions about four properties of your data: metadata, storage, standards and structure. Metadata is data about your data and contains information about the who, where, what, when and how of data collection allowing another user of your data to understand and reuse it without prior knowledge of the data. Storage is about whether your data is stored persistently and in a way that makes it findable and accessible to others. Standards describe uniform, community-accepted formats in which both data and metadata are stored and which enhance compatibility with other datasets. Structure is about whether your data is organised in a consistent and logical way. Besides the questions about structure, they are all directly based on the FAIR principles. Figure 2 visualises how the data properties in the assessment link to the letters of FAIR. Your answers will provide you with a circle-diagram showing how mature your dataset is in each of the properties and connected to that a list of chapters of this manual you can work through to further improve the maturity of your data. Figure 1.2: Link between data properties and FAIR. The letters on the left stand for the data properties used in the FAIR assessment (metadata, storage, standard, structure). Each plus indicates that the data property improves the maturity of each of the respective letter of FAIR. A blanc space means that this data property has no direct influence on the respective letter of FAIR. It will likely be difficult to reach the full score in all of the properties of your data, but this also not necessarily the aim of this assessment or guide. Every improvement is already a great step! Especially the components that link to interoperability are more difficult to implement, as true interoperability generally is a big challenge within and across disciplines (Pagano et al. (2013)). One step to reach interoperability is to use a language for knowledge representation and linked data, such as RDF (Resource Description Framework). However, as this is more the expertise of data or information scientists instead of ecologists, we will not look into these topics in this guide. References Pagano, P., Candela, L., &amp; Castelli, D. (2013). Data interoperability. Data Science Journal, 12(0), GRDI19–GRDI25. https://doi.org/10.2481/dsj.grdi-004 "],["02-example-datasets.html", " 2 Example data sets 2.1 Bud burst data 2.2 Cricket data 2.3 CLUE field vegetation cover data (i.e., CLUE data)", " 2 Example data sets To illustrate the process of FAIRifying ecological data we will rely on several example data sets we used to develop this guide. These data sets were selected because they cover a broad variety of different structures and different levels of FAIR, ranging from observational to experimental data and from highly unstructured and undocumented data to data that already fulfils some of the FAIR principles. 2.1 Bud burst data The first dataset contains long-term data on the phenology of different tree species. Every year in spring, deciduous trees develop new leaves from their buds at a certain point in time. The timing of this can be crucial for herbivores, such as caterpillars, relying on newly emerging leaves as an important food source. To monitor shifts in tree phenology, the Department of Animal Ecology at NIOO has assessed leaf development every year since 1988 by scoring the stage of bud opening on a fixed scale (cf. Visser &amp; Holleman (2001)). Figure 2.1: FAIR assessment of bud burst data. 2.2 Cricket data The third data set contains experimental data on invertebrates belonging to the publication of Vogels et al. (2021). They tested whether changes in plant N:P ratios following sod-cutting constrain the reproductive potential in the European field cricket (Gryllus campestris). Different phosphorus and liming treatments were applied to vegetation plots in the heathlands of the Veluwe and the plants of these plots used in a feeding experiment in which the reproductive success of the female crickets was measured. Figure 2.2: FAIR assessment of cricket data. 2.3 CLUE field vegetation cover data (i.e., CLUE data) The last example dataset consists of data on vegetation cover collected in a long-term grassland biodiversity field experiment site in the Veluwe, Netherlands, also abbreviated as CLUE fields (= changing land usage, enhancement of biodiversity and ecosystem development). Data collection started in 1996 and was conducted by the Department of Terrestrial Ecology of NIOO-KNAW. On a former agricultural field, experimental plots have been established in a random block design and different sowing treatments (high diversity, low diversity, continued agricultural rotation, and natural succession), as well as soil inoculation treatments have been applied in two different experiments in 1996. In both experiments, percent cover of each occurring plant species is measured annually. The data of both experiments result in two independent datasets, they are however very similar and their FAIRification process was nearly the same so that they will be treated as one dataset, called CLUE data, in the following. Figure 2.3: FAIR assessment of CLUE data. References Visser, M. E., &amp; Holleman, L. J. M. (2001). Warmer springs disrupt the synchrony of oak and winter moth phenology. Proceedings of the Royal Society of London. Series B: Biological Sciences, 268(1464), 289–294. https://doi.org/10.1098/rspb.2000.1363 Vogels, J. J., Verberk, W. C. E. P., Kuper, J. T., Weijters, M. J., Bobbink, R., &amp; Siepel, H. (2021). How to restore invertebrate diversity of degraded heathlands? A case study on the reproductive performance of the field cricket gryllus campestris (l.). Frontiers in Ecology and Evolution, 9. https://doi.org/10.3389/fevo.2021.659363 "],["03-workflow.html", " 3 Workflow", " 3 Workflow The general workflow that we present in this guide to make your data more FAIR is shown in Figure 3.1 and shortly described in the following section. To make your data more FAIR, you should: First, describe your data and thereby gather metadata. Afterwards, you should decide whether your raw data should be mobilised to an online repository. If yes, you have to choose a suitable repository and upload your metadata and data there. Next, you can start to standardise both the data and the metadata. With which of the two you start does not matter, as they can both be structured and standardised separately. For the metadata: First, choose a metadata standard. Next, map the metadata to the selected standard. For the data: First, choose a suitable data standard. Then, map the data to that standard. For further standardisation of your data, use ontologies and biological taxonomies where possible to describe the contents of the data. After standardising the data, you should choose a standardised structure for the data. Restructure your data according to the chosen structure. Once standards and structure are applied, structural metadata can be created that describes how the data is organised in its new form. As a last step your standardised data and metadata should be persistently stored in an online storage. If you have not chosen a repository before (or if you want to store the standardised data elsewhere), you should: choose a suitable repository that accepts your new data structure/format. mobilise the data and the metadata to the chosen repository. Even though this guide is tailored to ecological data, the workflow presented here should be generally applicable to any type of data, only the implementations of each step will differ. In the following chapters, we will go through each of the steps and provide some general background, introduce some options for implementation for ecological data and considerations that can help to choose the best suited option for your data, as well as a short insight into our own choices for the example datasets. Figure 3.1: Diagram of the workflow presented in this guide to FAIRify data. "],["04-Describe-data.html", " 4 Describe your data - Metadata (content) 4.1 Licencing", " 4 Describe your data - Metadata (content) The first step before you store your data properly should always be to describe and annotate it. This type of description is called metadata and basically provides information about your dataset that helps to understand it. We can differentiate into administrative metadata that, for example, contains information about the authors, the contents, coverage and the maintenance of the described data and structural metadata, which is giving information on how the data set is organised and how single data files connect to each other. Many repositories require a minimal set of metadata for uploading data, which can either directly be entered in a fill-in form provided by the repository or added in a separate file that is uploaded together with the data. The simplest form of metadata is a README text file containing some basic information on what the data is about. There are however no regulations on what information a README should contain and in reality, the extent of it varies a lot. In general, the richer the data is described the better. Based on best practices of writing readme files and in terms of standardising the metadata in a later step, we recommend to gather the following information about your data: About the dataset: Title of the dataset Short description what contents each data file contains Short description of the methods used for data collection Responsible parties, including who created the data and who can be contacted about the data When was the data collected? Which time span is covered? Where has the data been collected? Licencing information: How can others use your data? To find out more about different licences, see section Licencing. Changes in the data or updates should ideally also be recorded in the README About individual data files: What do columns mean? Which units belong to each column? If you include personal information, like email addresses, of other people in the metadata that is made available online later on, it should always be asked for permission beforehand. 4.1 Licencing If you deposit your data online it is important to communicate to potential users what permissions they have and arrange a legal agreement in the form of a licence. A commonly used set of licences are the copyright licences of Creative Commons (CC). Scientific datasets most often are published under a CC BY licence, but Creative Commons also offers other licensing options, as stated in Table 4.1. They also offer an interactive tool (License chooser) that can help you to choose the best suited licence for your data. Table 4.1. Creative commons licences and their meanings. Licence What can users do with your data? CC BY users can use, distribute and build upon the data but have to give credits to the creator CC BY-NC users can use, distribute and build upon the data but have to give credits to the creator and data cannot be used commercially CC BY-SA users can use, distribute and build upon the data, they have to give credits to the creator and adaptations need to be shared under the same terms CC BY-NC-SA users can use, distribute and build upon the data, they have to give credits to the creator, adaptations need to be shared under the same terms and data cannot be used commercially CC BY-ND users are not allowed to adapt or modify the data, but can copy or distribute it if they give credits to the creator CC BY-NC-ND users are not allowed to adapt or modify the data, but can copy or distribute it if they give credits to the creator and data cannot be used commercially CC0 (“CC Zero”) copyrights are given up and work is put into the public domain Carefully consider which licence to choose. It cannot be revoked and every user of the data needs to comply with the licence’s conditions of use, even if the data is no longer distributed. "],["05-storage.html", " 5 Storing your data 5.1 Choosing a repository 5.2 Repositories to store biodiversity data 5.3 Tools to help you 5.4 Our choice", " 5 Storing your data Data is often stored locally on personal computers or external hard drives which makes the data unavailable to others and increases the risk of data loss. These drawbacks can be overcome by mobilising your data to an online repository. Mobilising raw or standardised data? &lt;Mobilising raw data and benefits. Mobilising standard data and benefits. Metadata can “always” be mobilised.&gt; 5.1 Choosing a repository The range of existing repositories is very large and while some are domain specific, there are other repositories that accept data from different disciplines. To find the repository that is best suited for your data it can be helpful to take the following points into consideration: Which data formats are accepted? Depending on the format your data is stored in, not every repository will accept your data. Many repositories however accept a wide range of formats but have preferred formats to upload your data in, as these formats have best long-term guarantees in sustainability and accessibility (see here for an example). What repository is commonly used in your organisation, community or discipline? Check which repositories your direct colleagues in your organisation, or other data providers in your community or field of research use. Using such repositories will likely make your data better linked to other datasets and found more easily by others in your discipline. Is a persistent identifier assigned? To ensure that your dataset can always be found, it is important that your data is associated with a persistent identifier (PID). Most repositories assign a PID when you submit your data, which is preferred over repositories in which you would need to assign the PID yourself. Different repositories also use different identifier systems which can be another selection criterion if you have personal preferences for a specific identifier system. How can the dataset be accessed? Do users have to download the dataset directly from the repositories website or is there a more automated option available, such as an API (Application Programming Interfaces)? Can the data be easily accessed? Is metadata stored persistently even when the data is no longer available? The metadata of your dataset should be stored persistently so that users can still find out about the existence of your data, even if it is no longer available on the repository itself. Do you want version control? Not all repositories provide version control, meaning that changes in the data and reuploads are not tracked but only the most recent version is available. If this is an important component for you, you should make sure the chosen repository provides this option. At re3data.org, for example, you can directly filter for repositories with versioning. Do you retain custody of your data or is it transferred to the repository? There are repositories that obtain the custody of your data once it is uploaded there, meaning that the data management is done by the repository and your possibilities in managing the data are then very limited. This can be beneficial if you do not want to have further responsibility for your data, but if you want to still be able to manage the data yourself after uploading it, a repository where the custody stays with you will be better suited. 5.2 Repositories to store biodiversity data Here we want to provide you with some examples of commonly used repositories for biodiversity data but this is of course a non-exclusive list and other options might be more suitable for your data. 5.2.1 Global repositories GBIF, the Global Biodiversity Information Facility, is a data infrastructure and international network providing open access to biodiversity data. It holds species occurrence data from different sources, ranging from museum collections to field observations. Data is stored using data standards, such as Darwin Core, and is openly accessible to everyone. OBIS, the Ocean Biodiversity Information System, is an international databank system for maritime biodiversity and biogeographic data with the objective to provide the largest knowledge base on the diversity, distribution and abundance of marine organisms. Both GBIF and OBIS use an Integrated Publishing Tool (IPT) as a way to submit data. It is a tool that facilitates the creation of metadata in a standard format and helps with mapping the data to Darwin Core and structure it in a Darwin Core Archive (for more information see later chapters). There is extensive documentation on how to use the IPT by GBIF here. Once the dataset is published it is assigned a DOI. Dataverse is an open-source repository software that has installations (i.e., repositories based on the software) all over the world. Data can only be published on a Dataverse instance if you are part of one of the collaborating institutions but is always accessible to everyone. A full list of Dataverse installations can be found here. Zenodo is an open European repository hosted by CERN and in contrast to the other repositories listed here not domain specific. It contains research data from several research domains, provides versioning, DOI assignment, restricted access options if wanted and assures persistent long-term storage of your data. 5.2.2 Dutch repositories Since the project partners behind this guide are situated in the Netherlands, we also want to highlight some Dutch repositories. DataverseNL is a Dutch installation of Dataverse and is hosted by DANS-KNAW. Researchers from all collaborating institutes within the Netherlands can store their data there and make it available to everybody. Reasons for uploading data to DataverseNL: It automatically gets a persistent identifier The metadata is finable through web services like Google dataset search The data is easily retrievable through an API There is versioning to track changes and re-uploads of the data The custody of the data stays with the data owners DANS Data Station Life Sciences The DANS Data Station for Life Sciences is a repository hosted by DANS-KNAW which is based on the Dataverse software. It is open for every individual researcher independent of institution both to store and retrieve data. It is always free of charge to access data and up until a data size of 50 GB also to store data. The Data Station for life sciences is domain specific, accepting data of medical, health and green life sciences, while there are other Data Stations hosted by DANS for other domains, such as social sciences, archeology and physical and technical sciences. It also allows access restrictions, which is helpful if you want to store your data persistently but not necessarily make it open for everyone. 5.3 Tools to help you re3data.org is a registry of repositories for research data where you can filter a broad range of repositories for a different criteria, such as subject, country, persistent identifier system, versioning or licences fairsharing.org is a registry that lists a variety of standards, databases and policies. To find repositories you can either directly search for a specific one and get detailed information on it or search all repositories/databases and filter for a range of options, such as country, subject or specific tags. DataCite Commons provides a repository finder that integrates information on repositories from re3data within the FAIRisFAIR project. It allows users to search for repositories by keywords and filter the repositories for certificates, such as the CoreTrustSeal, or the software they are based on. 5.4 Our choice For the data sets we owned ourselves within our department (bud burst), we decided to store the primary data on DataverseNL. Besides the reasons listed above, we chose DataverseNL because it was already used by many other researchers from our institute and it was suitable for our data type. For us it was especially important that we retain the custody of our data, which was a reason to not choose the Data Station. Additionally, we wanted to store our data within the Netherlands and it was a natural choice to choose a repository hosted by one of the project partners. "],["06-standard.html", " 6 Standardise your data 6.1 Choose a data standard 6.2 Data standards for biodiversity data 6.3 Tools to help you 6.4 Our choice 6.5 Mapping your data to Darwin Core", " 6 Standardise your data 6.1 Choose a data standard Once you have deposited your data to an online repository, you can start to standardise the data. The first step here is to choose a data standard, i.e., a standard format to name and organise the data. There is a wide range of standards available and many of them are directly tailored to a certain domain. Standards within the field of biodiversity are mostly maintained by the non-profit organisation Biodiversity Information Standards (TDWG). Which standard is best suited for your data depends on its content and format. In order to choose a standard, you should consider: Is the standard accepted and used by your community? If the community is already familiar with the standard the threshold to reuse your standardised data is likely lower. Is the standard well maintained and stable? Data standards should be maintained, further developed and adapted by following the developments in the field they are tailored to and the needs of the community. Terms should also be stable and not change in meaning and always be referred to by a persistent identifier. 6.2 Data standards for biodiversity data Darwin Core (DwC) is a stable data standard tailored to describe biodiversity data by using a defined library of terms. It is an extension of the Dublin Core Metadata Initiative and maintained by TDWG (Biodiversity Information Standards). Darwin Core facilitates sharing of biodiversity data from various sources as every term has a clear definition and a URL, which can be used irrespective of technology, e.g., XML or RDF. ABCD (Access to Biological Collection Data) is a standard to facilitate the access to and exchange of data about specimens and observations. It is a highly structured, comprehensive and still evolving standard that is, for example, used within GBIF. It is maintained by TDWG and consists of an ontology of terms to describe the data. 6.3 Tools to help you A helpful tool to find a standard is FAIRsharing, which is a registry that lists a variety of standards and provides detailed information, for example, about their maintenance, background and how they are linked to other standards. It also allows filtering for certain criteria, such as licensing, country or domain. An overview of the standards maintained by TDWG can be found here. 6.4 Our choice For biodiversity data, a widely used data standard is Darwin Core. It is well suited to be used with species occurrence data that is either collected in the field, in experiments or comes from museum collections. For all the datasets we used to develop this guide, Darwin Core was a suitable standard, as it captures all the information in the data, is easy to apply through extensive documentation and applicable to the tabular format of our data. For this reason, we will focus mainly on Darwin Core in the following chapters, because we think that the tabular format is probably one of the most common formats biodiversity data is stored in. 6.4.1 Why Darwin Core? Darwin Core is widely used in the ecological community and required for publishing data on big ecological data infrastructures, such as GBIF and OBIS. With its primary focus on taxa and their occurrences in nature, it is well suited to describe most ecological datasets, with the necessary flexibility to describe different types of data, such as observational or experimental data. Additionally, mapping variables of ecological data to Darwin Core terms is relatively straightforward and there is detailed description available on each Darwin Core term. 6.5 Mapping your data to Darwin Core 6.5.1 Darwin Core namespaces The Darwin Core list of terms uses four different namespaces. The dwc: namespace marks Darwin Core terms and generally has string values, just as the dc: namespace that marks terms belonging to elements/1.1 namespace of Dublin Core. Terms of the dwciri: namespace are also Darwin Core terms but must be used with IRI values and are generally used in RDFs, while dcterms: refers to terms coming from the terms namespace of Dublin Core and their values depend on the details of the respective term. 6.5.2 Darwin Core terms Every column name in your data should be mapped to a Darwin Core term. This sometimes requires restructuring the data slightly or adding additional information in extra columns, e.g., units. The values within a column (i.e., DwC term) have to be compliant with the definitions given in the Darwin Core List of terms. Table 6.1. Selected set of Darwin Core terms of each class. Bold terms are described in more detail in the following sections. Event Occurrence Taxon MeasurementOrFact Location Organism MaterialEntity GeologicalContext eventID occurrenceID taxonID measurementID locationID organismID materialEntityID geologicalContext parentEventID recordedBy kingdom parentMeasurementID country organismName preparations earliestEonOrLowestEonothem eventType recordedByID phylum measurementType countryCode organismScope disposition earliestEraOrLowestErathem eventDate recordedByID class measurementValue verbatimLocality associatedOrganisms verbatimLabel earliestEpochOrLowestSeries eventTime individualCount order measurementUnit decimalLongitude previousIdentifications associatedSequences lowestBiostratigraphicZone year organismQuantity family measurementMethod decimalLatitude organismRemarks materialEntityRemarks lithostratigraphicTerms month organismQuantityType genus measurementAccuracy geodeticDatum formation day sex specificEpithet measurementDeterminedDate coordinateUncertaintyInMeters bed samplingProtocol behavior scientificName measurementDeterminedBy footprintWKT sampleSizeValue lifeStage acceptedNameUsage measurementRemarks minimumElevationInMeters sampleSizeUnit degreeOfEstablishment nameAccordingTo minimumDepthinMeters samplingEffort occurrenceStatus higherClassification georeferenceProtocol fieldNotes occurrenceRemarks taxonRank eventRemarks catalogNumber vernacularName 6.5.3 Terms of class Event Before assigning the terms of the event class to your data you should define what exactly one event is in your data. An event is generally defined as an action that occurs at a certain time and place. Depending on how your data is collected, there might also be some hierarchy in your events that should be accounted for. Defining how events are structured in your data and which measurements or occurrences belong together, makes it easier to properly map your data to the respective terms, especially eventID and parentEventID, and later on facilitates structuring of the data. 6.5.3.1 eventID &amp; parentEventID The eventID can be a globally unique identifier or an identifier specific to the data set. For more information on how to create globally unique identifiers, see this chapter. If you choose to create identifiers specific to the data set, we recommend establishing a structure that simultaneously is informative about the event. If there is a hierarchy in the events, eventIDs should build on the parentEventIDs. We recommend using separators (e.g., “_” or “-”) to indicate the different blocks of the event levels within an eventID. Some guides about persistent identifiers (e.g., Richards et al. (2011)) state that IDs should be opaque, meaning that they do not give any information about what they describe or relationships between resources. By this it can be avoided that the ID contains information that might no longer be true at a later time (because the resource has changed). However, these guides mostly referred to opaque identifiers on the data or resource level and not on the record level, which is why we decided to increase human-readability by creating informative dataset specific IDs. This problem does of course not occur if you choose GUIDs. This can be exemplified with our bud burst use case, where we have three different event levels. The highest level describes the event of going to the field in a certain year to a certain area. The eventID therefore consists of an area abbreviation and the year, e.g., HV2004. For the event level below, the sampling on a specific day within a year and area, the previous eventID becomes the parentEventID and the level 2 eventID extends the parentEventID by a separator and the day of the year, e.g., HV2004_99. The third and lowest event level describes the sampling of a specific tree on a day within an area and year. The eventID of level 2 becomes the parentEventID and the level 3 eventID extends the parentEventID by an underscore and the number of the tree, e.g. HV2004_99_5. This way, the eventID is easily human-readable and directly gives the most important information about the event. In the cricket data, there were two different event types, one relating to measurements that have been taken on plants and the other to measurements of individual crickets. For the plants, each plot-treatment combination was measured once, leading to one event for each of them. The eventIDs were therefore simply the plot name and a treatment code, for example, PM1-T1 (PM1 being the plot name, T1 the first treatment). Defining events for the cricket measurements were more difficult, as the data did not specify concrete date and time information of the events. Through data documentation it became clear which measurements have been taken at the same point in time, so that we could group them into the same event. This leads to 18 different event groups per individual cricket, from which we build the eventID by combining the cricket identifier with the event group number, e.g., Cr1-05 (= individual cricket number 1 and event group 5). 6.5.3.2 Date information (eventDate, year, month, day) eventDate is necessary to be filled but can vary in its precision. If a full date is available, use the ISO standard (8601-1:2019) date format of YYYY-MM-DD. You always should give the date as precise as possible, so if the exact day is not available, give at least the year and month, etc. You can also give an interval if a distinct point in time is not available by separating the days or months or years by a slash, e.g., 2014-04-01/08 meaning some time in the interval between the first April 2014 and the 08 April 2014. If the interval starts and ends at days of different months, also repeat the year (e.g., 2014-04-01/2014-07-01). In addition to eventDate, we recommend to use the terms year, month and day (if applicable) as this can facilitate later analysis with the data. If eventDates of different levels are not all of the same format, for example the parent event only has the year (2024), while the child event has an exact date (2024-11-05), the event dates can be transformed to characters, as programs like R do not accept different date formats in the same column. In this case, the additional columns of year, month and day are especially helpful, as missing values can be left empty/filled with NA, while the rest is numeric. 6.5.3.3 Time information (eventTime) If you also have time information in your data, this can either also be stored in eventDate or additionally in an extra column as eventTime. As with date, time should be given in time of day (in 24h system) following the ISO standard 8601-1:2019. Time intervals can again be given by separating times with a slash, e.g., 13:00:00/15:30:00 (the interval between 13:00 and 15:30). If time information should be added to eventDate (which is recommended, as eventDate should be as precise as possible), the date and the time are separated with a “T”, for example, 2018-08-29T15:19. Time Zones can be specified by adding a “Z” at the end, if the time is recorded in UTC (e.g., 2010-02-16T08:40Z) or by stating the deviation from UTC by adding a plus or minus and the number of hours of the deviation, e.g., 2010-02-16T08:40-0300 meaning UTC minus 3 hours). If neither of both is given, it is assumed that the time is given as the local time. 6.5.4 Terms of class Location 6.5.4.1 Coordinates (decimalLonigtude, decimalLatitude, geodeticDatum) Exact coordinates for locations can be given with the terms decimalLongitude and decimalLatitude. If these are filled, the term geodeticDatum should also be filled, giving the spatial reference system in which the coordinates are given. Best practice is to use the EPSG codes. For hierarchical events, like in the bud burst case, coordinates should only be assigned to the event levels they were actually measured, e.g., only for level 3 events where the exact coordinates of the trees are known. 6.5.4.2 Geographic descriptions (verbatimLocality, continent, islandGroup, island, country, municipality, city, stateProvince, county, countryCode) In addition to the coordinates, which we strongly recommend to include, the term verbatimLocality can sometimes be useful. It gives the original textual description of the location, which can be particularly useful if areas have well-known names and makes these locations easier recognisable. There is also a range of other terms that provide geographic description and can be used to determine the location of events, such as continent, islandGroup, island, country, municipality, city, stateProvince or county. In addition to country, you can also use countryCode, which gives the standard code for the country, ideally the two letter codes of ISO 3166-1-alpha-2. 6.5.5 Terms of class Organism The term “organism” in Darwin Core is defined more broadly than in the strict biological sense and can refer to either a specific organism or a defined group of organisms that are taxonomically homogenous. This means a single cricket can be considered an organism, as well as a flock of birds belonging to the same species. 6.5.5.1 organismID Individual organisms can be assigned a unique organismID. This is helpful, if you have several data records that belong to the same individual or organism for maintaining this link between records (e.g., measurements of the same tree in several years). Previously there was the DwC-term individualID, which nowadays is deprecated and replaced with organismID. As with other ID fields, the organismID can either be globally unique or specific to the dataset. 6.5.6 Terms of class Occurrence Occurrence is generally defined as the existence of an organism at a certain time and place. 6.5.6.1 occurrenceID The occurrenceID assigns a unique ID to every occurrence record. Several occurrenceIDs can belong to one eventID, for example when different species occurred at the same event. You can either assign GUIDs (see here) or identifiers specific to the data set. If you choose to create identifiers specific to the dataset, we recommend creating informative IDs that give information about the occurrences they describe (see eventID for more details). If you also have an eventID in your data, we recommend proceeding with the block-structure of IDs we have already used there, which means extending the eventID of the corresponding event by a new block that numbers the occurrences of that event. If there are occurrence records for different event levels, extending the eventID will lead to unequal length of the occurrenceIDs from the different event levels. This can be confusing and lead to doubled IDs, as higher level occurrenceIDs then have the same length as for example lower level eventIDs (i.e., HV2004_99_5 as an eventID refers to a third level event but as an occurrenceID it could also refer to the fifth occurrence record of the higher level eventID HV2004_99). To avoid this and create unique IDs, we therefore add the prefix “o” (for occurrence) in the block of the ID that numbers the occurrences, e.g., HV2004_99_5_o1. crickets: occurrenceID = Cr1-05_1 (corresponding to eventID Cr1-05, indicating the first occurrence of that event) 6.5.6.2 Taxonomic information Taxonomic information should always be specified to the lowest level possible. Which level this is can be specified with the term taxonRank. We recommend to use at least the following DwC terms to describe taxon information: kingdom phylum class order family genus specificEpithet scientificName The specificEpithet only contains the name of the first or species epithet, while the scientificName contains the full scientific name together with author and date information, if available. Taxonomic information can be added manually but we recommend to directly query a biological taxonomy for the complete classification of the taxa in your data, to avoid misspellings or the use of outdated classification. Generally, taxa sometimes come with several authorship information or with synonym names. To deal with this, it can again be helpful to directly retrieve the taxonomic information from a biological taxonomy, as they also contain information on the accepted usage of classifications. Which taxonomies exist for this and tools to help you use them, will be explained in more detail in the section biological taxonomies. However, even if you retrieve taxonomic information automatically from a taxonomy, we strongly recommend to double check the retrieved information manually, as mistakes can quickly occur through to, for example, similar taxa names or for rare species. For the cricket dataset, we queried the taxonomic information of the European field cricket (Gryllus campestris) directly from GBIF, which resulted in the following taxonomic terms (table XX). Table 6.1. Taxonomic information for Gryllus campestris as retrieved from GBIF stored in respective Darwin Core terms. scientificName kingdom phylum class order family genus specificEpithet taxonRank Gryllus campestris Linnaeus, 1758 Animalia Arthropoda Insecta Orthoptera Gryllidae Gryllus campestris species The CLUE data covers around 130 different plant species and many of them were either misspelt or synonym names were used. One example is shown in table XX where the species name in the data was Deschampsia flexuos which is a synonym of the species name Avenella flexuosa. This was automatically detected while retrieving the taxonomic information from GBIF and the corresponding information correctly assigned accordingly. Table 6.2. Taxonomic information for Deschampsia flexuosa, which is a synonym of Avenella flexuosa, as retrieved from GBIF and stored in respective Darwin Core terms. scientificName kingdom phylum class order family genus specificEpithet taxonRank Deschampsia flexuosa (L.) Trin. Plantae Tracheophyta Liliopsida Poales Poaceae Avenella flexuosa species 6.5.6.3 individualCount and organismQuantity In the occurrence file, it is important to quantify how many organisms have occurred. There are two different terms to do so, individualCount and organismQuantity. While individualCount is unitless and only gives the number of individuals present at the time of the occurrence, organismQuantity is more flexible, as it describes the quantity of an organism. As organisms do not have to be individuals (see above), it can for example also give the percentage of biomass of an organism or quantify it on a certain scale. Therefore, organismQuantity always requires the additional term organismQuantityType that describes the corresponding quantification system. GBIF provides a Quantity Type Vocabulary with terms they recommend to use to describe the organismQuantityType. There is ongoing discussion on which of the two terms to use. Some suggest deprecating the term individualCount, as the same information can easily be stored in organismQuantity with higher flexibility. However, individualCount seems to be a standard term that is widely used in specific disciplines, where the terms are not necessarily viewed as redundant. Moreover, it was demanded that there should first be standardised vocabulary in place for organismQuantity and organismQuantityType. Based on the greater flexibility and the potential of deprecating individualCount, we recommend using organismQuantity when possible. 6.5.7 Terms of class Measurement or fact The measurement or fact terms require your measurement records to be in a long format. As research data often is stored in a wide format, you will first need to pivot your data before mapping is possible, meaning that all your measured values (or facts) are in one column and have a variable description in a separate column. 6.5.7.1 measurementValue &amp; measurementUnit After transforming your data into the long format, the column containing the measured values or facts can be mapped to the Darwin Core term measurementValue. Measurements come with units, which are often either stored within the column heading or not specified at all in the raw data. Darwin Core accounts for that by the term measurementUnit, which has to be added if you use measurementValue. It is best practice to use SI (International System of Units) units if possible. For unitless measurements the measurementUnit should still be present in the data but can be left empty/filled with NA. 6.5.7.2 measurementType &amp; measurementMethod With the long format and all measurements being stored in measurementValue, you additionally need to store the information what has been measured, i.e., what each measurement or fact means. This is done by the term measurementType. Additionally, you should use the term measurementMethod that states how each measurement was measured. For both terms, it is recommended to use controlled vocabulary where possible. For more information on the use of ontologies, see section ontologies. Example: To exemplify how the four terms measurementValue, measurementUnit, measurementType and measurementMethod are used, the following shows the previous state of the measurements in the bud burst raw data (top) and how the columns containing measurements are mapped to the Darwin Core terms (bottom). Table 6.3. Bud burst data before (top) and after (bottom) mapping to Darwin Core terms. Year TreeID TreeTopScore TreeAllScore 1988 61 2 2 measurementType measurementValue measurementUnit measurementMethod bud burst stage (PO:0025532) of the tree crown 2 NA https://doi.org/10.1098/rspb.2000.1363 bud burst stage (PO:0025532) of the whole tree 2 NA https://doi.org/10.1098/rspb.2000.1363 6.5.7.3 measurementID The measurementID assigns a unique ID to every measurement or fact. Several measurementIDs can belong to one occurrenceID and/or eventID, for example when different characteristics of the same individual are measured. If you choose to create identifiers specific to the dataset, we recommend proceeding with the block-structure of IDs we have already used for the eventID and occurrenceID. If there are measurements at different event levels, extending the occurrenceID or eventID by a separator and a number for the measurement, will lead to unequal length of measurementIDs from different event levels and increases the possibility that IDs are not unique within the dataset. To avoid this, we add the prefix “m” (for measurement) in the block of the ID that numbers the measurements. budburst: measurementID = HV2004_99_5_1_m2 for the second measurement of the eventID HV2004_99_5 and the occurrenceID HV2004_99_5_1 crickets: measurementID = Cr1-05_1_m8 for the eighth measurement of eventID Cr1-05 and occurrenceID Cr-05_1 References Richards, K., White, R., Nicolson, N., &amp; Pyle, R. (2011). A beginner’s guide to persistent identifiers. https://doi.org/10.35035/MJGQ-D052 "],["06.1-taxonomies.html", "6.6 Ontologies and controlled vocabulary 6.7 Biological taxonomies 6.8 Creating GUIDs", " 6.6 Ontologies and controlled vocabulary Ontologies provide definitions of terms by defining their relation to other terms in a human interpretable way and thereby create a semantic model of the concepts that are used within a specific research domain. In simpler words, ontologies define terms and set them in relation to one another. With these references you can ensure that terms in your data are always interpreted the same way and it is clear for other users what each term means. This is especially useful for filling in Darwin Core terms like measurementType or measurementMethod (see chapter Terms of class Measurement or fact). Next to ontologies, you can also refer to terms listed in a thesaurus, which can be seen as a domain specific dictionary. In contrast to ontologies, searching for a specific term across different thesauri is a bit more cumbersome, as there are no look-up servers where you can directly query several thesauri at once. For filling in the keywords of your metadata a thesaurus can however be quite helpful and is recommended to use, so we want to list a few thesauri tailored to biodiversity or ecological data: GEMET - GEneral Multilingual Environmental Thesaurus EnvThes - Thesaurus for long term ecological research, monitoring and experiments UNESCO Thesaurus 6.6.1 Tools to help you The Ontology Lookup Service (OLS) is a repository for biomedical ontologies but it also holds plenty of terms and ontologies relevant for ecology. You can search across ontologies for specific terms or filter for certain ontologies. There is also an API available to facilitate the use of OLS in workflows/programs. Ontobee is a linked data server and another option to browse through around 260 different ontologies and directly search for specific terms. If you are more interested in finding an ontology dedicated to a specific domain, looking directly at the OBO foundry can be helpful. The OBO Foundry (Open Biological and Biomedical Ontology Foundry) is tailored to biological sciences and develops and maintains ontologies. It is not searchable for individual terms but provides information on each ontology. If you chose a specific ontology and before using it you want to assess how FAIR this ontology is, you can use FOOPS!. It is considered a ontology pitfall scanner for FAIR and by providing the URI of an ontology it assesses how well the ontology matches the FAIR principles. 6.7 Biological taxonomies There is a diversity of biological taxonomies that you can use to query taxonomic information for the taxa occurring in your dataset. In this guide we cannot cover all of them but we want to provide some more information on a selected set of taxonomies. 6.7.1 GBIF Backbone taxonomy The GBIF backbone taxonomy, as the name indicates, builds the basis of the indexing of the species occurrence records stored at GBIF and aims to cover all the species that GBIF deals with. It further aims to bring all different taxa names together and organise them. Taxa are assembled from a hierarchical list of 105 sources, using the Catalogue of Life (COL) as a starting point and thereby tightly linking these two taxonomies. Species not found in the COL are then assembled from the remaining sources that are checked afterwards, making the GBIF backbone taxonomy relatively wide-ranging. 6.7.2 Catalogue of Life (COL) The Catalogue of Life is an international community for listing species and aims to create a consistent and up-to-date list of currently accepted species across all known taxonomic groups, which is freely accessible. Besides listing taxa, it aims to show all scientific names a taxon is referenced by. 6.7.3 Encyclopedia of Life (EOL) The Encyclopedia of Life aims to gather knowledge about life on earth and make it globally, openly and freely accessible to everyone. Besides taxonomic information it also provides details on food webs and other ecological aspects of taxa. The community behind it consists of open access biodiversity knowledge providers, such as museums, libraries and universities. 6.7.4 Integrated Taxonomic Information System (ITIS) ITIS is an authoritative system that contains information about taxa and their relationships. It provides a comprehensive and openly available taxonomy and is used as the taxonomic backbone for the Encyclopedia of Life and within the Catalogue of Life. It aims to provide a comprehensive taxonomy of species worldwide to allow sharing of biodiversity data. 6.7.5 World Registry of Marine Species (WoRMS) WoRMS is authoritative classification and catalogue for marine taxa managed by taxonomists and thematic experts that includes accepted and synonym taxonomic information which allows interpretation of the taxonomic literature. It is the recommended biological taxonomy to retrieve information from when publishing data to OBIS. 6.7.6 Tools to help you If you want to retrieve taxonomic information directly from one of the aforementioned taxonomies, there is a helpful R package available that effectively uses the APIs of each of these taxonomies, which is called taxize (Chamberlain et al. (2022)). With taxize you can do plenty of different operations, for example, directly parsing in a list of taxa and retrieving their taxonomic classification or their identifiers from one of the taxonomies (e.g., get_gbifid_() retrieves the taxon information from the GBIF backbone taxonomy). If you want to check whether the species names you use in your data are up to date, if they are spelled correctly or if you only have common names but not scientific names in your data, you can use the global name resolving function of taxize (gnr_resolve()). The Global Names Resolver is a service provided by the EOL and shows you which names could be matched to your input name and in which taxonomies or data sources they can be found. If you query your taxonomic information from a taxonomy you should however always check manually, whether the taxa are identified correctly. Not all taxa are present in all taxonomies or names between taxa are so similar that they are confused for the same taxon in the name matching process. 6.8 Creating GUIDs A globally unique identifier (GUID) is a text string of 36 characters that can be used, amongst others, to assign unique identifiers to each data record. It was established as a variation of the Universally Unique Identifier (UUID) but now both are used synonymously. In contrast to other persistent identifiers (PID) that are assigned to the data level, such as DOI, GUIDs do not have to be issued by a central authority but can be created individually by using a generation algorithm/generator. There are different types of GUID, for more information see here. A GUID is build as follows: {XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX} where each X is a hexadecimal digit, meaning a number from 0 to 9 or a letter from A to F. Through its structure it is extremely unlikely that one GUID is repeated, making it globally unique. To assign GUIDs to your data you can: use an online GUID generator, for example https://www.uuidgenerator.net use the R package uuid(Urbanek &amp; Ts’o (2023)) and its function UUIDgenerate() References Chamberlain, S., Szoecs, E., Foster, Z., &amp; Arendsee, Z. (2022). Taxize: Taxonomic information from around the web. https://docs.ropensci.org/taxize/ Urbanek, S., &amp; Ts’o, T. (2023). Uuid: Tools for generating and handling of UUIDs. https://www.rforge.net/uuid "],["07-structure.html", " 7 Structuring your standardised data 7.1 Relational database 7.2 Darwin Core Archive 7.3 The Core &amp; its extensions 7.4 Which extension(s)?", " 7 Structuring your standardised data Research data comes in all different shapes and is by far not always structured or stored in an organised way. Reusing this data becomes much more difficult if no clear structure is detectable and easily leads to misinterpretations or misunderstanding of the data. By applying a data standard and thereby standardising your column names, as is done with using Darwin Core, your data already becomes much more structured and understandable, as there have to be defined columns that can correspond with these terms. Darwin Core terms (or other data standards) can however be applied irrespective of the overall structure of your data, so the next step in making your data more FAIR is to also standardise the structure of your data. Specifically tailored to Darwin Core, we present two options to structure your data in the following: a relational database and the Darwin Core Archive. 7.1 Relational database Relational databases are a common format in structuring your data files, where individual files are linked to each other by specific identifiers. This allows for a high flexibility in structuring your data and does as such not require your data to be in a standardised format. In Darwin Core several ID fields exist with which tables/files can be connected to each other, allowing the possibility to structure your standardised data as a relational database (Figure 7.1). In contrast to Darwin Core Archives (see Chapter Darwin Core Archive), relational databases can reduce redundancy in the data and are more flexible in linking information. Figure 7.1: Relational database structure of Darwin Core files. Arrows refer to the identifier through which two files are linked. 7.2 Darwin Core Archive For biodiversity data that uses Darwin Core terms, Darwin Core Archives (DwC-A) are one standard format to organise your data, which is also required when you want to publish data on the aforementioned repositories GBIF and OBIS. There are four main components that together, bundled in one zip-folder, build the Darwin Core Archive: the core file several extension files (optional) an EML file (see later chapter) a meta XML file (see later chapter). 7.3 The Core &amp; its extensions The core file is the central element of the archive to which each of the extension files has to be linked. Extensions can only be linked to the core and not to each other, resulting in a so-called star schema (see Figure 7.2). This structure is heavily influenced by GBIF and is widely accepted but it has limited flexibility and partly leads to unnecessary redundancy in the data. A new version of GBIFs data model is currently under development that aims to tackle this issue but that is not yet available, which is why we stick with the star schema for now. Both core and extensions contain one record per row and are linked by the core identifier. Figure 7.2: Schematic of Darwin Core Archive. The data is structured into a core file and sourrounded by extension files in a star shaped manner. The data is accompanied by two metadata files. 7.3.1 Find the core The first step to build your archive therefore is to choose your core file. Which core to choose depends on the type of your data, which for biodiversity data is likely on of the following: Sampling event data: data contains information on ecological studies or monitoring programs, where the sampling is mostly quantitative, calibrated and according to certain protocols and with a documented sampling effort. → core file: Event Checklist data: data contains information on annotated species checklists, taxonomic catalogues or other information about taxa. → core file: Taxon Occurrence data: data contains information on the evidence of an occurrence of a specific taxon either in nature, a collection or a datasets. This is the case if you have a list of species that occur at a particular place and a specific time but this was not measured by following a certain sampling protocol. → core file: Occurrence 7.3.2 Occurrence core vs. event core It is not always directly clear what the difference between the occurrence and the event core is and when you should use which of them. The most crucial difference between the two is whether the data was collected following a certain protocol with documented sampling effort. If that is the case, you have sampling event data and therefore an event core. If data collection did not follow a sampling protocol, your core will be the occurrence file. For measurement on individual specimens and museum collections in general, occurrence is however mostly the preferred core. Most of the ecological data falls into sampling event data, as was the case for all of the data sets we used in the development of this guide. We will therefore focus on Event as the core file. If you have checklist data, there is already a detailed description on how to transform that into a Darwin Core Archive in the publication of Reyserhove et al. (2020). 7.4 Which extension(s)? Once you have determined your core and it is clear which information goes into it, you have to find one or more extensions if you need to store remaining information. The following extension files are possible: Event (if event is not the core) Occurrence (if occurrence is not the core) Taxon (if taxon is not the core) (extended) measurement or fact: The measurementOrFact file contains information on all the actual measurement values or facts that have been recorded for the records in the core file. The measurementOrFact file is always an extension file. There is also the extendedMeasurementOrFact extension, which is developed to be used with an event core and creates an additional link between the occurrence extension file and the measurements by including the occurrenceID next to the coreID (i.e. eventID). ResourceRelationship: describes the relationships between resources in a Darwin Core Occurrence, Event, or Taxon Core to resources in an extension or external to the dataset Identification: additional information on multiple identifications for species listed in Occurrence core dnaDerivedData: DNA related data, used either with occurrence or event core Releve: GBIF specific relevé file containing information on vegetation plot survey measurements; to be used together with event core and occurrence extension MaterialSample Amplification: information on DNA amplifications based on a schema from the Global Genome Biodiversity Network (GGBN) HumboldtEcologicalInventory: information on ecological inventories related to event core Based on our use cases with an event core, we recommend occurrence and extended measurement or fact as extension files for sampling event data, because this allows storing taxonomic information for every event, as well as all measured values. References Reyserhove, L., Desmet, P., Oldoni, D., Adriaens, T., Strubbe, D., Davis, A. J. S., Vanderhoeven, S., Verloove, F., &amp; Groom, Q. (2020). A checklist recipe: making species data open and FAIR. Database, 2020, baaa084. https://doi.org/10.1093/database/baaa084 "],["07.1-arrange-DwCA.html", "7.5 Arrange Darwin Core terms in core and extension files", " 7.5 Arrange Darwin Core terms in core and extension files In the Darwin Core Archive the Darwin Core terms are sorted into the different files, each having terms that are required and some that we highly recommend to use, while others are optional additions. Generally, every extension file needs to contain the core identifier of the core file. For example, if you have an event core, the core ID is the field “eventID” and every extension file therefore needs to have the column eventID, linking records to the records in the core. In the following we provide an overview which terms can be included in which file. This is based on the documentation in the GBIF schema repository. Additionally we indicate which terms we view as required to have in each file and which we strongly recommend, adapted from the data quality requirements of GBIF for each data type (Sampling event, occurrence, checklist data). 7.5.1 General terms (Terms of class Record-level) There are terms that can always be included in the core-file, independent of what the core is. Those are record-level terms, which are generic and can apply to any type of records in the data. The content of these terms can be considered metadata, which is why it is likely that you do not have corresponding columns in your data yet but need to create new columns. Many of these terms are from the Dublin Core namespace and few come with a fixed vocabulary to use to fill them. The following terms can be useful to add, while none of them are required: type: to be filled with one of the terms in the DCMI type vocabulary language: use controlled vocabulary, such as the ISO language code licence: see section “licencing” rightsHolder accessRights bibliographicCitation: should provide clear information on how to cite the resource itself references institutionCode, institutionID datasetID basisOfRecord: best practise is to use controlled vocabulary, such as the names of the Darwin Core classes: MaterialEntity, PreservedSpecimen, FossilSpecimen, LivingSpecimen, MaterialSample, Event, HumanObservation, MachineObservation, Taxon, Occurrence, MaterialCitation 7.5.2 Event file The event file can include the following terms: all terms of the class Event all terms of the class Location all terms of the class GeologicalContext Required: eventID eventDate samplingProtocol sampleSizeValue &amp; sampleSizeUnit Recommended: parentEventID (if applicable) samplingEffort decimalLatitude &amp; decimalLongitude &amp; geodeticDatum coordinateUncertaintyInMeters countryCode 7.5.3 Occurrence file All of the terms that can be included in the event file can also be included in the occurrence file. The occurrence file is however more flexible and additionally allows for more terms. You can additionally include: all terms of the class Occurrence all terms of the class Organism all terms of the class Taxon all terms of the class Identification Required: scientificName occurrenceStatus basisOfRecord occurrenceID Recommended: taxonRank kingdom (or other higher taxonomy) decimalLongitue &amp; decimalLatitude &amp; geodeticDatum coordinateUncertaintyInMeters countryCode individualCount or organismQuantity &amp; organismQuantityType 7.5.4 Taxon file The taxon file only contains the record-level terms and all terms of the class taxon. Required: taxonID scientificName taxonRank Recommended: kingdom (or other higher taxonomy) parentNameUsageID acceptedNameUsageID 7.5.5 MeasurementOrFact file The MeasurementOrFact file is linked to the core by including the coreID and besides that can only include the terms of the class MeasurementOrFact, while no terms of other classes should be included. The extendedMeasurementOrFact extension additionally contains the occurrenceID and three ID fields (measurementTypeID, measurementValueID, measurementUnitID) which are however not from the Darwin Core namespace but refer to OBIS. As their IRIs are not resolvable, we would not recommend using them. "],["08-metadata.html", " 8 Standardise and structure your metadata 8.1 Metadata standards (for biodiversity data) 8.2 Tools to help you 8.3 Our choice 8.4 EML terms", " 8 Standardise and structure your metadata Once you have described your data and assigned metadata to it (see chapter 4), you can make your data more FAIR by also mapping your metadata to a standard format. Similar to data standards described in a previous section, there are also several metadata standards available to do so. The distinction between metadata standards and data standards is however often not that clear and Darwin Core sometimes is also referred to as a metadata standard. 8.1 Metadata standards (for biodiversity data) As with the data standards, a wide range of metadata standards exists, partly tailored to specific research domains. For ecological data, a widely used metadata standard, for example by OBIS and GBIF, is the Ecological metadata language (EML), which is also the required format for metadata in the Darwin Core Archives. It consists of a set of defined terms used to describe metadata and is compatible with other community standards. EML uses a readable XML markup syntax that balances the machine and human readability and is structured in modules making it relatively flexible in describing metadata. The developers provide a good overview of all the modules and EML terms together with important details and definitions in their interactive schema documentation. Dublin Core is a metadata standard consisting of a set of 15 terms used to describe metadata. The terms themselves consist of several dozen properties, classes, data types, and vocabulary encoding schemes and are maintained by the Dublin Core Metadata Initiative (DCMI). 8.2 Tools to help you The metadata standard catalog, provides a nice overview of the different metadata standards and is searchable, for example, for specific topics. 8.3 Our choice For all of our datasets we choose EML as the metadata standard because it is very flexible and provides all the necessary terms to describe our metadata. Especially through its terms about spatial, temporal and taxonomic coverage it captures the key elements of our datasets and is much better suited to describe ecological data then Dublin Core, for example. Additionally, as we went with Darwin Core Archives to structure the data, EML was required to use as a metadata standard. 8.4 EML terms EML consists of a wide range of terms of which some are required, while others might just be nice to have depending on what information your metadata contains. In general, terms can have several levels of subterms and we will not cover all of them here (detailed information on every term can be found here and some best practices here). For all terms containing text the “xml:lang” attribute should be added if the language is not English. Be aware that EML terms use the spelling of American English. Required terms EML comes with only three terms that are required to make the EML document schema valid. &lt;title&gt;: a brief title that provides enough information to distinguish this dataset or resource from others and makes clear what the data is about. The “xml:lang” attribute can be used to specify in which language the title is given, which is especially useful if you want to give alternative titles in different languages. &lt;creator&gt;: information on the person or organisation that created the dataset &lt;contact&gt;: information on the person or organisation that contacted about the data, e.g., if questions arise Highly recommended terms The following set of terms is not strictly required by EML but we would highly recommend to provide as much of them as possible, as this increases the richness of the metadata and provides valuable information about the data that is helpful for others to understand and reuse the data. &lt;metadataProvider&gt;: information about the person that has provides the metadata in the EML file Creator, contact and metadataProvider The creator and the contact can either be a named person, a certain position that is always staying the same even though the people in the position might change, or an organisation. For each of these cases, different subterms exist in all three terms (including metadataProvider) to describe the person/position/organisation accordingly. It is therefore required to choose at least one of the terms &lt;individualName&gt;, &lt;organizationName&gt; or &lt;positionName&gt;. &lt;individualName&gt;: the name of an individual person can be given with three subterms: &lt;salutation&gt;: can be a title (e.g., “Dr.”) or another salutation (“Mr.”/“Ms.”) &lt;givenName&gt; &lt;surName&gt; &lt;organisationName&gt; &lt;positionName&gt; &lt;address&gt; &lt;city&gt; &lt;postalCode&gt; &lt;country&gt; &lt;electronicMalAddress&gt; &lt;userID&gt;: an identifier that links the person or organisation to a directory if individuals, e.g., an ORCID. This terms requires the attribute &lt;directory&gt; to state which directory the ID refers to and will generally be an URL (e.g., https://orcid.org) The contact, creator and metadataProvider can be different people/organisations/positions but can also all be the same. In the latter case, do this. &lt;language&gt;: provides the language the resource is written in and can either be a well-known language name or ideally, a ISO language code &lt;abstract&gt;: a short summary of the data that includes basic information to give an idea what the data is about. &lt;para&gt;: Text is stored in this subterm, which can either directly have a text string or a set of subterms consisting of markup tags allowing for formatting of the text. &lt;keywordSet&gt;: can either be one or many keywords describing the resource &lt;keyword&gt;: each keyword is stored separately in this subterm as text. &lt;keywordThesaurus&gt;: If the keywords are part of a thesaurus, it can be specified which thesaurus they are derived from. &lt;coverage&gt;: Describes the spatial, temporal and taxonomic coverage of the resource. &lt;temporalCoverage&gt;: information on what time span is covered in the data. Use ISO 8601 (i.e., YYYY-MM-DD) for date and time information. Needs either one of the subterms: &lt;singleDateTime&gt;: describes a single date and time either by describing a calendar date or a geologic date, using the subterms: &lt;calendarDate&gt; &lt;time&gt; &lt;alternativeTimeScale&gt;: can describe alternative time scale, e.g., geological date, with a range of subterms &lt;rangeOfDates&gt;: allows to describe a time range by specifying start and end date (with the same subterms as for singleDateTime). Can be repeated if there are several time ranges that need to be described. &lt;beginDate&gt; &lt;endDate&gt; &lt;geographicCoverage&gt;: information about the spatial extent of the data that should be given as text and coordinates with the subterms: &lt;geographicDescription&gt;: short text explanation of the spatial extent &lt;boundingCoordinates&gt;: stating the four edges of a bounding box, by giving: &lt;westBoundingCoordinate&gt; &lt;eastBoundingCoordinate&gt; &lt;northBoundingCoordinate&gt; &lt;southBoundingCoordinate&gt; &lt;taxonomicCoverage&gt;: information about the taxa covered in the data &lt;generalTaxonomicCoverage&gt;: text description of the taxa included in the data &lt;taxonomicClassification&gt;: taxonomic classification of the range of taxa included in the data, specified through a set of subterms: &lt;taxonRankName&gt;: taxonomic level of information &lt;taxonRankValue&gt;: taxonomic nam &lt;commonName&gt; &lt;taxonID&gt;: identifier of the taxon from a certain provider that has to be specified with the attribute “provider” containing a URI (e.g., provider = “https://eol.org” if the taxon ID is retrieved from EOL) &lt;maintenance&gt;: information on with which frequency the data is updated and whether data collection is still ongoing &lt;maintenanceUpdateFrequency&gt;: needs to be filled with a term of the EML MaintUpFreqType, for example: annually, asNeeded, biannually, daily, irregular or unknown &lt;description&gt;: text description of maintenance stated with subterm &lt;para&gt; &lt;methods&gt;: stepwise information on methods for data collection &lt;methodStep&gt;: can be repeated for each method step and includes a range of subterms to specify instruments, software, protocols, descriptions and citations of the methods &lt;pubDate&gt;: publication date of the resource in ISO 8601 (i.e., YYYY-MM-DD) &lt;licensed&gt;: information on the licence of the data and how it can be used by others, for information in which licence to assign see chapter licencing &lt;licenseName&gt;: official name of the licence &lt;url&gt;: URL referring to the licence (e.g., https://creativecommons.org/licenses/by/4.0/) Other terms There are plenty of other terms that can be suitable to use for your data. The following are just some examples, for a full overview check the EML schema documentation. &lt;project&gt;: broader background information on the project in which the data was collected, needs to have the subterms &lt;title&gt; &lt;personnel&gt; Optional subterms: &lt;abstract&gt; &lt;funding&gt; &lt;studyAreaDescription&gt; &lt;intellectualRights&gt;: information on the intellectual property rights as text using subterm &lt;para&gt; &lt;alternateIdentifier&gt; &lt;additionalInfo&gt;: Can capture any information that cannot be captured by the remaining terms and is filled with text using subterm &lt;para&gt; &lt;introduction&gt;: overview of background and context of the dataset, similar to an introduction of a journal article &lt;usageCitation&gt;: citation to articles or other products where the data is used, consists of a range of subterms "],["08.1-structural-metadata.html", "8.5 Structural metadata - The meta.xml file", " 8.5 Structural metadata - The meta.xml file The meta-file contains the structural metadata and is required by the Darwin Core Archive. It describes how the files are organised, i.e., which file is the core, which are the extensions, and it links every column of each file to its corresponding Darwin Core term by providing its IRI (Internationalized Resource Identifier). The file format of the meta-file is XML. &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;archive xmlns=&quot;http://rs.tdwg.org/dwc/text/&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:xs=&quot;http://www.w3.org/2001/XMLSchema&quot; xsi:schemaLocation=&quot;http://rs.tdwg.org/dwc/text/ http://rs.tdwg.org/dwc/text/tdwg_dwc_text.xsd&quot;&gt; &lt;core encoding=&quot;UTF-8&quot; fieldsTerminatedBy=&quot;,&quot; linesTerminatedBy=&quot;\\n&quot; fieldsEnclosedBy=&quot;&quot; ignoreHeaderLines=&quot;1&quot; rowType=&quot;http://rs.tdwg.org/dwc/terms/Event&quot;&gt; &lt;files&gt; &lt;location&gt;budburst_event.csv&lt;/location&gt; &lt;/files&gt; &lt;id index=&quot;0&quot;/&gt; &lt;field index=&quot;0&quot; term=&quot;http://rs.tdwg.org/dwc/terms/eventID&quot;/&gt; &lt;field index=&quot;1&quot; term=&quot;http://rs.tdwg.org/dwc/terms/parentEventID&quot;/&gt; &lt;field index=&quot;2&quot; term=&quot;http://rs.tdwg.org/dwc/terms/samplingProtocol&quot;/&gt; &lt;field index=&quot;3&quot; term=&quot;http://rs.tdwg.org/dwc/terms/sampleSizeValue&quot;/&gt; &lt;field index=&quot;4&quot; term=&quot;http://rs.tdwg.org/dwc/terms/sampleSizeUnit&quot;/&gt; &lt;field index=&quot;5&quot; term=&quot;http://rs.tdwg.org/dwc/terms/eventDate&quot;/&gt; &lt;field index=&quot;6&quot; term=&quot;http://rs.tdwg.org/dwc/terms/year&quot;/&gt; &lt;field index=&quot;7&quot; term=&quot;http://rs.tdwg.org/dwc/terms/month&quot;/&gt; &lt;field index=&quot;8&quot; term=&quot;http://rs.tdwg.org/dwc/terms/day&quot;/&gt; &lt;field index=&quot;9&quot; term=&quot;http://rs.tdwg.org/dwc/terms/country&quot;/&gt; &lt;field index=&quot;10&quot; term=&quot;http://rs.tdwg.org/dwc/terms/countryCode&quot;/&gt; "],["08.2-XML-in-R.html", "8.6 Create XML files in R", " 8.6 Create XML files in R Both the EML and the meta file are XML files. Creating XML files in R is facilitated by a range of useful packages, such as emld (Boettiger et al. (2020)), EML (Boettiger &amp; Jones (2022)) and xml2 (Wickham et al. (2023)). The key for creating the EML file are lists. The content of every EML term has to be stored as a character in a list. If terms consist of subterms, they have to be stored in nested lists within the list of the parent term. # Provide keywords and their thesaurus fo the EML term &quot;keywordSet&quot; keywordSet &lt;- list(list(keyword = list(&quot;bud burst&quot;, &quot;trees&quot;, &quot;ecology&quot;, &quot;plant phenology&quot;), keywordThesaurus = &quot;envThes&quot;), list(keyword = &quot;oak&quot;, keywordThesaurus = &quot;GEMET&quot;)) keyword and keywordThesaurus are the EML subterms of keywordSet, which looks like this in XML: &lt;keywordSet&gt; &lt;keyword&gt;bud burst&lt;/keyword&gt; &lt;keyword&gt;trees&lt;/keyword&gt; &lt;keyword&gt;ecology&lt;/keyword&gt; &lt;keyword&gt;plant phenology&lt;/keyword&gt; &lt;keywordThesaurus&gt;envThes&lt;/keywordThesaurus&gt; &lt;/keywordSet&gt; &lt;keywordSet&gt; &lt;keyword&gt;oak&lt;/keyword&gt; &lt;keywordThesaurus&gt;GEMET&lt;/keywordThesaurus&gt; &lt;/keywordSet&gt; In contrast to the EML file, where the metadata is specific to the dataset and has to be filled in by hand, the meta file always consists of the same content only depending on the file and column names of the individual Darwin Core Archive. Instead of creating the meta file by hand, this process can be more easily automated. References Boettiger, C., &amp; Jones, M. B. (2022). EML: Read and write ecological metadata language files. https://docs.ropensci.org/EML/ Boettiger, C., Jones, M. B., &amp; Mecum, B. (2020). Emld: Ecological metadata as linked data. https://docs.ropensci.org/emld/ Wickham, H., Hester, J., &amp; Ooms, J. (2023). xml2: Parse XML. https://xml2.r-lib.org/ "],["09-glossary.html", " 9 Glossary", " 9 Glossary API An Application Programming Interface (API) defines a way of communication between two applications based on requests and responses, which allows data exchange between these applications. Source: https://www.ibm.com/topics/api Core file The core file is the central element of the Darwin Core Archive to which all other files are linked. Different files are possible to be used as the core depending on the type of the data. While checklist data requires a taxon file as the core, occurrence data requires an occurrence file and sampling-event data an event file. More information: https://ipt.gbif.org/manual/en/ipt/latest/dwca-guide Darwin Core (DwC) Darwin Core is a stable data standard maintained by TDWG (Biodiversity Information Standards) tailored to describe biodiversity data by using a defined library of terms. It is an extension of the Dublin Core Metadata Initiative and facilitates sharing of biodiversity data from various sources. Every term has a clear definition and a URL, which can be used irrespective of technology, e.g., XML or RDF. Source: https://www.tdwg.org/standards/dwc/ Darwin Core Archive Darwin Core Archives are a way of structuring a dataset that uses the Darwin Core standard. It consists of a core file and a number of extension files, as well as two files describing the metadata of the dataset. The first is called meta.xml and is a XML file describing the structural metadata, i.e., the organisation of the files and maps each column name (i.e., Darwin Core term) to its identifier. The second metadata file is the EML file containing the descriptive and administrative metadata using the metadata standard Ecological metadata language. More information: https://ipt.gbif.org/manual/en/ipt/latest/dwca-guide Data(set) Data is defined as information that is collected to be used for decision making or refers to information in a digital format that can be used by computers. Dataset refers to a collection of data. The two terms are not always used consistently across sources. We use data to describe the collected/recorded data itself (in contrast to metadata that describes it), which can (but does not have to) consist of several data files. Dataset is used when we refer to the data and the metadata together. Sources: https://dictionary.cambridge.org/dictionary/english/data https://dictionary.cambridge.org/dictionary/english/datset Data lifecycle The data lifecycle describes the stages that the data goes through, starting with planning of data collection, followed by the data collection or acquisition. Afterwards the data is processed to then be used and analysed. After that, the data should be properly stored and curated to be preserved in the long run, which finally leads to publishing and sharing the data, allowing others to reuse it, with which the cycle can start again. There are however different versions of the data lifecycle, differing slightly in the respective steps. More information: https://www.britishecologicalsociety.org/wp-content/uploads/Publ_Data-Management-Booklet.pdf Data management Data management is the practice of taking care of data throughout its entire lifecycle, from its collection, processing and use to its storage and sharing. Throughout the whole lifecycle, good data management is crucial to ultimately enhance the reusability of the data for yourself or others. More information: https://www.britishecologicalsociety.org/wp-content/uploads/Publ_Data-Management-Booklet.pdf Dublin Core Dublin Core is a metadata standard consisting of a set of 15 terms used to describe metadata. The terms themselves consist of several dozen properties, classes, data types, and vocabulary encoding schemes and are maintained by the Dublin Core Metadata Initiative (DCMI). More information: https://www.dublincore.org/specifications/dublin-core/dcmi-terms/ EML EML, the Ecological metadata language, is a metadata standard commonly used for ecological data. It consists of a set of terms with which metadata can be described. EML uses a readable XML markup syntax that balances the machine and human readability and is structured in modules making it relatively flexible in describing metadata. More information: https://eml.ecoinformatics.org/schema/ Event Event is a Darwin Core class describing an action that occurs at some location during some time. In the context of Darwin Core archives events can be stored in an event-file that stores information on the time and place of the events, but also more administrative information, such as the data collector, sampling effort or sample size. Sources: https://dwc.tdwg.org/terms/#event https://rs.gbif.org/core/dwc_event_2024-02-19.xml Extension file Extension files are a file type that can exist in Darwin Core Archives next to the core file. There can be an unlimited number of extensions, each holding a different kind of information and each row within an extension file directly links to a record in the core file through the core identifier. Commonly used extensions for biodiversity data collected on living organisms are occurrence, taxon or measurement or fact. More information: https://ipt.gbif.org/manual/en/ipt/latest/dwca-guide FAIR FAIR stands for Findable, Accessible, Interoperable and Reusable and comes with 15 guiding principles developed by Wilkinson et al. in 2015. Findability means that the metadata (and the data) can easily be found by humans and computers and that machine-readable metadata allows for automatic discovery of the data by machines. Additionally, it is clearly stated how the user can access the data and whether, for example, authorisation or authentication are required (Accessibility). Interoperability means that the data is fully compatible with other data making it integrable with other data resources and allows to incorporate it into workflows or applications. In easier words this means that data resources should ‘speak the same language’ to be used together. The ultimate goal of the FAIR concept is to make the data reusable (Reusability), which means that there is proper annotation in the form of metadata that allows users (and machines) to understand the data and correctly interpret it. More information: https://www.go-fair.org/fair-principles/ GBIF GBIF, the global biodiversity information facility, is a data infrastructure and international network providing open access to biodiversity data. Source: https://www.gbif.org/ IRI, URI and URL Uniform resource identifiers (URI) are a standardised way to identify resources of information on the internet. One of the most common forms of the URI is the URL, the uniform resource locator, often also called a web address. The IRI (Internationalized Resource Identifier) is an internationalised expansion of the URI supporting a wider range of characters besides the Latin alphabet. While URL is location oriented and describes the physical location of a resource on the internet, the URI is identifier oriented and refers to the resource itself. They are however often treated the same way. Sources and more information: GBIF (2011). A Beginner’s Guide to Persistent Identifiers, version 1.0. Released on 9 February 2011. Authors Kevin Richards, Richard White, Nicola Nicolson, Richard Pyle, Copenhagen: Global Biodiversity Information Facility, 33 pp, accessible online at http://links.gbif.org/persistent_identifiers_guide_en_v1.pdf https://www.w3.org/2001/Talks/0912-IUC-IRI/paper.html MeasurementOrFact Measurement or fact is a Darwin Core class describing measurement values and or facts about a resource. It can also be an extension file of the Darwin Core archive storing the measurements or facts belonging to the events or occurrences described in the core file. Source: https://dwc.tdwg.org/terms/#measurementorfact Metadata Metadata contains information about other data and provides descriptions that make the data easier to understand and reuse. Structural metadata contains information about how the data is organised, what variables mean and how files relate to one another, while administrative metadata provides information about ownership, preservation and rights and licences of the data. Thirdly, descriptive metadata describes the background of the data, such as how, where and when it was collected, who is responsible and what contents the data cover. More information: https://www.w3.org/TR/dwbp/#metadata Meta.xml The meta.xml file is a component of the Darwin Core Archive and contains its structural metadata in an XML file. The file describes how the data files in the archive are linked to each other and provides for each column the identifier of the corresponding Darwin Core term. More information: https://ipt.gbif.org/manual/en/ipt/latest/dwca-guide Mobilisation Transferring data that is stored locally to an online repository and thereby enabling its reuse by others. More information: https://www.gbif.org.nz/mobilising/ OBIS OBIS, the Ocean Biodiversity Information System, is an international databank system for maritime biodiversity and biogeographic data with the objective to provide the largest knowledge base on the diversity, distribution and abundance of marine organisms. Source: https://obis.org/about/ Occurrence Occurrence is a Darwin Core class describing the existence or presence of an organism at a certain place and time. In the context of Darwin Core archives, occurrences are stored in an occurrence-file that contains quantitative information about organisms, as well as taxonomic information. Sources: https://dwc.tdwg.org/terms/#occurrence https://rs.gbif.org/core/dwc_occurrence_2024-02-23.xml Ontology Ontologies provide definitions of terms by defining their relation to other terms in a human interpretable way and thereby create a semantic model of the concepts that are used within a specific research domain. More information: https://www.ontotext.com/knowledgehub/fundamentals/what-are-ontologies/ Persistent identifier (PID) A persistent identifier is a unique identification code belonging to a digital resource that ensures persistent identification of the resource, even if the web address of the creator of the resource changes. For detailed information on different PID systems see here. Provenance Provenance is a form of metadata that provides information about the history of the dataset, which includes people and parties that were involved in its creation, as well as how, where and when the data was created. Additionally, the provenance details should inform about previous versions of the data and the change history. More information: https://www.pldn.nl/wiki/Provenance RDF RDF, the resource description framework, is a framework to represent interconnected data on the web. In RDF, objects are linked using semantic triples consisting of two objects and the link between them, often referred to as subject (= object 1), predicate ( = link) and object (= object 2). RDFs are machine readable and allow high interoperability between different data sources. Source: https://www.w3.org/RDF/ Standardisation Converting data and/or metadata to a standard format to increase compatibility with other data. More information: https://www.sisense.com/glossary/data-standardization/ Star schema The star schema is the way the core file and the extension files are organised in the Darwin Core Archive. Each of the extension files directly links to the core file, while they cannot link to one another, creating what is considered to be a star-shape. More information: https://ipt.gbif.org/manual/en/ipt/latest/dwca-guide Taxonomy Taxonomy is a standard way of describing, naming and classifying things in a hierarchical way and in a biological context, classifying living and extinct organisms and the relationship between taxa. Sources: http://purl.obolibrary.org/obo/NCIT_C17469 http://purl.obolibrary.org/obo/GSSO_004259 XML Extensible markup language, XML, is a hardware- and software independent tool for storing and transmitting data by balancing human-readability with machine-readability. It is the file format used for the two metadata files in the Darwin Core Archive. Source: https://www.w3schools.com/xml/ "],["10-references.html", " 10 References", " 10 References Boettiger, C., &amp; Jones, M. B. (2022). EML: Read and write ecological metadata language files. https://docs.ropensci.org/EML/ Boettiger, C., Jones, M. B., &amp; Mecum, B. (2020). Emld: Ecological metadata as linked data. https://docs.ropensci.org/emld/ Chamberlain, S., Szoecs, E., Foster, Z., &amp; Arendsee, Z. (2022). Taxize: Taxonomic information from around the web. https://docs.ropensci.org/taxize/ Pagano, P., Candela, L., &amp; Castelli, D. (2013). Data interoperability. Data Science Journal, 12(0), GRDI19–GRDI25. https://doi.org/10.2481/dsj.grdi-004 Reyserhove, L., Desmet, P., Oldoni, D., Adriaens, T., Strubbe, D., Davis, A. J. S., Vanderhoeven, S., Verloove, F., &amp; Groom, Q. (2020). A checklist recipe: making species data open and FAIR. Database, 2020, baaa084. https://doi.org/10.1093/database/baaa084 Richards, K., White, R., Nicolson, N., &amp; Pyle, R. (2011). A beginner’s guide to persistent identifiers. https://doi.org/10.35035/MJGQ-D052 Urbanek, S., &amp; Ts’o, T. (2023). Uuid: Tools for generating and handling of UUIDs. https://www.rforge.net/uuid Visser, M. E., &amp; Holleman, L. J. M. (2001). Warmer springs disrupt the synchrony of oak and winter moth phenology. Proceedings of the Royal Society of London. Series B: Biological Sciences, 268(1464), 289–294. https://doi.org/10.1098/rspb.2000.1363 Vogels, J. J., Verberk, W. C. E. P., Kuper, J. T., Weijters, M. J., Bobbink, R., &amp; Siepel, H. (2021). How to restore invertebrate diversity of degraded heathlands? A case study on the reproductive performance of the field cricket gryllus campestris (l.). Frontiers in Ecology and Evolution, 9. https://doi.org/10.3389/fevo.2021.659363 Wickham, H., Hester, J., &amp; Ooms, J. (2023). xml2: Parse XML. https://xml2.r-lib.org/ Wilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., Silva Santos, L. B. da, Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR guiding principles for scientific data management and stewardship. Scientific Data, 3(1). https://doi.org/10.1038/sdata.2016.18 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
